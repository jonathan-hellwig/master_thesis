\documentclass[17pt,institute=e10]{tuhh_presentation}
%\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{tikz}

\usepackage{biblatex}[backend=biber,style=authoryear,autocite=inline]
\addbibresource{thesis.bib}
\usepackage[nolinks]{qrcode}
\usepackage{pythonhighlight}
\def\UrlBreaks{\do\/\do-}

\title{Modelling stochastic gradient
descent with stochastic differential equations}
\date{24.10.2022} 
\author[Jonathan Hellwig]{Jonathan Hellwig}
\email{jonathan.hellwig@tuhh.de}

\autofontdecrement % This commands will automatically reduce font size for itemize/enumerate environments
\definecolor{purple-pontifex}{RGB}{93,47,134}
\definecolor{blue-pontifex}{RGB}{171,232,239}
\definecolor{gray-pontifex}{RGB}{184,184,184}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Lo}{\mathcal{L} : \R^n \rightarrow \R}
\newcommand{\CL}{\mathcal{L}}
\newcommand{\norm}[1]{\lvert {#1} \rvert_2}
\newcommand{\E}[1]{\mathbb{E}\left[{#1} \right]}
\newcommand{\V}[1]{\mathbb{V}\left[{#1} \right]}
\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}
% \newcommand{\purple}[1]{\colortext{purple-pontifex}{#1}}
\begin{document}

\titlepage


\begin{frame}[agenda]
  \tableofcontents
\end{frame}

\section{Problem formulation}

\begin{frame}
  \frametitle{Problem formulation}
  \emph{Task}: Given a function $\CL : \R^n \rightarrow \R$, find $w^\star \in \R^n$ such that
  \begin{equation*}
    \CL(w^\star) = \min_{w \in \R^n} \CL(w).
  \end{equation*}

  In machine learning settings, we commonly have functions of the form
  \begin{equation*}
    \CL(w) = \frac{1}{n} \sum_{i=1}^n \ell_i(w).
  \end{equation*}
  Each individual function $\ell_i(w)$ measures the \textcolor{purple-pontifex}{\emph{loss}} with respect to a single data point $(x_i, y_i)$ for $i = 1,2, \dots, n$.
  We refer to this as \textcolor{purple-pontifex}{\emph{empirical risk minimization problem}}.
\end{frame}

\section{Gradient descent}

\begin{frame}[fragile]
  \frametitle{Gradient descent}

  \begin{block}{Definition}
    Let $\Lo$ be a function and $\eta > 0$. Then, given an initial value $w_0 \in R^n$ the iterates of \textcolor{purple-pontifex}{\emph{gradient descent}} (GD) are given by
  \begin{equation*}
    w_{k+1} = w_k - \eta \nabla \CL(w_k)
  \end{equation*}
  for $k = 1,2,\dots$.
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Example}
  
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/sample_data.pdf}
      \caption{Sample data}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Model}
  Given sample data $\{(x_i,y_i)\}_{i=1}^n$ we construct the model
  \begin{equation*}
    y_i = w x_i + b + \epsilon,
  \end{equation*}
  where $w, b \in \R$ and $\epsilon \sim \mathcal{N}(0,1)$.

  Next, we define the loss function $\CL : \R^2 \rightarrow \R$ by
  \begin{equation*}
    \CL(w,b) = \sum_{i = 1}^{n} \left(w x_i + b - y_i\right)^2.
  \end{equation*}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Loss function}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/quadratic_loss.pdf}
      \caption{Loss function}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Numerical experiments}
  We conduct numerical experiments with gradient descent with the following settings
  \begin{itemize}
    \item $\eta \in \{\expnumber{1}{-5},\expnumber{5}{-5}, \expnumber{1}{-4}, \expnumber{5}{-4}, \expnumber{1}{-3}\}$
    \item $w_0 = 0.62, b_0 = 0.53$
    \item 1000 total iterations.
  \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Gradient descent}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/linear_fit.pdf}
      \caption{Linear fit}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Evolution of weight and bias}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm,width=\textwidth]{plots/learning_rates.pdf}
      \caption{Weight and bias}
    \end{figure}
\end{frame}



\begin{frame}[fragile]
  \frametitle{Example: Evolution of weight and bias}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm,width=\textwidth]{plots/large_learning_rate.pdf}
      \caption{Weight and bias}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Continuous-time model}
  Recall that the iterates of GD are given by
  \begin{equation*}
    w_{k+1} = w_k - \eta \nabla \CL(w_k).
  \end{equation*}
  \begin{block}{Continuous-time model}
    If we interpret $\eta > 0$ as a time step, the iterates of GD are the same as the iterates of Euler's method for the system of ordinary differential equations (ODE)
  \begin{equation*}
    W'(t) = - \nabla \CL(W(t)), W(0) = w_0.
  \end{equation*} 
  \end{block}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Scaled weights and biases}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm,width=\textwidth]{plots/scaled_weights_biases.pdf}
      \caption{Scaled weight and bias}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Continuous-time model}
  Recall the loss function $\CL : \R^2 \rightarrow \R$, $\CL(w, b) = \sum_{i=1}^n (w x_i + b - y_i)^2$.

  The gradient is given by 
  \begin{align*}
    \nabla_w \CL(w,b) &= 2\left(\sum_{i=1}^n x_i^2 \right) w + 2\left(\sum_{i=1}^n x_i \right) b - 2 \sum_{i=1}^n x_i y_i \\
    \nabla_b \CL(w,b) &= 2\left(\sum_{i=1}^n x_i \right) w + 2 n b - 2 \sum_{i=1}^n y_i
  \end{align*}
  \begin{block}{Continuous-time model}
  \begin{equation*}
    \begin{bmatrix}
      w'(t)      \\
      b'(t)     
  \end{bmatrix}
  = 
  \begin{bmatrix}
    -2  \displaystyle\sum_{i=1}^n x_i^2  &  -2  \displaystyle\sum_{i=1}^n x_i      \\
      -2  \displaystyle\sum_{i=1}^n x_i  &  -2n      
  \end{bmatrix}
  \begin{bmatrix}
    w(t)     \\
    b(t)    
  \end{bmatrix}
  +
  \begin{bmatrix}
    2  \displaystyle\sum_{i=1}^n x_i y_i     \\
    2  \displaystyle\sum_{i=1}^n y_i    
  \end{bmatrix},
  \begin{bmatrix}
    w(0)     \\
    b(0)    
  \end{bmatrix}
  =
  \begin{bmatrix}
    w_0     \\
    b_0   
  \end{bmatrix}
  \end{equation*}
  
  \end{block}
\end{frame}

\section{Stochastic gradient descent}

\begin{frame}[fragile]
  \frametitle{Stochastic gradient descent}
  \begin{itemize}
    \item For large $n \in \mathbb{N}$ the evaluation of the gradient $\nabla \CL$ is expensive
    \item Idea: compute as sampled gradient
  \end{itemize}
  % For risk minimization problems with large $n \in \mathbb{N}$ the evaluation of each gradient $\nabla \CL$ is expensive. Instead of computing the full gradient we exploit the structure of the problem and compute as sampled gradient:
  % \begin{equation*}
  %   w_{k+1} = w_k - \eta \nabla \CL(w_k).
  % \end{equation*}
  \begin{block}{Definition}
    Let $(\Omega, \mathcal{F}, \mathcal{P})$ be a probability space and let $\Lo$ be a function of the form $\CL (w) = \frac{1}{n} \sum_{i=1}^n \ell_i(w)$.
    Then, the iterates of \textcolor{purple-pontifex}{\emph{stochastic gradient descent}} (SGD) are given by
    \begin{equation*}
      W_{k+1} = W_k - \eta \nabla \ell_{\gamma_k} (W_k), W_0 = w_0 \in \mathbb{R}^n
    \end{equation*}
    where $\gamma_k : \Omega \rightarrow \{1,2,\dots,n\}$. 
  \end{block}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Stochastic gradient descent}
  Notice that the iterates $\{W_k\}_{k=1}^\infty$ of SGD form a stochastic process.
  It can be decomposed as follows
  \begin{align*}
    W_{k+1} &= W_k - \eta \nabla \ell_{\gamma_k} (W_k) \\
    &= W_k - \eta \nabla \CL(W_k) + \eta \left(\nabla \CL(W_k) - \nabla \ell_{\gamma_k} (W_k) \right).
  \end{align*}
  We refer to $N_k = \eta \left(\nabla \CL(W_k) - \nabla \ell_{\gamma_k} (W_k) \right)$ as the \emph{\textcolor{purple-pontifex}{gradient noise}}.

  Its expected value is given by
  \begin{equation*}
    \E{N_k|W_k} = 0
  \end{equation*}
  and the \textcolor{purple-pontifex}{covariance matrix} is given by
  \begin{equation*}
    \V{N_k|W_k} = \eta^2 \; \E{\left(\nabla \ell_{\gamma_k}(W_k) - \nabla \CL(W_k)\right)\left(\nabla \ell_{\gamma_k}(W_k) - \nabla \CL(W_k)\right)^T}.
  \end{equation*}
  
\end{frame}


\begin{frame}[fragile]
  \frametitle{Example: Linear model}
  Recall the loss function $\CL : \R^2 \rightarrow \R$ defined by
  \begin{equation*}
    \CL(w,b) = \sum_{i = 1}^{n} \left(w x_i + b - y_i\right)^2.
  \end{equation*}
  The SGD iterates are given by sampling the gradient
  \begin{align*}
    w_{k+1} &= w_k - \eta \nabla_w \ell_{\gamma_k}(w_k, b_k) \\
    b_{k+1} &= b_k - \eta \nabla_b \ell_{\gamma_k}(w_k, b_k).
  \end{align*}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Numerical experiments}
    We conduct numerical experiments in the following setting
    \begin{itemize}
      \item $\eta = 0.1$
      \item 1000 iterations 
      \item 10000 runs.
    \end{itemize}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: SGD fit}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/sgd_linear_fit.pdf}
      \caption{Linear fit with SGD}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Evolution of weight and bias}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm,width=\textwidth]{plots/sgd_weight_bias.pdf}
      \caption{Weight and bias}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Weight distribution}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm,width=\textwidth]{plots/sgd_weight_histogram.pdf}
      \caption{Weight histogram for different iterations}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Example: Quantile-quantile plot}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/sgd_weight_qq.pdf}
      \caption{Quantile-quantile plot for observed data and normal distribution}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
  \frametitle{SGD: Gradient noise assumption}
  Recall the decomposition of the SGD iterates
  \begin{equation*}
    W_{k+1} = W_k - \eta \nabla \CL(W_k) + N_k,
  \end{equation*}
  where $N_k = \eta \left(\nabla \CL(W_k) - \nabla \ell_{\gamma_k} (W_k) \right)$ for $k = 1,2,\dots$.
  \begin{block}{Assumption \autocite{liStochasticModifiedEquations2019}}
  For the previous example it is reasonable to assume that 
  \begin{equation*}
    N_k \sim \mathcal{N}(0, \eta^2 \Sigma(W_k)),
  \end{equation*}
    where $\Sigma(W_k) = \E{\left(\nabla \ell_{\gamma_k}(W_k) - \nabla \CL(W_k)\right)\left(\nabla \ell_{\gamma_k}(W_k) - \nabla \CL(W_k)\right)^T}$.
\end{block}
Question: Can we find an analog to the system of ODEs for the stochastic case?
\end{frame}
\section{Stochastic differential equations}
\begin{frame}
  \frametitle{Brownian motion}
  \begin{block}{Definition \autocite{durrettProbabilityTheoryExamples2019}}
    A stochastic process $\{B_t\}_{t \geq 0}$ is called \emph{\textcolor{purple-pontifex}{Brownian motion}} if it satisfies the following properties
  \begin{enumerate}
    \item For any $t \geq s > u \geq v \geq 0$, $B_{t+s} - B_t$ and $B_{v+u} - B_v$ are independent.
    \item For any $s,t \geq 0$ $B_{t+s} - B_s \sim \mathcal{N}(0, tI_d)$.
    \item The paths $t \rightarrow B_t$ are continuous almost surely.
  \end{enumerate}
  \end{block}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Brownian motion}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/brownian_motion.pdf}
      \caption{Brownian motion samples}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  Let $t_0 = a < t_1 < \dots < t_n = b$ be a partition of the interval $[a,b]$. Then the \emph{\textcolor{purple-pontifex}{total variation}} of a process $X$ is defined as 
  \begin{equation*}
    V(X(\omega);[a,b]) = \sup_{t_0,t_1,\dots,t_n} \sum_{k=1}^n |X_{t_{k+1}} - X_{t_k}(\omega)|
  \end{equation*}
  and the \emph{\textcolor{purple-pontifex}{quadratic variation}} is defined as 
  \begin{equation*}
    Q(X(\omega);[a,b]) = \sup_{t_0,t_1,\dots,t_n} \sum_{k=1}^n |X_{t_{k+1}} - X_{t_k}(\omega)|^2.
  \end{equation*}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Brownian motion}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/brownian_motion_variation.pdf}
      \caption{Quadratic and total variation of Brownian motion}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
  \frametitle{Brownian motion}
  It can be shown that 
  \begin{itemize}
    \item Brownain motion has infinite total variation almost surely on finite intervals
    \item Brownain motion has finite quadratic variation and $Q(B(\omega);[0,T]) = T$ almost surely \autocite{eAppliedStochasticAnalysis2021}.
  \end{itemize}
\end{frame}
\begin{frame}
  \frametitle{Itô integral}
  % TODO: Add rigor to this definition
  % TODO: Add examples
  % TODO: Verweise auf die Literatur

  % Note: This is not the definition for the Ito integral!
  \begin{block}{Definition \autocite{eAppliedStochasticAnalysis2021}}
    Let $t_0 = 0 < t_1 < \dots < t_n = T$ be a parition of $[0,T]$.
    For a class of appropriate functions we define the \emph{\textcolor{purple-pontifex}{Itô integral}} as the following limit
\begin{equation*}
  \int_0^T f(W_s) dB_s := \lim\limits_{|\delta| \rightarrow 0} \sum_{j=1}^n f(W_{t_j})(B_{t_{j+1}} - B_{t_j}),
\end{equation*}
where $(B_t)_{t \geq 0}$ is a one-dimensional Brownian motion and\;$\delta = max_{j =1,\dots,n} |t_{j+1} - t_j|$.
\end{block}

\end{frame}
\begin{frame}
We define \emph{\textcolor{purple-pontifex}{stochastic differential equations}} as integral equations of the following form
\begin{equation*}
  W_t = W_0 + \int_0^t b(W_s,s) ds + \int_0^t \sigma(W_s, s) dB_s,
\end{equation*}
where $b : \R^n \times [0,T] \rightarrow \R^n$ is called \emph{\textcolor{purple-pontifex}{drift}} and $\sigma : \R^{n,n} \times [0,T] \rightarrow \R^n$ \emph{\textcolor{purple-pontifex}{diffusion}}.
\end{frame}
\section{SDE model for SGD}
\begin{frame}
  \frametitle{SDE model}
  \begin{block}{Definition \autocite{kloedenNumericalSolutionStochastic2013}}
    Let $0 \leq t_0 < t_1 < \dots < t_N = T$, $\Delta t_n = t_{n+1} - t_n$ and let $\Delta B_n \sim \mathcal{N}(0,\Delta t_n I_n)$. Then, the time-discrete scheme $\{W_n\}_{n=0}^\infty$ given by
\begin{equation*}
  W_{n+1} = W_n + b(W_n, t_n)\Delta t + \sigma(W_n, t_n) \Delta B_n
\end{equation*}
for $0 \leq n \leq N$ is called \emph{\textcolor{purple-pontifex}{Euler-Maruyama scheme}}.
  \end{block}
  Recall the decomposition of the SGD iterates
  \begin{equation*}
    W_{k+1} = W_k - \eta \nabla \CL(W_k) + N_k,
  \end{equation*}
  where $N_k \sim \mathcal{N}(0, \eta^2\Sigma(W_k))$. By rewriting we obtain
  \begin{equation*}
    W_{k+1} = W_k - \nabla \CL(W_k) \eta + \sqrt{\eta}\Sigma(W_k) Z_k,
  \end{equation*}
  where $Z_k \sim \mathcal{N}(0, \eta I_n)$.
\end{frame}
\begin{frame}
  \frametitle{SDE model}
  From the previous slide we follow that \textcolor{purple-pontifex}{continuous-time} model for SGD is given by
  \begin{equation*}
    W_t = W_0 + \int_0^t -\nabla  \CL(W_s) ds + \int_0^t \sqrt{\eta} \Sigma(W_s) dB_s.
  \end{equation*}
\end{frame}
\begin{frame}
  \frametitle{Example \autocite{liStochasticModifiedEquations2019}}
  Let $H \in \mathbb{R}^{d\times d}$ be a symmetric, positive matrix. Define the sample objective 
\begin{equation*}
  f_{\gamma}(w) = \frac{1}{2} (w - \gamma)^T H (w - \gamma) - \frac{1}{2} \text{Tr}(H)
\end{equation*}
for $\gamma \sim N(0,I)$. The total objective is $f(w) = \E{f_{\gamma}(w)} = \frac{1}{2} w^T H w$.
The SDE model is given by
\begin{equation*}
  dW_t = -H W_t dt + \sqrt{\eta}H dB_t.
\end{equation*}
This process is called \emph{Ornstein-Uhlenbeck} process and has the analytical solution
\begin{equation*}
  W_t = e^{-t H}(W_0 + \sqrt{\eta}\int_0^te^{s H}H dB_s).
\end{equation*}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Loss value for SGD and SDE}
    \vspace{-1cm}
    \begin{figure}
      \hspace*{-4.7cm}
      \includegraphics[height=12cm, width=1.4\textwidth]{plots/sde_sgd.pdf}
      \caption{Loss value of SDE and SGD}
    \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Weak approximation}
\begin{block}{Definition}
  A continuous-time process is an order $\alpha$ \emph{\textcolor{purple-pontifex}{weak approximation}} of a discrete-time processes if for every $g \in G^{\alpha + 1}$, there exists a positive constant such that
  \begin{equation*}
    \max_{k=0,\dots,N} |\E{g(w_k)} - \E{g(W_{k\eta})}] \leq C \eta^\alpha,
  \end{equation*}
  where $G^{\alpha+1}$ denotes the class of $(\alpha+1)$-times continuously differentiable functions with at most polynomial growth.
\end{block}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: Weak approximation}
    \vspace{-1cm}
    \begin{figure}
      \centering
      \includegraphics[height=12cm]{plots/convergence_rate.pdf}
      \caption{Log error with respect to the learning rate}
    \end{figure}
\end{frame}

\begin{frame}
\begin{block}{Theorem \autocite{liStochasticModifiedEquations2019}}
  The continuous-time process $\{W_t\}$ is an order-1 weak approximation of SGD, i.e. for each $g \in G^2$, there exists a constant $C > 0$ independent of $\eta$ such that
  \begin{equation*}
    \max_{k=0,\dots,N} |\E{g(w_k)} - \E{g(W_{k\eta})}] \leq C \eta.
  \end{equation*}
\end{block}
\end{frame}
\section{Summary}
\begin{frame}
  \frametitle{Summary}
  \begin{itemize}
    \item We looked at the behavior of GD
    \item We formulated a system of ODEs that models the limiting behavior of GD
    \item We investigated the behavior of SGD
    \item We formulated a stochastic model for SGD
  \end{itemize}
  \end{frame}
\begin{frame}
  \frametitle{Source code}
  The source code to recreate all figures of this presentation can be found at \url{https://github.com/jonathan-hellwig/sgd_sde_model}.
\end{frame}
\begin{frame}
  \frametitle{Bibliography}
% TODO: Further resources
% TODO: Link to notebooks
\printbibliography
\end{frame}

\end{document}
