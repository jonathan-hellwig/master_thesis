\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{nty/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\abx@aux@cite{0}{brownLanguageModelsAre2020}
\abx@aux@segm{0}{0}{brownLanguageModelsAre2020}
\abx@aux@cite{0}{rameshHierarchicalTextConditionalImage2022}
\abx@aux@segm{0}{0}{rameshHierarchicalTextConditionalImage2022}
\abx@aux@cite{0}{schrittwieserMasteringAtariGo2020}
\abx@aux@segm{0}{0}{schrittwieserMasteringAtariGo2020}
\abx@aux@cite{0}{jumperHighlyAccurateProtein2021}
\abx@aux@segm{0}{0}{jumperHighlyAccurateProtein2021}
\abx@aux@cite{0}{glorotUnderstandingDifficultyTraining2010}
\abx@aux@segm{0}{0}{glorotUnderstandingDifficultyTraining2010}
\abx@aux@cite{0}{heDelvingDeepRectifiers2015a}
\abx@aux@segm{0}{0}{heDelvingDeepRectifiers2015a}
\abx@aux@cite{0}{duchiAdaptiveSubgradientMethods2011}
\abx@aux@segm{0}{0}{duchiAdaptiveSubgradientMethods2011}
\abx@aux@cite{0}{geoffreyhintonnitishsrivastavaandkevinswer-NeuralNetworksMachine2012}
\abx@aux@segm{0}{0}{geoffreyhintonnitishsrivastavaandkevinswer-NeuralNetworksMachine2012}
\abx@aux@cite{0}{kingmaAdamMethodStochastic2017}
\abx@aux@segm{0}{0}{kingmaAdamMethodStochastic2017}
\abx@aux@cite{0}{wilsonMarginalValueAdaptive2017}
\abx@aux@segm{0}{0}{wilsonMarginalValueAdaptive2017}
\abx@aux@cite{0}{goyalAccurateLargeMinibatch2018}
\abx@aux@segm{0}{0}{goyalAccurateLargeMinibatch2018}
\abx@aux@cite{0}{orvietoContinuoustimeModelsStochastic2019a}
\abx@aux@segm{0}{0}{orvietoContinuoustimeModelsStochastic2019a}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@cite{0}{smithOriginImplicitRegularization2021}
\abx@aux@segm{0}{0}{smithOriginImplicitRegularization2021}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{wuNoisyGradientDescent2020a}
\abx@aux@segm{0}{0}{wuNoisyGradientDescent2020a}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation}{8}{section.1}\protected@file@percent }
\newlabel{sec:Motivation}{{1}{8}{Motivation}{section.1}{}}
\abx@aux@cite{0}{evansPartialDifferentialEquations2010}
\abx@aux@segm{0}{0}{evansPartialDifferentialEquations2010}
\@writefile{toc}{\contentsline {section}{\numberline {2}Notation}{9}{section.2}\protected@file@percent }
\newlabel{sec:notation}{{2}{9}{Notation}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Derivatives}{9}{subsection.2.1}\protected@file@percent }
\abx@aux@cite{0}{lycheNumericalLinearAlgebra2020}
\abx@aux@segm{0}{0}{lycheNumericalLinearAlgebra2020}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Function spaces}{10}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Matrices}{10}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Miscellaneous}{10}{subsection.2.4}\protected@file@percent }
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\@writefile{toc}{\contentsline {section}{\numberline {3}Introduction to stochastic gradient descent}{11}{section.3}\protected@file@percent }
\newlabel{sec:stochastic_optimization}{{3}{11}{Introduction to stochastic gradient descent}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Stochastic processes}{11}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:stochastic_processes}{{3.1}{11}{Stochastic processes}{subsection.3.1}{}}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{durrettProbabilityTheoryExamples2019}
\abx@aux@segm{0}{0}{durrettProbabilityTheoryExamples2019}
\newlabel{thm:conditional_expectation}{{3.8}{12}{\protect {\cite [pp.~14]{eAppliedStochasticAnalysis2021}}}{theorem.3.8}{}}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Risk minimization}{14}{subsection.3.2}\protected@file@percent }
\newlabel{def:model}{{3.13}{14}{Model, loss function}{definition.3.13}{}}
\newlabel{eq:empirical_risk}{{3.2}{14}{Expected risk, empirical risk, \protect { \cite {bottouOptimizationMethodsLargeScale2018}}}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Characterization of minimizers}{14}{subsection.3.3}\protected@file@percent }
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{brownLanguageModelsAre2020}
\abx@aux@segm{0}{0}{brownLanguageModelsAre2020}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\newlabel{eq:minimizer}{{3.4}{15}{Local and global minimizer, \protect {\cite [pp.~12]{nocedalNumericalOptimization2006}}}{equation.3.4}{}}
\newlabel{thm:necessary_condition}{{3.16}{15}{\protect {\cite [pp.~15]{nocedalNumericalOptimization2006}}}{theorem.3.16}{}}
\newlabel{eq:StationaryPoint}{{3.5}{15}{Stationary point, \protect {\cite [pp.~15]{nocedalNumericalOptimization2006}}}{equation.3.5}{}}
\newlabel{thm:sufficient_condition}{{3.18}{15}{\protect {\cite [pp.~16]{nocedalNumericalOptimization2006}}}{theorem.3.18}{}}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Gradient descent and stochastic gradient descent}{16}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:gd_sgd}{{3.4}{16}{Gradient descent and stochastic gradient descent}{subsection.3.4}{}}
\newlabel{eq:descent_direction}{{3.6}{16}{Descent direction, \protect {\cite [pp.~30]{nocedalNumericalOptimization2006}}}{equation.3.6}{}}
\newlabel{lem:descent_direction}{{3.20}{16}{}{lemma.3.20}{}}
\newlabel{eq:continuity}{{3.8}{16}{Gradient descent and stochastic gradient descent}{equation.3.8}{}}
\abx@aux@cite{0}{dengImageNetLargescaleHierarchical2009}
\abx@aux@segm{0}{0}{dengImageNetLargescaleHierarchical2009}
\abx@aux@cite{0}{polyakMethodsSpeedingConvergence1964}
\abx@aux@segm{0}{0}{polyakMethodsSpeedingConvergence1964}
\newlabel{eq:gradient_descent}{{3.9}{17}{One-step method, gradient descent, step size, learning rate \protect {\cite {nocedalNumericalOptimization2006}}}{equation.3.9}{}}
\newlabel{def:sgd}{{3.22}{17}{Stochastic gradient descent, \protect {\cite {polyakMethodsSpeedingConvergence1964}}}{definition.3.22}{}}
\newlabel{eq:stochastic_gradient_descent}{{3.10}{17}{Stochastic gradient descent, \protect {\cite {polyakMethodsSpeedingConvergence1964}}}{equation.3.10}{}}
\newlabel{eq:mini_batch_sgd}{{3.11}{17}{Stochastic gradient descent, \protect {\cite {polyakMethodsSpeedingConvergence1964}}}{equation.3.11}{}}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Convergence analysis of stochastic gradient descent}{18}{subsection.3.5}\protected@file@percent }
\newlabel{subsec:convergence_analysis_of_sgd}{{3.5}{18}{Convergence analysis of stochastic gradient descent}{subsection.3.5}{}}
\abx@aux@cite{0}{allen-zhuConvergenceTheoryDeep2019}
\abx@aux@segm{0}{0}{allen-zhuConvergenceTheoryDeep2019}
\abx@aux@cite{0}{mertikopoulosAlmostSureConvergence2020}
\abx@aux@segm{0}{0}{mertikopoulosAlmostSureConvergence2020}
\abx@aux@cite{0}{vaswaniFastFasterConvergence2019}
\abx@aux@segm{0}{0}{vaswaniFastFasterConvergence2019}
\abx@aux@cite{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@segm{0}{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@cite{0}{liConvergenceStochasticGradient2019}
\abx@aux@segm{0}{0}{liConvergenceStochasticGradient2019}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{nesterovLecturesConvexOptimization2018}
\abx@aux@segm{0}{0}{nesterovLecturesConvexOptimization2018}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Convergence assumptions}{19}{subsubsection.3.5.1}\protected@file@percent }
\newlabel{def:lipschitz_continuity}{{3.25}{19}{Lipschitz continuity, Lipschitz constant, \protect {\cite [pp.~10]{nesterovLecturesConvexOptimization2018}}}{definition.3.25}{}}
\newlabel{def:l_smooth}{{3.26}{19}{$L$-smoothness, \protect {\cite {bottouOptimizationMethodsLargeScale2018}}}{definition.3.26}{}}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@segm{0}{0}{gowerSGDGeneralAnalysis2019}
\newlabel{eq:Lsmoothproof1}{{3.13}{20}{Convergence assumptions}{equation.3.13}{}}
\newlabel{eq:Lsmoothproof2}{{3.14}{20}{Convergence assumptions}{equation.3.14}{}}
\abx@aux@cite{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@segm{0}{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@cite{0}{boydConvexOptimization2004}
\abx@aux@segm{0}{0}{boydConvexOptimization2004}
\abx@aux@cite{0}{boydConvexOptimization2004}
\abx@aux@segm{0}{0}{boydConvexOptimization2004}
\abx@aux@cite{0}{boydConvexOptimization2004}
\abx@aux@segm{0}{0}{boydConvexOptimization2004}
\abx@aux@cite{0}{boydConvexOptimization2004}
\abx@aux@segm{0}{0}{boydConvexOptimization2004}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{moulinesNonAsymptoticAnalysisStochastic2011}
\abx@aux@segm{0}{0}{moulinesNonAsymptoticAnalysisStochastic2011}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@segm{0}{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@cite{0}{boydConvexOptimization2004}
\abx@aux@segm{0}{0}{boydConvexOptimization2004}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\newlabel{lemma:convexity}{{3.31}{21}{\protect {\cite [pp.~69]{boydConvexOptimization2004}}}{lemma.3.31}{}}
\abx@aux@cite{0}{robbinsStochasticApproximationMethod1951}
\abx@aux@segm{0}{0}{robbinsStochasticApproximationMethod1951}
\abx@aux@cite{0}{robbinsStochasticApproximationMethod1951}
\abx@aux@segm{0}{0}{robbinsStochasticApproximationMethod1951}
\abx@aux@cite{0}{zhouStochasticMirrorDescent2017}
\abx@aux@segm{0}{0}{zhouStochasticMirrorDescent2017}
\abx@aux@cite{0}{nguyenSGDHogwildConvergence2018}
\abx@aux@segm{0}{0}{nguyenSGDHogwildConvergence2018}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@segm{0}{0}{bottouOptimizationMethodsLargeScale2018}
\abx@aux@cite{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@segm{0}{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{gowerSGDGeneralAnalysis2019}
\abx@aux@segm{0}{0}{gowerSGDGeneralAnalysis2019}
\newlabel{as:sgd_convergence}{{3.33}{22}{\protect {\cite {bottouOptimizationMethodsLargeScale2018}}}{assumption.3.33}{}}
\newlabel{eq:variance_linear_growth}{{{{(ii)}}}{22}{\protect {\cite {bottouOptimizationMethodsLargeScale2018}}}{Item.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Convergence results}{22}{subsubsection.3.5.2}\protected@file@percent }
\newlabel{sec:convergence_results}{{3.5.2}{22}{Convergence results}{subsubsection.3.5.2}{}}
\abx@aux@cite{0}{nesterovLecturesConvexOptimization2018}
\abx@aux@segm{0}{0}{nesterovLecturesConvexOptimization2018}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{wuNoisyGradientDescent2020a}
\abx@aux@segm{0}{0}{wuNoisyGradientDescent2020a}
\newlabel{lemma:gradient_inequality}{{3.35}{23}{\protect {\cite {gowerSGDGeneralAnalysis2019}}}{lemma.3.35}{}}
\newlabel{lemma:sgd_iterates}{{3.36}{23}{\protect {\cite {sebbouhAlmostSureConvergence2021}}}{lemma.3.36}{}}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\newlabel{eq:iterate_norm}{{3.18}{24}{Convergence results}{equation.3.18}{}}
\newlabel{thm:SGD_bound}{{3.37}{24}{\protect {\cite {sebbouhAlmostSureConvergence2021}}}{theorem.3.37}{}}
\abx@aux@cite{0}{nocedalNumericalOptimization2006}
\abx@aux@segm{0}{0}{nocedalNumericalOptimization2006}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@cite{0}{sebbouhAlmostSureConvergence2021}
\abx@aux@segm{0}{0}{sebbouhAlmostSureConvergence2021}
\newlabel{thm:almost_sure_convergence}{{3.39}{26}{\protect {\cite {sebbouhAlmostSureConvergence2021}}}{theorem.3.39}{}}
\newlabel{cor:sgd_convergence}{{3.40}{26}{\protect {\cite {sebbouhAlmostSureConvergence2021}}}{theorem.3.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Summary and outlook}{26}{subsection.3.6}\protected@file@percent }
\abx@aux@cite{0}{ahmadTextbookOrdinaryDifferential2015}
\abx@aux@segm{0}{0}{ahmadTextbookOrdinaryDifferential2015}
\abx@aux@cite{0}{ahmadTextbookOrdinaryDifferential2015}
\abx@aux@segm{0}{0}{ahmadTextbookOrdinaryDifferential2015}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{hairerGeometricNumericalIntegration2013}
\abx@aux@segm{0}{0}{hairerGeometricNumericalIntegration2013}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\@writefile{toc}{\contentsline {section}{\numberline {4}Stochastic differential equations}{27}{section.4}\protected@file@percent }
\newlabel{sec:BackgroundSDETheory}{{4}{27}{Stochastic differential equations}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Ordinary differential equations}{27}{subsection.4.1}\protected@file@percent }
\newlabel{eq:initial_value_problem}{{4.1}{27}{Ordinary differential equations}{equation.4.1}{}}
\newlabel{thm:ode_existence}{{4.1}{27}{\protect {\cite [pp.~73]{ahmadTextbookOrdinaryDifferential2015}}}{theorem.4.1}{}}
\newlabel{eq:ode_euler}{{4.3}{27}{Euler's method, see, e.g., \protect {\cite [pp.~3]{hairerGeometricNumericalIntegration2013}}}{equation.4.3}{}}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{durrettProbabilityTheoryExamples2019}
\abx@aux@segm{0}{0}{durrettProbabilityTheoryExamples2019}
\abx@aux@cite{0}{durrettProbabilityTheoryExamples2019}
\abx@aux@segm{0}{0}{durrettProbabilityTheoryExamples2019}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\newlabel{thm:euler_convergence}{{4.3}{28}{}{theorem.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}It么 Integral}{28}{subsection.4.2}\protected@file@percent }
\newlabel{subsec:ItoIntegral}{{4.2}{28}{It么 Integral}{subsection.4.2}{}}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{capassoIntroductionContinuousTimeStochastic2012}
\abx@aux@segm{0}{0}{capassoIntroductionContinuousTimeStochastic2012}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\newlabel{thm:ito_isometry}{{4.7}{29}{\autocite {eAppliedStochasticAnalysis2021}}{theorem.4.7}{}}
\newlabel{item:zero_integral}{{{{(i)}}}{29}{\autocite {eAppliedStochasticAnalysis2021}}{Item.24}{}}
\newlabel{eq:ito_integral_ev}{{4.4}{29}{\autocite {eAppliedStochasticAnalysis2021}}{equation.4.4}{}}
\newlabel{eq:ito_product}{{4.5}{29}{\autocite {eAppliedStochasticAnalysis2021}}{equation.4.5}{}}
\newlabel{eq:ito_isometry}{{4.6}{29}{\autocite {eAppliedStochasticAnalysis2021}}{equation.4.6}{}}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{capassoIntroductionContinuousTimeStochastic2012}
\abx@aux@segm{0}{0}{capassoIntroductionContinuousTimeStochastic2012}
\abx@aux@cite{0}{capassoIntroductionContinuousTimeStochastic2012}
\abx@aux@segm{0}{0}{capassoIntroductionContinuousTimeStochastic2012}
\abx@aux@cite{0}{baldiEquazioniDifferenzialiStocastiche2000}
\abx@aux@segm{0}{0}{baldiEquazioniDifferenzialiStocastiche2000}
\newlabel{lem:ito}{{4.9}{30}{}{lemma.4.9}{}}
\newlabel{eq:multivariate_ito_isometry}{{4.7}{30}{}{equation.4.7}{}}
\newlabel{eq:ito_process}{{4.8}{30}{It么 process, \protect {\cite [pp.~230]{capassoIntroductionContinuousTimeStochastic2012}}}{equation.4.8}{}}
\newlabel{thm:ito_formula}{{4.12}{30}{It么's formula, \protect {\cite [pp.~232]{capassoIntroductionContinuousTimeStochastic2012}}}{theorem.4.12}{}}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{oksendalStochasticDifferentialEquations2003}
\abx@aux@segm{0}{0}{oksendalStochasticDifferentialEquations2003}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{eq:sde}{{4.10}{31}{Drift, diffusion, SDE, \protect {\cite [pp.~149]{eAppliedStochasticAnalysis2021}}}{equation.4.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Existence and uniqueness}{31}{subsection.4.3}\protected@file@percent }
\newlabel{subsec:sde_existence_uniqueness}{{4.3}{31}{Existence and uniqueness}{subsection.4.3}{}}
\newlabel{as:sde_existence}{{4.14}{31}{\protect {\cite [pp. 30]{liStochasticModifiedEquations2019}}}{assumption.4.14}{}}
\newlabel{thm:sde_existence}{{4.15}{31}{}{theorem.4.15}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{uhlenbeckTheoryBrownianMotion1930}
\abx@aux@segm{0}{0}{uhlenbeckTheoryBrownianMotion1930}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\newlabel{ex:ornstein_uhlenbeck}{{4.16}{32}{Ornstein-Uhlenbeck process, see, e.g., \protect {\cite {uhlenbeckTheoryBrownianMotion1930}}}{example.4.16}{}}
\newlabel{eq:ornstein_solution_1d}{{4.11}{32}{Ornstein-Uhlenbeck process, see, e.g., \protect {\cite {uhlenbeckTheoryBrownianMotion1930}}}{equation.4.11}{}}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@cite{0}{kloedenNumericalSolutionStochastic2013}
\abx@aux@segm{0}{0}{kloedenNumericalSolutionStochastic2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Numerical approximations}{33}{subsection.4.4}\protected@file@percent }
\newlabel{subsec:sde_numerical_methods}{{4.4}{33}{Numerical approximations}{subsection.4.4}{}}
\newlabel{def:strong_convergence}{{4.18}{33}{Strong convergence, \protect {\cite [pp.~323]{kloedenNumericalSolutionStochastic2013}}}{definition.4.18}{}}
\newlabel{def:weak_convergence}{{4.19}{33}{Weak convergence, \protect {\cite [pp.~327]{kloedenNumericalSolutionStochastic2013}}}{definition.4.19}{}}
\abx@aux@cite{0}{eAppliedStochasticAnalysis2021}
\abx@aux@segm{0}{0}{eAppliedStochasticAnalysis2021}
\abx@aux@cite{0}{liStochasticModifiedEquations2017}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2017}
\abx@aux@cite{0}{hairerGeometricNumericalIntegration2013}
\abx@aux@segm{0}{0}{hairerGeometricNumericalIntegration2013}
\abx@aux@cite{0}{santambrogioEuclideanMetricWasserstein2017}
\abx@aux@segm{0}{0}{santambrogioEuclideanMetricWasserstein2017}
\newlabel{def:euler_maruyama}{{4.20}{34}{Euler-Maruyama scheme, \protect {\cite [pp.~305]{kloedenNumericalSolutionStochastic2013}}}{definition.4.20}{}}
\newlabel{eq:euler_maruyama}{{4.15}{34}{Euler-Maruyama scheme, \protect {\cite [pp.~305]{kloedenNumericalSolutionStochastic2013}}}{equation.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Stochastic modified differential equations}{34}{section.5}\protected@file@percent }
\newlabel{sec:sde_model}{{5}{34}{Stochastic modified differential equations}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Gradient descent}{34}{subsection.5.1}\protected@file@percent }
\newlabel{sec:gradient_flow}{{5.1}{34}{Gradient descent}{subsection.5.1}{}}
\newlabel{eq:gradient_flow}{{5.1}{34}{Gradient flow, see, e.g., \protect {\cite {santambrogioEuclideanMetricWasserstein2017}}}{equation.5.1}{}}
\newlabel{ex:linear_regression}{{5.2}{34}{Linear regression}{example.5.2}{}}
\abx@aux@cite{0}{romanAdvancedLinearAlgebra2010}
\abx@aux@segm{0}{0}{romanAdvancedLinearAlgebra2010}
\abx@aux@cite{0}{romanAdvancedLinearAlgebra2010}
\abx@aux@segm{0}{0}{romanAdvancedLinearAlgebra2010}
\newlabel{def:pos_neg_def}{{5.3}{35}{Positive definite, negative definite, indefinite}{definition.5.3}{}}
\newlabel{lem:positive_eigen_values}{{5.4}{35}{\protect {\cite [pp.~250]{romanAdvancedLinearAlgebra2010}}}{lemma.5.4}{}}
\newlabel{thm:ode_system}{{5.5}{36}{}{theorem.5.5}{}}
\abx@aux@cite{0}{hairerGeometricNumericalIntegration2013}
\abx@aux@segm{0}{0}{hairerGeometricNumericalIntegration2013}
\abx@aux@cite{0}{hairerGeometricNumericalIntegration2013}
\abx@aux@segm{0}{0}{hairerGeometricNumericalIntegration2013}
\newlabel{eq:modified_ode}{{5.3}{37}{Modified ordinary differential equation, \protect {\cite [pp.~337]{hairerGeometricNumericalIntegration2013}}}{equation.5.3}{}}
\abx@aux@cite{0}{barrettImplicitGradientRegularization2021}
\abx@aux@segm{0}{0}{barrettImplicitGradientRegularization2021}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{eq:gradient_descent_function}{{5.4}{38}{}{equation.5.4}{}}
\newlabel{eq:modified_equation_powers}{{5.5}{38}{}{equation.5.5}{}}
\newlabel{eq:second_order_ode}{{5.6}{38}{}{equation.5.6}{}}
\newlabel{eq:linear_regression_gradient_flow}{{5.7}{39}{\protect {\autoref {ex:linear_regression}} continued}{equation.5.7}{}}
\newlabel{eq:linear_regression_second_order}{{5.8}{39}{\protect {\autoref {ex:linear_regression}} continued}{equation.5.8}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Stochastic gradient descent}{40}{subsection.5.2}\protected@file@percent }
\newlabel{sec:smde_sgd}{{5.2}{40}{Stochastic gradient descent}{subsection.5.2}{}}
\newlabel{eq:sgd_decomposition}{{5.9}{40}{Stochastic gradient descent}{equation.5.9}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{simsekliTailIndexAnalysisStochastic2019}
\abx@aux@segm{0}{0}{simsekliTailIndexAnalysisStochastic2019}
\abx@aux@cite{0}{zhouTheoreticallyUnderstandingWhy2020}
\abx@aux@segm{0}{0}{zhouTheoreticallyUnderstandingWhy2020}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{ex:sgd}{{5.11}{41}{\autoref {ex:linear_regression} continued}{example.5.11}{}}
\newlabel{as:normality}{{5.12}{41}{\protect {\cite {liStochasticModifiedEquations2019}}}{assumption.5.12}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{evansPartialDifferentialEquations2010}
\abx@aux@segm{0}{0}{evansPartialDifferentialEquations2010}
\newlabel{eq:sgd_normal_decomposition}{{5.10}{42}{Stochastic gradient descent}{equation.5.10}{}}
\newlabel{eq:first_oder_smde}{{5.11}{42}{Stochastic modified differential equation}{equation.5.11}{}}
\newlabel{as:sde_model}{{5.14}{42}{\protect {\cite {liStochasticModifiedEquations2019}}}{assumption.5.14}{}}
\newlabel{as:bounded_gradient}{{{{(ii)}}}{42}{\protect {\cite {liStochasticModifiedEquations2019}}}{Item.34}{}}
\newlabel{eq:general_sde}{{5.12}{42}{Stochastic gradient descent}{equation.5.12}{}}
\abx@aux@cite{0}{evansPartialDifferentialEquations2010}
\abx@aux@segm{0}{0}{evansPartialDifferentialEquations2010}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{def:weak_derivatives}{{5.15}{43}{Weak derivatives, \protect {\cite [pp.~242]{evansPartialDifferentialEquations2010}}}{definition.5.15}{}}
\newlabel{eq:weak_derivative}{{5.14}{43}{Weak derivatives, \protect {\cite [pp.~242]{evansPartialDifferentialEquations2010}}}{equation.5.14}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{eq:one_step_error}{{5.15}{44}{Stochastic gradient descent}{equation.5.15}{}}
\newlabel{lem:sgd_one_step}{{5.18}{44}{\protect {\cite [Lemma 5]{liStochasticModifiedEquations2019}}}{lemma.5.18}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{lem:sde_one_step}{{5.19}{45}{\protect {\cite {liStochasticModifiedEquations2019}}}{lemma.5.19}{}}
\newlabel{thm:approximation}{{5.20}{45}{\protect {\cite [pp.~9]{liStochasticModifiedEquations2019}}}{theorem.5.20}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{lemma:moment_bound}{{5.21}{46}{\protect {\cite [Lemma 29]{liStochasticModifiedEquations2019}}}{lemma.5.21}{}}
\newlabel{thm:second_order}{{5.22}{46}{\autocite {liStochasticModifiedEquations2019}}{theorem.5.22}{}}
\newlabel{eq:second_order_sde}{{5.16}{46}{\autocite {liStochasticModifiedEquations2019}}{equation.5.16}{}}
\newlabel{eq:second_order_convergence}{{5.17}{46}{\autocite {liStochasticModifiedEquations2019}}{equation.5.17}{}}
\newlabel{eq:u_lipschitz}{{5.18}{47}{Stochastic gradient descent}{equation.5.18}{}}
\newlabel{eq:sigma_inequality}{{5.19}{47}{Stochastic gradient descent}{equation.5.19}{}}
\newlabel{eq:mapping_lipschitz}{{5.20}{47}{Stochastic gradient descent}{equation.5.20}{}}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\newlabel{cor:first_order}{{5.23}{49}{\protect {\cite {liStochasticModifiedEquations2019}}}{theorem.5.23}{}}
\newlabel{eq:first_order_sde}{{5.21}{49}{\protect {\cite {liStochasticModifiedEquations2019}}}{equation.5.21}{}}
\newlabel{eq:first_order_convergence}{{5.22}{49}{\protect {\cite {liStochasticModifiedEquations2019}}}{equation.5.22}{}}
\newlabel{ex:quadratic_smde}{{5.24}{49}{\autocite {liStochasticModifiedEquations2019}}{example.5.24}{}}
\newlabel{eq:objective_function}{{5.23}{49}{\autocite {liStochasticModifiedEquations2019}}{equation.5.23}{}}
\newlabel{eq:ornstein_solution}{{5.24}{50}{\autocite {liStochasticModifiedEquations2019}}{equation.5.24}{}}
\newlabel{eq:expected_value_ornstein}{{5.25}{50}{\autocite {liStochasticModifiedEquations2019}}{equation.5.25}{}}
\abx@aux@cite{0}{dengMNISTDatabaseHandwritten2012}
\abx@aux@segm{0}{0}{dengMNISTDatabaseHandwritten2012}
\abx@aux@cite{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@segm{0}{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@cite{0}{aggarwalNeuralNetworksDeep2018}
\abx@aux@segm{0}{0}{aggarwalNeuralNetworksDeep2018}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@cite{0}{heIdentityMappingsDeep2016}
\abx@aux@segm{0}{0}{heIdentityMappingsDeep2016}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\newlabel{eq:analytical_expected_value}{{5.26}{51}{\autocite {liStochasticModifiedEquations2019}}{equation.5.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Stochastic modified equations in deep learning}{51}{section.6}\protected@file@percent }
\newlabel{sec:smdedl}{{6}{51}{Stochastic modified equations in deep learning}{section.6}{}}
\abx@aux@cite{0}{goodfellowDeepLearning2016}
\abx@aux@segm{0}{0}{goodfellowDeepLearning2016}
\abx@aux@cite{0}{dengMNISTDatabaseHandwritten2012}
\abx@aux@segm{0}{0}{dengMNISTDatabaseHandwritten2012}
\abx@aux@cite{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@segm{0}{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@cite{0}{goodfellowDeepLearning2016}
\abx@aux@segm{0}{0}{goodfellowDeepLearning2016}
\abx@aux@cite{0}{aggarwalNeuralNetworksDeep2018}
\abx@aux@segm{0}{0}{aggarwalNeuralNetworksDeep2018}
\abx@aux@cite{0}{paszkePyTorchImperativeStyle2019}
\abx@aux@segm{0}{0}{paszkePyTorchImperativeStyle2019}
\abx@aux@cite{0}{dengMNISTDatabaseHandwritten2012}
\abx@aux@segm{0}{0}{dengMNISTDatabaseHandwritten2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Deep learning architectures}{52}{subsection.6.1}\protected@file@percent }
\newlabel{subsec:deep_learning_architectures}{{6.1}{52}{Deep learning architectures}{subsection.6.1}{}}
\newlabel{eq:nn}{{6.1}{52}{Feed foward neural network, weight matrix, bias \protect {\cite [pp.~19]{aggarwalNeuralNetworksDeep2018}}}{equation.6.1}{}}
\abx@aux@cite{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@segm{0}{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@cite{0}{lecunConvolutionalNetworksImages1995}
\abx@aux@segm{0}{0}{lecunConvolutionalNetworksImages1995}
\abx@aux@cite{0}{goodfellowDeepLearning2016}
\abx@aux@segm{0}{0}{goodfellowDeepLearning2016}
\abx@aux@cite{0}{goodfellowDeepLearning2016}
\abx@aux@segm{0}{0}{goodfellowDeepLearning2016}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2012}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Datasets}{53}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Fully connected neural network}{53}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Fully connected neural network for MNIST\relax }}{53}{table.caption.14}\protected@file@percent }
\newlabel{tab:fc_architecture}{{1}{53}{Fully connected neural network for MNIST\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}AlexNet}{53}{subsubsection.6.1.3}\protected@file@percent }
\abx@aux@cite{0}{heIdentityMappingsDeep2016}
\abx@aux@segm{0}{0}{heIdentityMappingsDeep2016}
\abx@aux@cite{0}{ioffeBatchNormalizationAccelerating2015}
\abx@aux@segm{0}{0}{ioffeBatchNormalizationAccelerating2015}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces AlexNet for CIFAR-10\relax }}{54}{table.caption.15}\protected@file@percent }
\newlabel{tab:alexnet_architecture}{{2}{54}{AlexNet for CIFAR-10\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.4}PreResNet}{54}{subsubsection.6.1.4}\protected@file@percent }
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces PreResNet block with channel parameter $\in \{16,32,64\}$\relax }}{55}{table.caption.16}\protected@file@percent }
\newlabel{tab:preresnet_basic_block}{{3}{55}{PreResNet block with channel parameter $\in \{16,32,64\}$\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Simulating stochastic modified equations}{55}{subsection.6.2}\protected@file@percent }
\abx@aux@cite{0}{golubMatrixComputations1996}
\abx@aux@segm{0}{0}{golubMatrixComputations1996}
\abx@aux@cite{0}{wuNoisyGradientDescent2020a}
\abx@aux@segm{0}{0}{wuNoisyGradientDescent2020a}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces PreResNet-32 architecture\relax }}{56}{table.caption.17}\protected@file@percent }
\newlabel{tab:preresnet_architecture}{{4}{56}{PreResNet-32 architecture\relax }{table.caption.17}{}}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\newlabel{thm:svag}{{6.6}{57}{\cite {liValidityModelingSGD2021}}{theorem.6.6}{}}
\abx@aux@cite{0}{dengMNISTDatabaseHandwritten2012}
\abx@aux@segm{0}{0}{dengMNISTDatabaseHandwritten2012}
\abx@aux@cite{0}{heDeepResidualLearning2016a}
\abx@aux@segm{0}{0}{heDeepResidualLearning2016a}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@cite{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@segm{0}{0}{krizhevskyLearningMultipleLayers2009}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{paszkePyTorchImperativeStyle2019}
\abx@aux@segm{0}{0}{paszkePyTorchImperativeStyle2019}
\abx@aux@cite{0}{bisongGoogleColaboratory2019}
\abx@aux@segm{0}{0}{bisongGoogleColaboratory2019}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2012}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Experiments}{58}{subsection.6.3}\protected@file@percent }
\newlabel{subsec:experiments}{{6.3}{58}{Experiments}{subsection.6.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Fully connected network on MNIST}{58}{subsubsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}AlexNet on CIFAR-10}{58}{subsubsection.6.3.2}\protected@file@percent }
\abx@aux@cite{0}{heIdentityMappingsDeep2016}
\abx@aux@segm{0}{0}{heIdentityMappingsDeep2016}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@segm{0}{0}{krizhevskyImageNetClassificationDeep2012}
\abx@aux@cite{0}{goyalAccurateLargeMinibatch2018}
\abx@aux@segm{0}{0}{goyalAccurateLargeMinibatch2018}
\abx@aux@cite{0}{krizhevskyOneWeirdTrick2014}
\abx@aux@segm{0}{0}{krizhevskyOneWeirdTrick2014}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}PreResNet on CIFAR-10}{59}{subsubsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Discussion}{59}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Applications}{59}{section.7}\protected@file@percent }
\newlabel{sec:applications}{{7}{59}{Applications}{section.7}{}}
\abx@aux@cite{0}{orvietoContinuoustimeModelsStochastic2019a}
\abx@aux@segm{0}{0}{orvietoContinuoustimeModelsStochastic2019a}
\abx@aux@cite{0}{liValidityModelingSGD2021}
\abx@aux@segm{0}{0}{liValidityModelingSGD2021}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2017}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2017}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{61}{section.8}\protected@file@percent }
\abx@aux@cite{0}{evansPartialDifferentialEquations2010}
\abx@aux@segm{0}{0}{evansPartialDifferentialEquations2010}
\abx@aux@cite{0}{evansPartialDifferentialEquations2010}
\abx@aux@segm{0}{0}{evansPartialDifferentialEquations2010}
\abx@aux@cite{0}{evansPartialDifferentialEquations2010}
\abx@aux@segm{0}{0}{evansPartialDifferentialEquations2010}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{liStochasticModifiedEquations2019}
\abx@aux@segm{0}{0}{liStochasticModifiedEquations2019}
\abx@aux@cite{0}{evansPartialDifferentialEquations2010}
\abx@aux@segm{0}{0}{evansPartialDifferentialEquations2010}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{68}{appendix.A}\protected@file@percent }
\newlabel{lemma:inequality}{{A.1}{68}{}{lemma.A.1}{}}
\newlabel{eq:inequality}{{A.1}{68}{}{equation.A.1}{}}
\newlabel{def:mollifier}{{A.2}{68}{Mollifier, \protect {\cite {evansPartialDifferentialEquations2010}}}{definition.A.2}{}}
\newlabel{lemma:mollifiers}{{A.3}{68}{\protect {\cite [pp. 630]{evansPartialDifferentialEquations2010}}}{lemma.A.3}{}}
\abx@aux@cite{0}{koenigsbergerAnalysis2006}
\abx@aux@segm{0}{0}{koenigsbergerAnalysis2006}
\newlabel{lemma:mollifier_bound}{{A.4}{69}{\protect {\cite [Lemma 30]{liStochasticModifiedEquations2019}}}{lemma.A.4}{}}
\newlabel{thm:dominated_convergence}{{A.5}{69}{Dominated convergence theorem, \protect {\cite [pp.~648]{evansPartialDifferentialEquations2010}}}{theorem.A.5}{}}
\newlabel{lemma:linear_growth}{{A.6}{69}{}{lemma.A.6}{}}
\newlabel{lem:sum_positive_definite}{{A.7}{69}{}{lemma.A.7}{}}
\newlabel{thm:taylor}{{A.8}{69}{\protect {\cite [pp.~65]{koenigsbergerAnalysis2006}}}{theorem.A.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mean square loss function for the linear regression problem with $n=1000$ samples and weight $w=4$ and bias $b=1$.\relax }}{70}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:quadratic_loss_function}{{1}{70}{Mean square loss function for the linear regression problem with $n=1000$ samples and weight $w=4$ and bias $b=1$.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Evolution of weight $w$ and bias $b$ for the mean square loss using GD for the learning rates $\eta \in \{0.0005,0.0001,0.00005,0.00001\}$.\relax }}{71}{figure.caption.6}\protected@file@percent }
\newlabel{fig:learning_rates}{{2}{71}{Evolution of weight $w$ and bias $b$ for the mean square loss using GD for the learning rates $\eta \in \{0.0005,0.0001,0.00005,0.00001\}$.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Scaled evolution of weight $w$ and bias $b$ for the mean square loss using GD for the learning rates $\eta \in \{0.0005,0.0001,0.00005,0.00001\}$.\relax }}{71}{figure.caption.7}\protected@file@percent }
\newlabel{fig:scaled_weights_biases}{{3}{71}{Scaled evolution of weight $w$ and bias $b$ for the mean square loss using GD for the learning rates $\eta \in \{0.0005,0.0001,0.00005,0.00001\}$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Error between GD iterates and the solution of first and second order modified equation for linear regression problem with $n=1000$ samples, dimension $d=5$ and final time $T=1$ for descreasing learning rates $\eta $.\relax }}{72}{figure.caption.8}\protected@file@percent }
\newlabel{fig:linear_regression_error}{{4}{72}{Error between GD iterates and the solution of first and second order modified equation for linear regression problem with $n=1000$ samples, dimension $d=5$ and final time $T=1$ for descreasing learning rates $\eta $.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Samples of the SGD iterates for the linear regression problem and the mean of $m=10000$ independent runs for the learning rate $\eta =0.1$.\relax }}{73}{figure.caption.9}\protected@file@percent }
\newlabel{fig:sgd_linear_fit}{{5}{73}{Samples of the SGD iterates for the linear regression problem and the mean of $m=10000$ independent runs for the learning rate $\eta =0.1$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Histogram of the weight $w$ for the linear regression problem at 12.5\%, 25\%, 50\% and 100\% of the total iterations for a learning rate of $\eta =0.1$.\relax }}{73}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sgd_weight_histogram}{{6}{73}{Histogram of the weight $w$ for the linear regression problem at 12.5\%, 25\%, 50\% and 100\% of the total iterations for a learning rate of $\eta =0.1$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Quantile-quantile plot for the normal distribution and the sampled values of the weight $w$ at the last iteration of SGD for $m=10000$ independent runs.\relax }}{74}{figure.caption.11}\protected@file@percent }
\newlabel{fig:sgd_weight_qq}{{7}{74}{Quantile-quantile plot for the normal distribution and the sampled values of the weight $w$ at the last iteration of SGD for $m=10000$ independent runs.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Expected loss value for the first order stochastic modified equation and mean of SGD iterates for the learning rates $\eta \in \{0.0625, 0.125, 0.25, 0.5\}$.\relax }}{75}{figure.caption.12}\protected@file@percent }
\newlabel{fig:sde_sgd}{{8}{75}{Expected loss value for the first order stochastic modified equation and mean of SGD iterates for the learning rates $\eta \in \{0.0625, 0.125, 0.25, 0.5\}$.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Logarithmic plot of the maximum of the absolute difference of the analytical expected value of the first order stochastic modified equation and the mean of $m=1000$ independent SGD runs.\relax }}{76}{figure.caption.13}\protected@file@percent }
\newlabel{fig:convergence_rate}{{9}{76}{Logarithmic plot of the maximum of the absolute difference of the analytical expected value of the first order stochastic modified equation and the mean of $m=1000$ independent SGD runs.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.12$ and a batch size $B=128$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{77}{figure.caption.18}\protected@file@percent }
\newlabel{fig:fc_128_012}{{10}{77}{Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.12$ and a batch size $B=128$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.48$ and a batch size $B=512$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{78}{figure.caption.19}\protected@file@percent }
\newlabel{fig:fc_512_048}{{11}{78}{Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.48$ and a batch size $B=512$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.48$ and a batch size $B=512$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{79}{figure.caption.20}\protected@file@percent }
\newlabel{fig:alexnet_128_04}{{12}{79}{Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.48$ and a batch size $B=512$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces PreResNet-32 on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.12$ and a batch size $B=128$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{80}{figure.caption.21}\protected@file@percent }
\newlabel{fig:preresnet_128_012}{{13}{80}{PreResNet-32 on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.12$ and a batch size $B=128$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces PreResNet-32 on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.96$ and a batch size $B=1024$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{81}{figure.caption.22}\protected@file@percent }
\newlabel{fig:preresnet1024_lr096}{{14}{81}{PreResNet-32 on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.96$ and a batch size $B=1024$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }{figure.caption.22}{}}
\abx@aux@read@bbl@mdfivesum{AA4CB6B1F5CFD6074728CE0C7852D2E9}
\abx@aux@defaultrefcontext{0}{aggarwalNeuralNetworksDeep2018}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{ahmadTextbookOrdinaryDifferential2015}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{allen-zhuConvergenceTheoryDeep2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{baldiEquazioniDifferenzialiStocastiche2000}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{barrettImplicitGradientRegularization2021}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{bisongGoogleColaboratory2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{bottouOptimizationMethodsLargeScale2018}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{boydConvexOptimization2004}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{brownLanguageModelsAre2020}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{capassoIntroductionContinuousTimeStochastic2012}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{dengImageNetLargescaleHierarchical2009}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{dengMNISTDatabaseHandwritten2012}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{duchiAdaptiveSubgradientMethods2011}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{durrettProbabilityTheoryExamples2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{eAppliedStochasticAnalysis2021}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{evansPartialDifferentialEquations2010}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{geoffreyhintonnitishsrivastavaandkevinswer-NeuralNetworksMachine2012}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{glorotUnderstandingDifficultyTraining2010}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{golubMatrixComputations1996}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{goodfellowDeepLearning2016}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{gowerSGDGeneralAnalysis2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{goyalAccurateLargeMinibatch2018}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{hairerGeometricNumericalIntegration2013}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{heDeepResidualLearning2016a}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{heDelvingDeepRectifiers2015a}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{heIdentityMappingsDeep2016}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{ioffeBatchNormalizationAccelerating2015}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{jumperHighlyAccurateProtein2021}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{kingmaAdamMethodStochastic2017}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{kloedenNumericalSolutionStochastic2013}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{koenigsbergerAnalysis2006}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{krizhevskyLearningMultipleLayers2009}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{krizhevskyOneWeirdTrick2014}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{krizhevskyImageNetClassificationDeep2012}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{lecunConvolutionalNetworksImages1995}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{liStochasticModifiedEquations2017}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{liStochasticModifiedEquations2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{liConvergenceStochasticGradient2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{liValidityModelingSGD2021}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{lycheNumericalLinearAlgebra2020}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{mertikopoulosAlmostSureConvergence2020}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{moulinesNonAsymptoticAnalysisStochastic2011}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{nesterovLecturesConvexOptimization2018}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{nguyenSGDHogwildConvergence2018}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{nocedalNumericalOptimization2006}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{oksendalStochasticDifferentialEquations2003}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{orvietoContinuoustimeModelsStochastic2019a}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{paszkePyTorchImperativeStyle2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{polyakMethodsSpeedingConvergence1964}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{rameshHierarchicalTextConditionalImage2022}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{robbinsStochasticApproximationMethod1951}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{romanAdvancedLinearAlgebra2010}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{santambrogioEuclideanMetricWasserstein2017}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{schrittwieserMasteringAtariGo2020}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{sebbouhAlmostSureConvergence2021}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{simsekliTailIndexAnalysisStochastic2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{smithOriginImplicitRegularization2021}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{uhlenbeckTheoryBrownianMotion1930}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{vaswaniFastFasterConvergence2019}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{wilsonMarginalValueAdaptive2017}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{wuNoisyGradientDescent2020a}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{zhouTheoreticallyUnderstandingWhy2020}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{zhouStochasticMirrorDescent2017}{nty/global//global/global}
\gdef \@abspage@last{81}
