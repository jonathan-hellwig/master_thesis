\babel@toc {english}{}\relax 
\contentsline {figure}{\numberline {1}{\ignorespaces Mean square loss function for the linear regression problem with $n=1000$ samples and weight $w=4$ and bias $b=1$.\relax }}{70}{figure.caption.5}%
\contentsline {figure}{\numberline {2}{\ignorespaces Evolution of weight $w$ and bias $b$ for the mean square loss using GD for the learning rates $\eta \in \{0.0005,0.0001,0.00005,0.00001\}$.\relax }}{71}{figure.caption.6}%
\contentsline {figure}{\numberline {3}{\ignorespaces Scaled evolution of weight $w$ and bias $b$ for the mean square loss using GD for the learning rates $\eta \in \{0.0005,0.0001,0.00005,0.00001\}$.\relax }}{71}{figure.caption.7}%
\contentsline {figure}{\numberline {4}{\ignorespaces Error between GD iterates and the solution of first and second order modified equation for linear regression problem with $n=1000$ samples, dimension $d=5$ and final time $T=1$ for descreasing learning rates $\eta $.\relax }}{72}{figure.caption.8}%
\contentsline {figure}{\numberline {5}{\ignorespaces Samples of the SGD iterates for the linear regression problem and the mean of $m=10000$ independent runs for the learning rate $\eta =0.1$.\relax }}{73}{figure.caption.9}%
\contentsline {figure}{\numberline {6}{\ignorespaces Histogram of the weight $w$ for the linear regression problem at 12.5\%, 25\%, 50\% and 100\% of the total iterations for a learning rate of $\eta =0.1$.\relax }}{73}{figure.caption.10}%
\contentsline {figure}{\numberline {7}{\ignorespaces Quantile-quantile plot for the normal distribution and the sampled values of the weight $w$ at the last iteration of SGD for $m=10000$ independent runs.\relax }}{74}{figure.caption.11}%
\contentsline {figure}{\numberline {8}{\ignorespaces Expected loss value for the first order stochastic modified equation and mean of SGD iterates for the learning rates $\eta \in \{0.0625, 0.125, 0.25, 0.5\}$.\relax }}{75}{figure.caption.12}%
\contentsline {figure}{\numberline {9}{\ignorespaces Logarithmic plot of the maximum of the absolute difference of the analytical expected value of the first order stochastic modified equation and the mean of $m=1000$ independent SGD runs.\relax }}{76}{figure.caption.13}%
\contentsline {figure}{\numberline {10}{\ignorespaces Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.12$ and a batch size $B=128$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{77}{figure.caption.18}%
\contentsline {figure}{\numberline {11}{\ignorespaces Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.48$ and a batch size $B=512$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{78}{figure.caption.19}%
\contentsline {figure}{\numberline {12}{\ignorespaces Fully connected neural network on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.48$ and a batch size $B=512$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{79}{figure.caption.20}%
\contentsline {figure}{\numberline {13}{\ignorespaces PreResNet-32 on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.12$ and a batch size $B=128$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{80}{figure.caption.21}%
\contentsline {figure}{\numberline {14}{\ignorespaces PreResNet-32 on CIFAR-10 with SVAG for the scaling factors $l=1,2,4,8,16$, an initial learning rate $\eta =0.96$ and a batch size $B=1024$ for $80l$ epochs for training. The case $l=1$ corresponds to the SGD iterates.\relax }}{81}{figure.caption.22}%
