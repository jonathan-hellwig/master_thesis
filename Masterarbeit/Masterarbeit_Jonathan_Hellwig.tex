\documentclass[12pt]{article}

% Layout
\usepackage[a4paper,includeheadfoot,margin=2.54cm]{geometry}

% Spracheinstellungen, alle Sprachen laden, letzte ist aktiv
\usepackage[english]{babel}
\usepackage{csquotes}
% hilfreiche Pakete der AMS (American Mathematical Society) laden
\usepackage{amsmath}
\usepackage{amssymb}

% Sätze, Lemmata, ..
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% Formelnummerierung
\numberwithin{equation}{section}

% moderne Literaturverwaltung mittels Biber, erstzt BibLaTeX,
% kann UTF-8
\usepackage{biblatex}
\addbibresource{Masterarbeit_Jonathan_Hellwig.bib}

% Einbinden von Grafik, neue Version
\usepackage{graphicx}

% Unterabbildungen
\usepackage{subcaption}

% TikZ ist kein Zeichenprogramm (doch)
\usepackage{tikz}

% listings bindet Code ein
\usepackage{listings}
\definecolor{hellgrau}{rgb}{0.90,0.90,0.90}
\definecolor{commentcol}{rgb}{0.0823,.4902,0.0}
\lstset{language=Python,
        basicstyle={\footnotesize\ttfamily},
        keywordstyle={\sffamily\bfseries},
        tabsize=2,
        numbers=left,
        numberstyle=\tt,
        stepnumber=1,
        numbersep=7pt,
        breaklines=true,
        frame=single,
        frameround=ffff,
        commentstyle=\color{commentcol},
        backgroundcolor=\color{hellgrau},
        literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {Ã}{{\~A}}1 {ã}{{\~a}}1 {Õ}{{\~O}}1 {õ}{{\~o}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {·}{{$\cdot$}}1
}

% Hyperlinks in Texten
\usepackage{hyperref}
\hypersetup{%
  pdftitle     = {Modeling of stochastic optimization algorithms with stochastic differential equations},
  pdfsubject   = {Masterarbeit von Jonathan Hellwig},
  pdfkeywords  = {Masterarbeit, neuronale Netze},
  pdfauthor    = {\textcopyright\ Jonathan Hellwig 2022},
  linkcolor    = red,     % links to same page
  urlcolor     = blue,     % links to URLs
  citecolor    = green!50!black,     % links to citations
  breaklinks   = true,       % links may (line) break
  colorlinks   = true,
  citebordercolor=0 0 0,  % color for \cite
  filebordercolor=0 0 0,
  linkbordercolor=0 0 0,
  menubordercolor=0 0 0,
  urlbordercolor=0 0 0,
  pdfhighlight=/P,   % moeglich /I, /P, ...
  pdfborder=0 0 0,   % keine Box um die Links!
}
% nützliche Kurzkommandos für natürliche, ..., reelle, .. Zahlen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}

% ein paar dick gedruckte Buchstaben
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}

% Beginn des Dokumentes
\begin{document}

% Titelseite
\thispagestyle{empty}

\begin{center}
  \includegraphics[height=2.3cm]{pics/MATH_de}
  \hfill
  \includegraphics[height=2.3cm]{pics/TUHH_de}
\end{center}

\vspace*{5em}

\begin{center}
  {\Huge
    \textsc{Relations between variants of stochastic gradient descent and stochastic differential equations}\\[2em]
  }
  {\LARGE
    Masterarbeit
  }

  \vspace*{2em}

  {\Large
    von\\
    Jonathan Hellwig\\
    aus Hamburg\\
    Matrikelnummer: 7381194\\
    Studiengang: Technomathematik\\
  }
\end{center}

\vfill
\begin{center}
  \today
\end{center}
\vfill

\begin{tabbing}
  % längste Zeile zuerst duplizieren, mit kill löschen
  Erstprüferin: \= Dr. Jens-Peter M. Zemke\kill
  Erstprüferin: \> Dr. Jens-Peter M. Zemke\\
  Zweitprüfer:  \> Dr. Jens-Peter M. Zemke\\
  Betreuer:     \> Dr. Jens-Peter M. Zemke\\
\end{tabbing}
\newpage
% this page intentionally left blank
\thispagestyle{empty}
\mbox{}
\newpage

\section*{Eidestattliche Erklärung}

Hiermit versichere ich an Eides statt, dass ich die vorliegende
Bachelorarbeit mit dem Titel
\begin{quote}
  „Titel der Bachelorarbeit“  
\end{quote}
selbständig und ohne unzulässige fremde Hilfe verfasst habe. Ich habe
keine anderen als die angegebenen Quellen und Hilfsmittel benutzt,
sowie wörtliche und sinngemäße Zitate kenntlich gemacht. Die Arbeit
hat in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde
vorgelegen. Ich versichere, dass die eingereichte schriftliche Fassung
der auf dem beigefügten Medium gespeicherten Fassung entspricht.

\vspace*{3em}

\begin{tabbing}
  \rule{.4\textwidth}{1pt} \hspace*{.2\textwidth}
  \= \rule{.4\textwidth}{1pt} \\
  Ort, Datum \> Unterschrift
\end{tabbing}

\newpage
\mbox{}
\newpage
% TOC - Table of Contents
\tableofcontents
\newpage
\listoffigures
\newpage

\section{Motivation}
\label{sec:Motivation}
\subsection{Common activation functions}
\section{Background on Optimization}
\label{sec:Optimization}
% This chapter covers the background of optimization theory. It goes into the formal definition of optimization problems, the necessary and sufficient conditions for minimizers, and covers iterative methods for obtaining such minimizers.
In recent years, large-scale machine learning using deep neural networks has shown to be applicable across domains. Models like DALLE2 (citation) for image generation, GPT-3 for text generation (citation), MuZero for solving games (citation) and AlphaFold for prediction of protein geometry (citation) are prominent examples.
One challenging area of research remains in understanding why these models work as well as they do. Underlying the construction of these models is an optimization problem. This chapter covers the mathematical basis for constructing and solving these optimization problems.

\subsection{Formal problem definition}
% Risk minimization
% Formal definition of data distribution
This section introduces a general optimization problem that is approximated in practice. The goal is to construct a model that represents a set of data in an optimal way. Further, a model should generalize well, i.e. deliver good performance on unseen data. More formally, a model is a mapping $m_w$ that depends on some set of parameters $w \in \mathbb{R}^d$ and maps input features $x \in X$ to output features $y \in Y$. The data points $(x,y) \in X \times Y$ follow a joint probability distribution $p(x,y)$. The mapping $l$ quantifies the distance between a prediction $m_{w}$ and is called loss function. Finding a model that generalizes well can be understood as minimizing the \emph{expected risk}
\begin{equation}
  \min_{w \in \mathbb{R}^d} R(w) = \mathbb{E}[l(m_{w}(x),y)].
\end{equation}
In practice, the distribution $p(x,y)$ is unknown and the computation of the expected risk is not possible. Defining the minimization problem in terms of the sample average also known as the \emph{empirical risk} allow the problem to be solved. For a sample of data points $\{(x_i, y_i)\}$ the empirical risk is defined as 
\begin{equation}
  \label{eq:emperical_risk}
  \min_{w \in \mathbb{R}^d}  R_n(w) = \frac{1}{n}\sum_{i=1}^n[l(m_{w}(x_i),y_i)].
\end{equation}
A model $m_{w}$ generalizes well if $|R - R_n|$ is small. This notion will become relevant in the following section in the discussion of iterative methods for solving (\ref{eq:emperical_risk}). Whether a model generalizes well both depends on the model architecture and the procedure to obtain the parameters $w$. 
The selection of the loss function is contingent on the kind of data. For regression problems the \emph{mean square loss} is used, for multi-class classification problems the \emph{cross entropy loss} is used.

\subsection{Characterization of minimizers}
First, we have to introduce the concept of a minimizer to understand how the empirical risk minimization problem can be solved. In the following we consider a more general formulation of the above problem that can be found in optimization textbooks. Consider the unconstrained minimization problem
\begin{equation}
  \min_{w \in \mathbb{R}^d} f(w),
\end{equation}
where $f:\mathbb{R}^d \rightarrow \mathbb{R}$.
For this problem we define two notions of minimizers.
\begin{definition}
  A value $w^\star$ is called global minimizer if 
  \begin{equation}
    f(w^\star) \leq f(w)
  \end{equation} 
  for all $w \in \mathbb{R}^d$.
\end{definition}
\begin{definition}
  A value $w^\star$ is called local minimizer if there exists an $\epsilon > 0$ such that
  \begin{equation}
    f(w^\star) \leq f(w)
  \end{equation} 
  for all $w \in \mathbb{R}^d, ||w-w^\star|| < \epsilon$.
\end{definition}
Notice that every global minimizer is also a local minimizer. It is difficult to obtain global minimizers in deep learning settings. One key aspect of neural networks is that they are differentiable. If we make some smoothness assumptions on the function $f$, we can obtain sufficient and necessary conditions that provide some insight on local minimizers.
\begin{theorem}[\protect{\cite[page~5]{kushner2003stochastic}}]
  If $w^\star$ is a local minimizer and $f$ is continuously differentiable in an open neighborhood of $w^\star$, then $\nabla f(w^\star) = 0$.
\end{theorem}
\begin{theorem}
  If $w^\star$ is a local minimizer of $f$ and $\nabla^2 f$ exists and is continuous in an open neighborhood of $w^\star$, then $\nabla f(w^\star)$ and $\nabla^2f(w^\star)$ is positive semidefinite.
\end{theorem}
The first theorem builds the foundation for all commonly used optimization techniques in deep learning. The gradient can be  efficiently calculated with a procedure called backpropagation which makes use of the recursive structure of neural networks. Finding candidates for local minima boils down to finding $w^\star$ such that $\nabla f(w^\star) = 0$. This leads to the following definition.
\begin{definition}
  A solution $w^\star \in \mathbb{R}^d$ to the equation
  \begin{equation}
  \label{eq:StationaryPoint}
    \nabla f(w^\star) = 0
  \end{equation}
  is called stationary point.
\end{definition}
It is unclear how to obtain stationary points for neural networks, since $\nabla f$ is usually highly nonlinear. The next section will cover iterative methods to obtain approximate solutions to the stationary point problem.

\subsection{Iterative methods}
For simple functions it is possible to determine the solution to (\ref*{eq:StationaryPoint}) in analytical form. However, state of the art neural networks contain trillions of parameters and finding a solution analytically is infeasible. Iterative methods provide a way to start with an initial guess and improve upon that guess gradually. In particular, iterative methods considered in this thesis make use of gradient information for updating the parameters $w \in \mathbb{R}^d$. Before defining iterative descent methods, we introduce the concept of descent directions. 
\begin{definition}
  Let $f$ be a continuously differentiable function. Then a vector $g \in \mathbb{R}^d$ is called \emph{descent direction} at $w \in \mathbb{R}^d$ if it satisfies 
  \begin{equation}
    g^T \nabla f(w) = 0.
  \end{equation} 
\end{definition}
It is easy to show that given an initial value $w \in \mathbb{R}^d$ and a descent direction $g \in \mathbb{R}^d$ we have $f(w) > f(w + g)$. This motivates the definition of one-step iterative methods.
\begin{definition}
  For a given initial value $x_0 \in \mathbb{R}^d$ a sequence defined by
\begin{equation}
  w^{(k+1)} = w^{(k)} + \eta^{(k)} g(w^{(k)}),
\end{equation}
  where $\{\eta^{(k)}\}_{k=1}^\infty$ is called step size and $\{g(w^{(k)})\}_{k=1}^\infty$ is a sequence of descent directions is called \emph{one-step descent method}.
\end{definition}

The most quintessential descent method for $g^{(k)} = - \nabla f(w^{(k)})$ is given by
\begin{equation}
  \label{eq:gradient_descent}
  w^{(k+1)} = w^{(k)} - \eta^{(k)} \nabla f(w^{(k)}).
\end{equation}
It is easy to see that we have ${g^{(k)}}^T \nabla f(w^{(k)}) = -|| f(w^{(k)}) ||^2 < 0$ as long as the iterates have not converged to a stationary point. This iterative scheme is called \emph{gradient descent method} (GD) and forms the basis for many of the iterative schemes used in deep learning practice. It is important to note that after defining such an iterative scheme it is unclear whether it actually converges to a minimizer. In the later sections of this chapter we will discuss the classes of functions for which convergence can be established and how they relate to deep learning practice. Before diving into the theoretical analysis of descent methods let us consider an extension to the gradient descent method. \\ 
Notice that in (\ref{eq:gradient_descent}) the computation of each iterate requires the computation of the gradient $\nabla f$. For objective functions of the form $f(w) = \frac{1}{n} \sum_{i=1}^n f_i(w)$ this requires the computation of each $\nabla f_i(w)$ individually. Remember that in deep learning settings $f_i$ represents the loss $l(m_w(x_i), y_i)$ with respect to a single data point $(x_i, y_i)$. When the number of data points is large, computing the full gradient $\nabla f$ becomes prohibitively expensive. In fact, ImageNet (citation), a benchmark dataset for image classification, contains over 1.2 million images. If we treat each individual gradient $\nabla f_i(w)$ as a realization of a random variable $\nabla f_{\gamma}(w)$ with expected value $\mathbb{E}(\nabla f_{\gamma}(w)) = \nabla f(w)$, we can estimate $\nabla f(w)$ by calculating the sample average. In particular, this leads to the definition of the following random process:
\begin{equation}
  \label{eq:stochastic_gradient_descent}
  w^{(k+1)} = w^{(k)} - \eta^{(k)} \nabla f_{\gamma_k}(w^{(k)}),
\end{equation}
where $\gamma_k$ are independently distributed random variables with the same distribution as $\gamma$. This iteration method is called \emph{stochastic gradient descent} (SGD).
In view of (\ref{eq:stochastic_gradient_descent}), let us define a class of one-step stochastic descent methods:
\begin{definition}
  For a given initial value $x_0 \in \mathbb{R}^d$ the stochastic process defined by
\begin{equation}
  w^{(k+1)} = w^{(k)} + \eta^{(k)} g(w^{(k)}, \gamma_k),
\end{equation}
  where $g:\mathbb{R}^d \times \Gamma \rightarrow \mathbb{R}^d$ is a measurable function, is called \emph{stochastic one-step descent method}.
\end{definition}
This class of stochastic iterative methods encompasses many commonly used optimization methods. Note that while introducing stochasticity to the optimization process allows for increased computational speed, it makes analysis of a method more difficult. In fact, for the analysis of stochastic methods it is not sufficient to only consider the expected value $\mathbb{E}(\nabla f_{\gamma}(w))$ of the gradients. As we will see in the following sections, the variance $\mathbb{V}(\nabla f_{\gamma}(w)) = \mathbb{E}((\nabla f_{\gamma}(w) - \nabla f(w)){(\nabla f_{\gamma}(w) - \nabla f(w))}^T)$ of the gradients also needs to be considered. (rephrase) \\
The SGD method serves as a prototype for a lot of optimization methods. (rephrase) A simple extension is \emph{stochastic gradient descent with momentum}. 
\begin{align*}
  w^{(k+1)} &= \mu v^{(k)} - \eta \nabla f_{\gamma_k}(w^{(k)}) \\
  v^{(k+1)} &= w^{(k)} + v^{(k)},
\end{align*}
where $\mu \in (0,1)$ is called \emph{momentum parameter}. (citation)
\subsection{Convergence analysis}
% \begin{definition}
%   An iterative method ($\{\eta^{(k)}\}_{k=1}^\infty$, $\{g^{(k)}\}_{k=1}^\infty$) is said to converge linearly if there exists a constant $1 > C > 0$ such that 
%   \begin{equation}
%     \lim_{k \rightarrow \infty} \frac{||w^{(k+1)} - w^\star||}{||w^{(k)} - w^\star||} < M
%   \end{equation}
%   and it is said to converge sublinearly if 
%   \begin{equation}
%     \lim_{k \rightarrow \infty} \frac{||w^{(k+1)} - w^\star||}{||w^{(k)} - w^\star||} = 1
%   \end{equation}
%   holds.
% \end{definition}
In this section, we will discuss some convergence results for SGD present in literature. First, we will introduce smoothness and convexity concepts.  

\begin{definition}
  \label{def:l_smooth}
  A function $f$ is said to be \emph{$L$-smooth} if its gradient are Lipschitz continuous, that is 
  \begin{equation}
    ||\nabla f(w) - \nabla f(v) || \leq L ||w-v||,
  \end{equation}
  for $w,v \in \mathbb{R}^d$.
\end{definition}
From the above definition we have the following lemma.
\begin{lemma}
  Let $f : \mathbb{R}^d \rightarrow \mathbb{R}$ be a $L$-smooth function. Then, we have 
  \begin{equation}
    f(w) \leq f(v) + \langle \nabla f(w), v - w \rangle + \frac{L}{2} || v - w ||^2,
  \end{equation}
  for all $v, w \in \mathbb{R}^d$.
\end{lemma}
\begin{definition}
  A function $f$ is said to be \emph{convex} if 
  \begin{equation}
    f(tx+(1-t)y) \leq tf(x)+(1-t)f(y),
  \end{equation}
  for all $x,y \in \mathbb{R}^n$ and $t \in [0,1]$.
\end{definition}

\begin{definition}
  A function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is said to be $\mu$-\emph{strongly convex} if
  \begin{equation}
    f(w) \geq f(v) + \langle \nabla f(w), v - w \rangle + \frac{\mu}{2} || v - w ||^2,
  \end{equation}
  for all $v, w \in \mathbb{R}^d$.
\end{definition}

\begin{assumption}
  \begin{enumerate}
    \item The sequence of iterates $w^{(k)}$ is contained in an open set over which $f$ is bounded from below by a scalar $f_{inf}$.
    \item There exist scalars $\mu_G \leq \mu > 0$ such that, for all $k \in \mathbb{N}$, \begin{align}
      \nabla {f(w^{(k)})}^T\mathbb{E}(g(w^{(k)}, \gamma_k)) &\leq || \nabla f (w^{(k)}) ||^2 \\
      || \mathbb{E}(g(w^{(k)}, \gamma_k))|| &\leq \mu_G ||\nabla f (w^{(k)})||.
    \end{align}
    \item There exist scalars $M \leq 0$ and $M_V \leq 0$ such that, for all $k \in \mathbb{N}$, \begin{equation}
      \mathbb{V}(g(w^{(k)}, \gamma_k)) \leq M + M_V ||\nabla f(w^{(k)}) ||^2
    \end{equation} 
  \end{enumerate}
\end{assumption}
(citation)

\begin{assumption}
  A function $f:\mathbb{R}^d \rightarrow \mathbb{R}$ is said to be $L$-\emph{smooth in expectation} with respect to the distribution $\mathcal{D}$ if there exists $L = L(f, \mathcal{D}) > 0$ such that 
  \begin{equation}
    \mathbb{E}(||\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)||^2) \leq 2 L(f(w) - f(w^\star)),
  \end{equation}
  for all $w \in \mathbb{R}^d$. 
\end{assumption}

% Conventional convergence results require exact line search.

% \cite{nesterov2003introductory}
% \begin{theorem}
%   Let $f$ satisfy assumptions ... and let $x^{(0)}$ be chosen such that 
%   \begin{equation}
%     r^{(0)} = ||x^{(k)} - x^\star|| \leq \frac{\bar{r}r^{(0)}}{\bar{r}-r^{(0)}}(1-\frac{2l}{L+3l}),
%   \end{equation}
%   where $\bar{r} = \frac{2l}{M}$.
% \end{theorem}

\begin{theorem}
  Let $f$ be convex and L-smooth and let $\{w^{(k)}\}$ be a sequence generated by the gradient descent method. It follows that 
  \begin{equation}
    f(w^{(k)}) - f(w^\star) \leq \frac{2L||w^{(0)} - w^\star||^2}{k-1}
  \end{equation}
\end{theorem}

\begin{theorem}
  Let $f$ be L-smooth. It follows
  \begin{equation}
    \min_{k=1,\dots,N} ||\nabla f(w^{(k)})||^2 \leq \frac{L^2}{N}||w^{(0)}-w^\star||^2
  \end{equation}
\end{theorem}

\begin{theorem}
  Let $f$ be L-smooth and $\mu$-strongly convex. From a given $w^{(0)}$ and $\frac{1}{L} \geq \alpha > 0$, the iterates generated by the gradient descent method converge according to
  \begin{equation}
    ||w^{(k+1)} - w^\star ||^2 \leq (1-\alpha \mu)^{k+1}||w^{(0)} - w^\star||^2
  \end{equation}
\end{theorem}


\section{Background SDE theory}
\label{sec:BackgroundSDETheory}
\subsection{Ito Integral}
\label{subsec:ItoIntegral}
\subsection{SDE Definition}
\label{subsec:SDEDefinition}

An Ito process 
\begin{equation}
  X_t = X_0 + \int_0^tb(s, X_s)ds + \int_0^t \sigma(s, X_s)dB_s
\end{equation}
\subsection{SDE Existence Uniqueness}
\label{subsec:SDEExistenceUniqueness}
\begin{theorem}
  Let $T > 0$ and $b(\cdot,\cdot):[0,T] \times \mathbb{R}^n \rightarrow \mathbb{R}^n$, $\sigma(\cdot,\cdot):[0,T] \times \mathbb{R}^n \rightarrow \mathbb{R}^{n \times m}$ be measurable functions satisfying 
  \begin{equation}
    |b(t,x)| + |\sigma(t,x)| \leq C(1+|x|); x \in \mathbb{R}^n, t \in [0,T]
  \end{equation}
  for some constant $C$, and such that 
  \begin{equation}
    |b(t,x) - b(t,y)| + |\sigma(t,x) - \sigma(t,y)| \leq D(|x-y|); x,y \in \mathbb{R}^n, t \in [0,T]
  \end{equation}
  for some constant $D$. Let $Z$ be a random variable which is independent of the $\sigma$-algebra $\mathcal{F}_{\infty}^{m}$ generated by $B_s(\cdot)$, $s\geq 0$ and such that 
  \begin{equation}
    E[|Z|^2] < \infty
  \end{equation}
Then the stochstic differential equation 
\begin{equation}
  dX_t = b(t,X_t)dt + \sigma(t, X_t)dB_t, 0 \leq t \leq T, X_0 = Z
\end{equation}
has a unique t-continuous solution $X_t(\omega)$ with the property that $X_t(\omega)$ is adapted to the filtration $\mathcal{F}_t^Z$ generated by $Z$ and $B_s(\cdot)$; $s \leq t$
and 
\begin{equation}
  E[\int_0^T|X_t|^2dt] < \infty.
\end{equation}

\end{theorem}
\subsection{SDE Numerical methods}
\label{subsec:SdeNumericalMethods}
There are two classes of numerical approximations for SDEs: Strong and weak approximations. 
\begin{definition}[Time discrete approximation;]
  A time discrete approximation $\widehat{X}_h$ with step size $h$ is a right continuous process with left hand limits. The approximation $\widehat{X}_{n,h} = \widehat{X}_h(t_n)$ is $\mathcal{F}_{t_n}$ measurable w.r.t. a time discretization $\mathcal{T}^M_h$ and resucsively defined by a function $\psi$ such that for $n=0,1,\dots,M-1$ holds
  \begin{equation}
    \widehat{X}_{n+1, h} = \psi(\widehat{X}_{0,h}, \dots, \widehat{X}_{n,h}, t_0, \dots, t_n, Z^1_n,\dots, Z_n^k)
  \end{equation}
  for some finite number $k$ of $\mathcal{F}_{t_{n+1}}$ measurable random variables $Z^j_n, 1 \leq j \leq k$.
\end{definition}
\begin{definition}
  A time discrete approximation $\widehat{X}$ with maximum step size $h$ \emph{converges strongly} to $X$ at time $T$ as $h \rightarrow 0$ if 
  \begin{equation}
    \lim_{h \rightarrow 0} \mathbb{E}(||X_T - \widehat{X}(T)||) = 0.
  \end{equation}
  The time discrete approximation $\widehat{X}$ converges strongly with order $p>0$ to $X$ at time $T$ as $h \rightarrow 0$ if there exists a constant $K > 0$, which does not depend on $h$, and a $\delta_0 > 0$ such that 
  \begin{equation}
    \mathbb{E}(||X_T - \widehat{X}(T)||) \leq K h^p
  \end{equation}
  holds for each $h \in ]0, \delta_0[$.
\end{definition}

\begin{definition}
  A time discrete approximation $\widehat{X}$ converges weakly to $X$ at time $T$ as $h \rightarrow 0$ with respect to a class $\mathcal{C}$ of test functions $f: \mathbb{R}^n \rightarrow \mathbb{R}$ if 
  \begin{equation}
    \lim_{h \rightarrow 0} |\mathbb{E}(f(X_T)) - \mathbb{E}(f(\widehat{X}(T)))| = 0.
  \end{equation}
  holds for all $f \in \mathcal{C}$.
  A time discrete approximation $\widehat{X}$ converges weakly with order $p$ to $X$ at time $T$ as $h \rightarrow 0$ if for each $f \in C^{2(p+1)}(\mathbb{R}^n, \mathbb{R})$ there exists a constant $K_f$ and a finite $\delta_0$ such that 
  \begin{equation}
     |\mathbb{E}(f(X_T)) - \mathbb{E}(f(\widehat{X}(T)))| = K_f h^p.
  \end{equation}
  for each $h \in ]0, \delta_0[$.
\end{definition}

A commonly used time discrete scheme is the Euler-Maruyama scheme. It is given by the following recurrence relations
\begin{equation}
  X_{n+1} = X_n + b(X_n, t_n)\Delta t + \sigma(X_n, t_n) \Delta B_n
\end{equation}
for $0 \leq n \leq N$.
\section{Modeling stochastic model with SDE}

\subsection{SGD}

\begin{assumption}
  The random variable satisfies 
  \begin{enumerate}
    \item $f_{\gamma}(w) \in \mathcal{L}^1(\Omega)$ for all $w \in \mathbb{R}^d$
    \item $f_{\gamma}(w)$ is continuously differentiable in $w$ almost surely and for each $R > 0$, there exists a random variable $M_{R,\gamma}$ such that $\max_{||x|| \leq R}|| \nabla f_{\gamma}(w) || \leq M_{R,\gamma}$ almost surely, with $\mathbb{E} |M_{R,\gamma}| < \infty$.
    \item $\nabla f_{\gamma}(w) \in \mathcal{L}^2(\Omega)$ for all $w \in \mathbb{R}^d$.
  \end{enumerate}
\end{assumption}
A class of loss function $f$ relevant to machine learning applications is usually given in the form $f(x,y) = \sum_{i=1}^n f_i(x,y)$.
\begin{equation}
  x^{(k+1)} = x^{(k)} - \psi^k \nabla f(x^{(k)}) h +  \psi(t)\sqrt{h/b^{(k)}} \sigma_{MB}(x^{(k)})Z^{(k)}
\end{equation}
\begin{equation}
  dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t))dB(t)
\end{equation}
\begin{equation}
  dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t), X(t-\xi(t)))dB(t)
\end{equation}
A simplified model SDE model is given by 
\begin{equation}
  dX_t = -\nabla f(X_t)dt + (\eta \Sigma(X_t))^{\frac{1}{2}}dB_t
\end{equation}
where
\begin{equation}
  \Sigma(X_t) = \frac{1}{N} \sum_{i=1}^N (\nabla f(x) - \nabla f_i(x))(\nabla f(x) - \nabla f_i(x))^T.
\end{equation}
One key observation is that the model includes the full gradient as well as a covariance term that requires the evaluation of each individual gradient term. Many commonly used numerical schemes  to obtain samples of the solution of this SDE require the evaluation of the bias $b$ and the drift $\sigma$. For example, the Euler-Maruyama scheme introduced in section (\ref{subsec:SdeNumericalMethods}) requires the evaluation of $b$ and $\sigma$ in each time step. 
Additionally, the evaluation of $\sigma$ requires the storage of $n^2$ entries, where $n \in \mathbb{N}$ is the number of paramters in the model.
In (\cite*{li2021validity}) the authors introduce the time discrete approximation called stochastic variance amplified gradient (SVAG).This method does need evaluations of the full gradient. It is given the following recurrence relation
\begin{equation}
  X_{k+1} = X_k - \frac{\eta}{l} \nabla f^l(X_k)
\end{equation}
where $f^l$ is defined as
\begin{equation}
  f^l_{i,j}(x) = \frac{1+\sqrt{2l - 1}}{2}f_i(x) + \frac{1-\sqrt{2l - 1}}{2}f_j(x).
\end{equation}
for independently sampled $i,j$.
\subsection{Generalization}
\section{Solving SDE Model}
\label{sec:SolvingSDEModel}
\subsection{Assumptions for solving}
\section{Experimental Verification} 
\label{sec:ExperimentalVerification}
Consider the following model: Let $H \in \mathbb{R}^{d\times d}$ be a symmetric, positive matrix. Define the sample objective 
\begin{equation}
  f_{\gamma}(w) = \frac{1}{2} (w - \gamma)^T H (w - \gamma) - \frac{1}{2} Tr(H)
\end{equation}
for $\gamma ~ N(0,I)$. The total objective is $f(w) = \mathbb{E} f_{\gamma}(w) = \frac{1}{2} w^T H w$.
The stochastic differential equation becomes 
\begin{equation}
  dW_t = -H W_t dt + \sqrt{\eta}H dB_t.
\end{equation}
This process is called Ornstein-Uhlenbeck process and posses the analytical solution
\begin{equation}
  W_t = e^{-t H}(W_0 + \sqrt{\eta}\int_0^te^{s H}H dB_s).
\end{equation}
\section{Conclusion}

\printbibliography
\end{document}
