\documentclass[12pt]{article}

% Layout
\usepackage[a4paper,includeheadfoot,margin=2.54cm]{geometry}

% Spracheinstellungen, alle Sprachen laden, letzte ist aktiv
\usepackage[english,ngerman]{babel}

% hilfreiche Pakete der AMS (American Mathematical Society) laden
\usepackage{amsmath}
\usepackage{amssymb}

% Sätze, Lemmata, ..
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% Formelnummerierung
\numberwithin{equation}{section}

% moderne Literaturverwaltung mittels Biber, erstzt BibLaTeX,
% kann UTF-8
\usepackage{biblatex}
\addbibresource{Masterarbeit_Jonathan_Hellwig.bib}

% Einbinden von Grafik, neue Version
\usepackage{graphicx}

% Unterabbildungen
\usepackage{subcaption}

% TikZ ist kein Zeichenprogramm (doch)
\usepackage{tikz}

% listings bindet Code ein
\usepackage{listings}
\definecolor{hellgrau}{rgb}{0.90,0.90,0.90}
\definecolor{commentcol}{rgb}{0.0823,.4902,0.0}
\lstset{language=Python,
        basicstyle={\footnotesize\ttfamily},
        keywordstyle={\sffamily\bfseries},
        tabsize=2,
        numbers=left,
        numberstyle=\tt,
        stepnumber=1,
        numbersep=7pt,
        breaklines=true,
        frame=single,
        frameround=ffff,
        commentstyle=\color{commentcol},
        backgroundcolor=\color{hellgrau},
        literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {Ã}{{\~A}}1 {ã}{{\~a}}1 {Õ}{{\~O}}1 {õ}{{\~o}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {·}{{$\cdot$}}1
}

% Hyperlinks in Texten
\usepackage{hyperref}
\hypersetup{%
  pdftitle     = {Modeling of stochastic optimization algorithms with stochastic differential equations},
  pdfsubject   = {Masterarbeit von Jonathan Hellwig},
  pdfkeywords  = {Masterarbeit, neuronale Netze},
  pdfauthor    = {\textcopyright\ Jonathan Hellwig 2022},
  linkcolor    = red,     % links to same page
  urlcolor     = blue,     % links to URLs
  citecolor    = green!50!black,     % links to citations
  breaklinks   = true,       % links may (line) break
  colorlinks   = true,
  citebordercolor=0 0 0,  % color for \cite
  filebordercolor=0 0 0,
  linkbordercolor=0 0 0,
  menubordercolor=0 0 0,
  urlbordercolor=0 0 0,
  pdfhighlight=/P,   % moeglich /I, /P, ...
  pdfborder=0 0 0,   % keine Box um die Links!
}
% nützliche Kurzkommandos für natürliche, ..., reelle, .. Zahlen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}

% ein paar dick gedruckte Buchstaben
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}

% Beginn des Dokumentes
\begin{document}

% Titelseite
\thispagestyle{empty}

\begin{center}
  \includegraphics[height=2.3cm]{pics/MATH_de}
  \hfill
  \includegraphics[height=2.3cm]{pics/TUHH_de}
\end{center}

\vspace*{5em}

\begin{center}
  {\Huge
    \textsc{Modeling of stochastic optimization algorithms with stochastic differential equations}\\[2em]
  }
  {\LARGE
    Masterarbeit
  }

  \vspace*{2em}

  {\Large
    von\\
    Jonathan Hellwig\\
    aus Hamburg\\
    Matrikelnummer: 7381194\\
    Studiengang: Technomathematik\\
  }
\end{center}

\vfill
\begin{center}
  \today
\end{center}
\vfill

\begin{tabbing}
  % längste Zeile zuerst duplizieren, mit kill löschen
  Erstprüferin: \= Dr. Jens-Peter M. Zemke\kill
  Erstprüferin: \> Dr. Jens-Peter M. Zemke\\
  Zweitprüfer:  \> Dr. Jens-Peter M. Zemke\\
  Betreuer:     \> Dr. Jens-Peter M. Zemke\\
\end{tabbing}
\newpage
% this page intentionally left blank
\thispagestyle{empty}
\mbox{}
\newpage

\section*{Eidestattliche Erklärung}

Hiermit versichere ich an Eides statt, dass ich die vorliegende
Bachelorarbeit mit dem Titel
\begin{quote}
  „Titel der Bachelorarbeit“  
\end{quote}
selbständig und ohne unzulässige fremde Hilfe verfasst habe. Ich habe
keine anderen als die angegebenen Quellen und Hilfsmittel benutzt,
sowie wörtliche und sinngemäße Zitate kenntlich gemacht. Die Arbeit
hat in gleicher oder ähnlicher Form noch keiner Prüfungsbehörde
vorgelegen. Ich versichere, dass die eingereichte schriftliche Fassung
der auf dem beigefügten Medium gespeicherten Fassung entspricht.

\vspace*{3em}

\begin{tabbing}
  \rule{.4\textwidth}{1pt} \hspace*{.2\textwidth}
  \= \rule{.4\textwidth}{1pt} \\
  Ort, Datum \> Unterschrift
\end{tabbing}

\newpage
\mbox{}
\newpage
% TOC - Table of Contents
\tableofcontents
\newpage
\listoffigures
\newpage

\section{Motivation}
\label{sec:Motivation}
\subsection{Common activation functions}
\section{Background on Optimization}
\label{sec:Optimization}
This chapter covers the background of optimization theory. It goes into the formal definition of optimization problems, the necessary and sufficient conditions for minimizers, and covers iterative methods for obtaining such minimizers.
\subsection{Formal definition, Existence, Uniqueness}
In machine learning one is interesed in fitting a model $M$ parameterized by a set of parameters $\{(W_i,b_i)\}_{i=1}^m$ to a dataset such that the model minimizers a certain metric, where $W_i \in \mathbb{R}^{n_{i+1} \times n_i}$,$b_i \in \mathbb{R}^{n_{i+1}}$ for $i = 1,\dots,m-1$. This metric is commonly refered to as loss function. \\
The loss function $l:\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ quantifies the distance between a prediciton $M(x) = \hat{y}$ and a true value $y$. In that sense, fitting a model to a dataset $\{(x_i,y_i)\}_{i=1}^n$ is defined as solving the minimization problem
\begin{equation}
  \min_{\{(W_i,b_i)\}_{i=1}^n} \frac{1}{n}\sum_{i=1}^n l(M(x_i), y_i)
\end{equation}
The selection of a loss function play an important role in the success of a machine learning model. Depending on the type of data there are many different loss function that posess different properties. \\
In the following, a more general version of the minimization problem is studied to obtain conditions for existence.
Consider the unconstrained minimization problem
\begin{equation}
  \min_{x \in \mathbb{R}^n} f(x),
\end{equation}
where $f:\mathbb{R}^n \rightarrow \mathbb{R}$.
\begin{definition}
  A value $x^\star$ is called global minimizer if 
  \begin{equation}
    f(x^\star) \leq f(x)
  \end{equation} 
  for all $x \in \mathbb{R}^n$.
\end{definition}
\begin{definition}
  A value $x^\star$ is called local minimizer if there exists an $\epsilon > 0$ such that
  \begin{equation}
    f(x^\star) \leq f(x)
  \end{equation} 
  for all $x \in \mathbb{R}^n, ||x-x^\star|| < \epsilon$.
\end{definition}

\begin{definition}
  A function $f$ is said to be L-smooth if its gradient are Lipschitz continuous, that is 
  \begin{equation}
    ||\nabla f(x) - \nabla f(y) || \leq L ||x-y||,
  \end{equation}
  for $x,y \in \mathbb{R}$.
\end{definition}

\begin{definition}
  A function $f$ is said to be convex if 
  \begin{equation}
    f(tx+(1-t)y) \leq tf(x)+(1-t)f(y),
  \end{equation}
  for all $x,y \in \mathbb{R}^n$ and $t \in [0,1]$.
\end{definition}
Obtaining global minimizers is difficult in practice. Therefore, in the following algorithms that converge to local minimizers are being considered.

Next, it remains to be answered how one can obtain a local minimizer $x^\star$ of $f$.
The following results provide some insight what sufficient and necessary conditions have to be fulfilled for local minimizer \cite{nocedal1999numerical}.
\begin{theorem}
  If $x^\star$ is a local minimizer and $f$ is continuously differentiable in an open neighborhood of $x^\star$, then $\nabla f(x^\star) = 0$.
\end{theorem}
\begin{theorem}
  If $x^\star$ is a local minimizer of $f$ and $\nabla^2 f$ exists and is continuous in an open neighborhood of $x^\star$, then $\nabla f(x^\star)$ and $\nabla^2f(x^\star)$ is positive semidefinite.
\end{theorem}
\begin{definition}
  A solution $x^\star \in S$ to the equation
  \begin{equation}
  \label{eq:StationaryPoint}
    \nabla f(x^\star) = 0
  \end{equation}
  is called stationary point.
\end{definition}
\subsection{Iterative methods}
For simple functions it is possible to determine the solution to (\ref*{eq:StationaryPoint})
in analytically form. However, state of the art neural networks contain billions of parameters. Therefore, iterative methods that approximate solution gradually are being used. The general form of an iterative scheme is given by
\begin{equation}
  x^{(k+1)} = x^{(k)} + \eta^{(k)} g^{(k)},
\end{equation}
where $\{\eta^{(k)}\}_{k=1}^\infty$ is called step size and $\{g^{(k)}\}_{k=1}^\infty$ is a sequence of descent directions. A descent direction is given by 

\begin{definition}
  An iterative method ($\{\eta^{(k)}\}_{k=1}^\infty$, $\{g^{(k)}\}_{k=1}^\infty$) is said to converge linearly if there exisits a constant $1 > C > 0$ such that 
  \begin{equation}
    \lim_{k \rightarrow \infty} \frac{||x^{(k+1)} - x^\star||}{||x^{(k)} - x^\star||} < M
  \end{equation}
  and it is said to converge sublinearly if 
  \begin{equation}
    \lim_{k \rightarrow \infty} \frac{||x^{(k+1)} - x^\star||}{||x^{(k)} - x^\star||} = 1
  \end{equation}
  holds.
\end{definition}
\subsubsection{Gradient Descent Method}
\begin{equation}  
  x^{(k+1)} = x^{(k)} - \eta^{(k)} \nabla f(x^{(k)}),
\end{equation}
Conventional convergence results require exact line search.

\cite{nesterov2003introductory}
\begin{theorem}
  Let $f$ satisfy assumptions ... and let $x^{(0)}$ be chosen such that 
  \begin{equation}
    r^{(0)} = ||x^{(k)} - x^\star|| \leq \frac{\bar{r}r^{(0)}}{\bar{r}-r^{(0)}}(1-\frac{2l}{L+3l}),
  \end{equation}
  where $\bar{r} = \frac{2l}{M}$.
\end{theorem}

\begin{theorem}
  Let $f$ be convex and L-smooth and let $\{x^{(k)}\}$ be a sequence generated by the gradient descent method. It follows that 
  \begin{equation}
    f(x^{(k)}) - f(x^\star) \leq \frac{2L||x^{(0)} - x^\star||^2}{k-1}
  \end{equation}
\end{theorem}

\begin{theorem}
  Let $f$ be L-smooth. It follows
  \begin{equation}
    \min_{k=1,\dots,N} ||\nabla f(x^{(k)})||^2 \leq \frac{L^2}{N}||x^{(0)}-x^\star||^2
  \end{equation}
\end{theorem}

\begin{theorem}
  Let $f$ be L-smooth and $\mu$-strongly convex. From a given $x^{(0)}$ and $\frac{1}{L} \geq \alpha > 0$, the iterates generated by the gradient descent method converge according to
  \begin{equation}
    ||x^{(k+1)} - x^\star ||^2 \leq (1-\alpha \mu)^{k+1}||x^{(0)} - x^star||^2
  \end{equation}
\end{theorem}
\subsubsection{Newton's method}
\begin{equation}
  x^{(k+1)} = x^{(k)} - (\nabla^2 f(x^{(k)}))^{-1}\nabla f(x^{(k)}),
\end{equation}
\subsection{Stochastic Optimization}
\label{sec:StochasticOptimization}
\begin{equation}
  x^{(k+1)} = x^{(k)} - \eta^{(k)} Z^{(k)},
\end{equation}
\subsubsection{Stochastic Gradient Descent}
% Convergence results
% Variants
% Gradient flow

\subsubsection{State of the art algorithms}
\label{sec:StateOfTheArtAlgorithms}

\section{Background SDE theory}
\label{sec:BackgroundSDETheory}
\subsection{Ito Integral}
\label{subsec:ItoIntegral}
\subsection{SDE Definition}
\label{subsec:SDEDefinition}
\subsection{SDE Existence Uniqueness}
\label{subsec:SDEExistenceUniqueness}
\subsection{SDE Numerical methods}

\section{Modeling stochastic model with SDE}
\subsection{SGD}
\begin{equation}
  x^{(k+1)} = x^{(k)} - \psi^k \nabla f(x^{(k)}) h +  \psi(t)\sqrt{h/b^{(k)}} \sigma_{MB}(x^{(k)})Z^{(k)}
\end{equation}
\begin{equation}
  dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t))dB(t)
\end{equation}
\begin{equation}
  dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t), X(t-\xi(t)))dB(t)
\end{equation}
\subsection{Generalization}
\section{Solving SDE Model}
\label{sec:SolvingSDEModel}
\subsection{Assumptions for solving}
\section{Experimental Verification} 
\label{sec:ExperimentalVerification}
\section{Conclusion}
Test

\printbibliography
\end{document}
