% LTeX: enabled=false
\documentclass[12pt]{article}

\usepackage{nomencl}
\makenomenclature

% Layout
\usepackage[a4paper,includeheadfoot,margin=2.54cm]{geometry}

% Spracheinstellungen, alle Sprachen laden, letzte ist aktiv
\usepackage[english]{babel}
\usepackage{csquotes}
% hilfreiche Pakete der AMS (American Mathematical Society) laden
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{aliascnt}
% Sätze, Lemmata, ..
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]

\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\providecommand*{\lemmaautorefname}{Lemma}

\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}

\newaliascnt{example}{theorem}
\newtheorem{example}[example]{Example}
\aliascntresetthe{example}
\providecommand*{\exampleautorefname}{Example}

\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Definition}
\aliascntresetthe{definition}
\providecommand*{\definitionautorefname}{Definition}


\newaliascnt{assumption}{theorem}
\newtheorem{assumption}[assumption]{Assumption}
\aliascntresetthe{assumption}
\providecommand*{\assumptionautorefname}{Assumption}
% Formelnummerierung
\numberwithin{equation}{section}

% For different enumeration styles
\usepackage{enumitem}
% moderne Literaturverwaltung mittels Biber, erstzt BibLaTeX,
% kann UTF-8
\usepackage{biblatex}
\addbibresource{thesis.bib}

% Einbinden von Grafik, neue Version
\usepackage{graphicx}

% Unterabbildungen
\usepackage{subcaption}

% TikZ ist kein Zeichenprogramm (doch)
\usepackage{tikz}

% listings bindet Code ein
\usepackage{listings}
\definecolor{hellgrau}{rgb}{0.90,0.90,0.90}
\definecolor{commentcol}{rgb}{0.0823,.4902,0.0}
\lstset{language=Python,
        basicstyle={\footnotesize\ttfamily},
        keywordstyle={\sffamily\bfseries},
        tabsize=2,
        numbers=left,
        numberstyle=\tt,
        stepnumber=1,
        numbersep=7pt,
        breaklines=true,
        frame=single,
        frameround=ffff,
        commentstyle=\color{commentcol},
        backgroundcolor=\color{hellgrau},
        literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {Ã}{{\~A}}1 {ã}{{\~a}}1 {Õ}{{\~O}}1 {õ}{{\~o}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {·}{{$\cdot$}}1
}

% Hyperlinks in Texten
\usepackage{hyperref}
\hypersetup{%
  pdftitle     = {Modeling of stochastic optimization algorithms with stochastic differential equations},
  pdfsubject   = {Masterarbeit von Jonathan Hellwig},
  pdfkeywords  = {Masterarbeit, neuronale Netze},
  pdfauthor    = {\textcopyright\ Jonathan Hellwig 2022},
  linkcolor    = red,     % links to same page
  urlcolor     = blue,     % links to URLs
  citecolor    = green!50!black,     % links to citations
  breaklinks   = true,       % links may (line) break
  colorlinks   = true,
  citebordercolor=0 0 0,  % color for \cite
  filebordercolor=0 0 0,
  linkbordercolor=0 0 0,
  menubordercolor=0 0 0,
  urlbordercolor=0 0 0,
  pdfhighlight=/P,   % moeglich /I, /P, ...
  pdfborder=0 0 0,   % keine Box um die Links!
}
% nützliche Kurzkommandos für natürliche, ..., reelle, .. Zahlen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\BP}{\mathbb{P}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}

\newcommand{\CF}{\mathcal{F}}
\newcommand{\CL}{\mathcal{L}}
\newcommand{\CR}{\mathcal{R}}
\newcommand{\CN}{\mathcal{N}}
\newcommand{\CV}{\mathcal{V}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\CO}{\mathcal{O}}
\newcommand{\CH}{\mathcal{H}}
% ein paar dick gedruckte Buchstaben
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}

% Typesetting for transpose
\newcommand{\T}{\mathsf{T}}

% Mollifier 
\newcommand{\moll}{\nu^{\epsilon}}

\newcommand{\ev}[1]{\mathbb{E}\left[{#1}\right]}
\newcommand{\var}[1]{\mathbb{V}\left[{#1}\right]}
\newcommand{\norm}[1]{\lVert{#1}\rVert_2}
\newcommand{\normf}[1]{\lVert{#1}\rVert_F}
\newcommand{\norml}[1]{\lVert{#1}\rVert_{\CL^2(\Omega)}}
\newcommand{\scp}[2]{\langle{#1}, {#2}\rangle_2}

% Custom math operators
\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\vol}{Vol}
\DeclareMathOperator{\diag}{diag}

% Beginn des Dokumentes
\begin{document}

% Titelseite
\thispagestyle{empty}

\begin{center}
  \includegraphics[height=2.3cm]{pics/MATH_de}
  \hfill
  \includegraphics[height=2.3cm]{pics/TUHH_de}
\end{center}

\vspace*{5em}

\begin{center}
  {\Huge
    \textsc{Relations between variants of stochastic gradient descent and stochastic differential equations}\\[2em]
  }
  {\LARGE
    Masterarbeit
  }

  \vspace*{2em}

  {\Large
    von\\
    Jonathan Hellwig\\
    aus Hamburg\\
    Matrikelnummer: 7381194\\
    Studiengang: Technomathematik\\
  }
\end{center}

\vfill
\begin{center}
  \today
\end{center}
\vfill

\begin{tabbing}
  % längste Zeile zuerst duplizieren, mit kill löschen
  Zweitprüfer:  \= Prof. Dr. Matthias Schulte\kill
  Erstprüfer: \> Dr. Jens-Peter M. Zemke\\
  Zweitprüfer:  \> Prof. Dr. Matthias Schulte\\
  Betreuer:     \> Dr. Jens-Peter M. Zemke\\
\end{tabbing}
\newpage
% this page intentionally left blank
\thispagestyle{empty}
\mbox{}
\newpage

\section*{Eidestattliche Erklärung}

Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit im Masterstudiengang Technomathematik selbstständig verfasst und keine anderen als die angegebenen Hilfsmittel – insbesondere keine im Quellenverzeichnis nicht benannten Internet-Quellen – benutzt habe. Alle Stellen, die wörtlich oder sinngemäß aus Veröffentlichungen entnommen wurden, sind als solche kenntlich gemacht. Ich versichere weiterhin, dass ich die Arbeit vorher nicht in einem anderen Prüfungsverfahren eingereicht habe und die eingereichte schriftliche Fassung der auf dem elektronischen Speichermedium entspricht.


\vspace*{3em}

\begin{tabbing}
  \rule{.4\textwidth}{1pt} \hspace*{.2\textwidth}
  \= \rule{.4\textwidth}{1pt} \\
  Ort, Datum \> Unterschrift
\end{tabbing}

\newpage
\mbox{}
\newpage
% TOC - Table of Contents
\tableofcontents
\newpage
\listoffigures
\newpage
\printnomenclature
\newpage
% LTeX: enabled=true
\section{Motivation}
\label{sec:Motivation}
In recent years, large-scale machine learning using deep neural networks has shown to be applicable across domains. Models like GPT-3 for text generation \autocite{brownLanguageModelsAre2020}, DALL-E2 for image generation \autocite{rameshHierarchicalTextConditionalImage2022}, MuZero for solving games \autocite{schrittwieserMasteringAtariGo2020} and AlphaFold for predicting protein geometry \autocite{jumperHighlyAccurateProtein2021} are prominent examples.

% One challenging area of research remains in understanding why these models work as well as they do. What allows these models to perform well on unseen data? 
% Researchers approach this question by studying these models emperically and by creating mathematical frameworks to understand which model properties lead to good generalizations. 
While the practical usefulness of deep neural networks is beyond dispute, theoretical results that explain how and why these models work have been lacking. Researchers want to understand the following question: how does the performance of a model depend on the choice of initialization, training algorithm and hyperparameters. Empirical studies can provide insights into initialization that ensure fast convergence \cite{glorotUnderstandingDifficultyTraining2010, heDelvingDeepRectifiers2015a}. Further, adaptive variants of stochastic gradient descent, namely Ada-Grad \cite{duchiAdaptiveSubgradientMethods2011}, RMSProp \cite{geoffreyhintonnitishsrivastavaandkevinswer-NeuralNetworksMachine2012} and Adam \cite{kingmaAdamMethodStochastic2017} have been well studied for different datasets and model classes \cite{wilsonMarginalValueAdaptive2017}. Goyal et al. \cite{goyalAccurateLargeMinibatch2018} observe that by linearly scaling learning rate and batch size training dynamics can be preserved. 
However, the underlying mechanisms for the empirical observations are not well understood. Ideally, for an initialization $w_0 \in \R^D$, a set of hyperparameters $\CH$ and an update rule $g : \R^d \times \CH \rightarrow \R^D$ we want to have a closed form solution of the difference equation
\begin{equation*}
  w_{k+1} = g(w_k, \CH),
\end{equation*}
for $k = 1,\dots, N$. By studying the solution $w_N = h(w_0, g, \CH)$, we can understand how initialization, training algorithm and hyperparameters influence the final model. Clearly, for large non-linear models explicit solutions to this difference equation are intractable. 
% Nonetheless, we might infer some properties of the its dynamics by studying
% Some theoretical research focuses on dense models in the infinite width limit \cite{leeWideNeuralNetworks2019, leeDeepNeuralNetworks2022, matthewsGaussianProcessBehaviour2022}. These works show that the trained model can be understood as a Gaussian process and investiagte properties of the trained model. 
% At this point authors make the assumption that the parameters follow an ordinary differential equation of the form
% \begin{equation*}
%   w'(t) = b(w(t),t).
% \end{equation*}

For smooth and convex models the training process has been scrutinized extensively and we cover some basic convergence analyses in \autoref{sec:Optimization} of this thesis. However, state-of-the-art models are neither smooth nor convex and classic literature does not answer questions about how the dynamics of a given optimization algorithm influences the training process. In this thesis, we want to study the dynamics of gradient descent and its stochastic version by approximating the discrete optimization process by a continuous process. First, we study the gradient flow equation and see that this equation belongs to a more general class of equations called modified equations. These equations allow us to model the dynamics of gradient descent more closely. By analyzing higher order modified equations we see the regularizing effect of finite learning rate gradient descent. Next, generalize to the case of stochastic gradient descent which requires stochastic calculus to formulate a continuous time model. By assuming a certain noise structure we construct a continuous model that captures the dynamics of stochastic gradient descent in a distributional sense.
However, the theorems that provide these approximation results are asymptotic and it is not clear if they hold in practice. Further, we need to make strong assumptions on the regularity of the gradients. In \autoref{sec:smdedl}, we investigate how closely the solution of the continuous process matches the discrete process. Moreover, we discuss how the assumption of the noise class influences the validity of the continuous model.
Finally, in \autoref{sec:applications} we highlight some insights we can gain from the dynamics of the continuous model.
% Underlying the construction of these models is a class of optimization problems.
% Conventionally, models with more parameters than data points were avoided because they lead to overfitting.


% - conventional wisdom says large models lead to overfitting
% - the most successful models are overparameterized models
% - loss functions are non-convex and even not differentiable
% - traditional convergence analyses cannot be applied
% - we want to understand the dynamics of the parameters during training
% - one approach is to study networks emperically: loss landscape, noise
% - another approach: approximate the discrete process by a continuous process determined by a differential equation
% - introduce flexibilty to the differential equation to reduce gap between discrete and continuous process
% - the dynamics of a stochastic optimizer cannot be modeled well by a deterministic process
% - by asserting a certain structure to the noise one can derive stochastic differential equations
% - in this thesis we introduce theorems that describe the how discrete and continuous process are related
% - the error between continuous and discrete is made up of discretization error and noise error
% - we discuss the influence of both errors
% - we discuss that the constants in the approximation theorems can be large
% - using the dynamic model we discuss special cases, and see how the discrete process relates to generalization
% - we discuss what conclusions we can draw from the continous model
% - we explain convergence for convex functions
% - we discuss that the assumption of the approximation theorems are not fulfilled by modern neural network architecture and that the SDE approximation is heuristic

\section{Notation}
Let $\CC^k(\R^d, \R)$ space of functions $f: \R^d \rightarrow \R$ with continuous derivatives of order $k \in \N$. Further, by $\CC_c^k(\R^d, \R)$ and $\CC_b^k(\R^d, \R)$ we denote the subset of functions of $\CC^k(\R^d, \R)$ which are compactly supported and bounded respectively.
For a multiindex $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_n)$ by $|\alpha| = \alpha_1 + \dots + \alpha_n$ we denote the order of $\alpha$.
For a multiindex $\alpha$ we write 
\begin{equation*}
  \nabla^{\alpha} \phi = \frac{\partial^{\alpha^1}}{\partial x_1^{\alpha_1}}\dots\frac{\partial^{\alpha^n}}{\partial x_n^{\alpha_n}} \phi
\end{equation*}
for the strong partial derivative. We use $D^{\alpha}\phi$ to denote weak partial derivatives for the multiindex $\alpha$.
For a probability space $(\Omega, \CF, \BP)$ we define the Lebesgue space $\CL^p(\Omega, \CF, \BP)$ for $p \in (1,\infty)$, and use $\CL^p(\Omega)$ if the $\sigma$-algebra $\CF$ and probability measure $\BP$ are clear. If we have $f \in\CL^p(\Omega, \CF, \BP)$, then
\begin{enumerate}[label=(\roman*)]
  \item $f$ is $\CF$-measurable,
\end{enumerate}
and
\begin{enumerate}[resume, label=(\roman*)]
  \item the norm 
  \begin{equation*}
    \lVert f \rVert_{\CL^p(\Omega, \CF, \BP)} \defeq \int_{\Omega} |f|^p d\BP < \infty
  \end{equation*}
  is finite.
\end{enumerate}
Further, the space $\CL(0,T;\CL^2(\Omega))$ consists of all measurable functions $W_{\cdot} : [0,T] \rightarrow \CL^2(\Omega)$ with $\lVert W_{\cdot} \rVert_{\CL(0,T;\CL^2(\Omega))} \defeq \int_0^T \lVert W_t \rVert_{\CL^2(\Omega)}dt = \int_0^T \ev{\norm{W_t}^2}dt < \infty$.

For a symmetric matrix $A \in \R^{d \times d}$ we write $\lambda_i(A)$ to denote the $i$-th eigenvalue for $i=1,\dots,d$, where $\lambda_1(A) \leq \lambda_2(A) \leq \dots \leq \lambda_d(A)$.
\section{Stochastic optimization}
\label{sec:Optimization}
% This chapter covers the background of optimization theory. It goes into the formal definition of optimization problems, the necessary and sufficient conditions for minimizers, and covers iterative methods for obtaining such minimizers.
 This section covers the mathematical basis for constructing and solving optimization problems in the context of neural networks.
 \subsection{Stochastic processes}
 Before we introduce the formal problem definition, we cover the fundamental concepts for stochastic processes and refer to E. et al \cite{eAppliedStochasticAnalysis2021} for further details. We define three concepts that are fundamental to the construction of stochastic processes: sample spaces, $\sigma$-algebras and probability measures.
 \begin{definition}[Sample space,  \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   The \emph{sample space} $\Omega$ is the set of all possible outcomes. Each element $\omega \in \Omega$ is called a \emph{sample point}.
 \end{definition}
 \begin{definition}[$\sigma$-algebra, $\sigma(\Omega)$, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   A $\sigma$-algebra $\CF$ is a collection of subsets of $\Omega$ that satisfies the following conditions:
   \begin{enumerate}[label=(\roman*)]
     \item $\Omega \in \CF$
     \item if $A \in \CF$, then $A^C \in \CF$, where $A^C = \Omega \setminus A$ is the complement of $A$ in $\Omega$
     \item if $A_n \in \CF$ for $n \in \N$, then $\bigcup_{n=1}^{\infty} A_n \in \CF$.
   \end{enumerate}
   We write $\sigma(\Omega)$ to denote the smallest $\sigma$-algebra on $\Omega$.
 \end{definition}
 \begin{definition}[Probability measure, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   The mapping $\BP : \CF \rightarrow [0,1]$ is called \emph{probability measure} if it statisfies
   \begin{enumerate}[label=(\roman*)]
     \item $\BP(\Omega) = 1$
     \item if $A_n \in \CF$ are pairwise disjoint for $n\in\N$, i.e. $A_i \cap A_j = \emptyset$ for $i \neq j$, then
     \begin{equation*}
       \BP(\bigcup_{n=1}^{\infty}A_n) = \sum_{n=1}^{\infty}\BP(A_n)
     \end{equation*}
   \end{enumerate}
 \end{definition}
 \begin{definition}[Probability space, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   The triplet $(\Omega, \CF, \BP)$ of sample space, $\sigma$-algebra and probability measure is called \emph{probability space}.
 \end{definition}
 \begin{definition}[Random variable, stochastic independence, \protect{\cite[pp.~12]{eAppliedStochasticAnalysis2021}}]
  For a probability space $(\Omega, \CF, \BP)$ a mapping $X : \Omega \rightarrow \R^d$ is called \emph{random variable}, if it is measurable, i.e. for each $A \in \sigma(\R^d)$ we have $X^{-1}(A) \in \CF$.
  Two random variable $X, Y$ are called \emph{independent} if for all $A,B \in \CF$ we have
  \begin{equation*}
    \BP(\{X \in A\} \cap \{Y \in B\}) = \BP(\{X \in A\})\BP(\{Y \in B\}).
  \end{equation*}
 \end{definition}
 \begin{example}[\protect{\cite[pp.~11]{eAppliedStochasticAnalysis2021}}]
  For a probability space $(\Omega, \CF, \BP)$ a random variable $W : \Omega \rightarrow \R$ is \emph{normally distributed} if its probability measure is given by
  \begin{equation*}
    \BP(X \in A) = \frac{1}{\sqrt{2 \pi}} \int_A e^{-\frac{1}{2}x^2}dx
  \end{equation*}
  for all $A \in \sigma(\R)$.
 \end{example}
 \begin{definition}(Filtration, $\CF_t$-adapted, \protect{\cite[pp.~104]{eAppliedStochasticAnalysis2021}})
   Given the probability space $(\Omega, \CF, \BP)$, the \emph{filtration} is a nondecreasing family of $\sigma$-algebras $\{\CF_t\}_{t\geq 0}$ such that $\CF_s \subset \CF_t \subset \CF$ for any $0 \leq s < t$. Fruther, a $\R^d$-valued stochastic process $\{X_t\}_{t \geq 0}$ on the probability space  $(\Omega, \CF, \BP)$ is called $\CF_t$-\emph{adapted} if $X_t$ is $\CF_t$-measureable, i.e. $X_t^{-1}(B) \in \CF_t$ for any $t \geq 0$ and $B \in \sigma(\R^d)$.
 \end{definition}
 
\subsection{Formal problem definition}
% Risk minimization
% Formal definition of data distribution
In this section, we introduce a general optimization problem that is approximated in practice. The goal is to construct a model that represents a set of data in an optimal way. Further, a model should generalize well, i.e.\ deliver good performance on unseen data. More formally, a model is a mapping $m_w$ that depends on some set of parameters $w \in \mathbb{R}^d$ and maps input features $x \in X$ to output features $y \in Y$. The data points $(x,y) \in X \times Y$ follow a joint probability distribution $p(x,y)$. The mapping $\ell$ quantifies the distance between a prediction $m_{w}$ and is called loss function. Finding a model that generalizes well can be understood as minimizing the \emph{expected risk}
\begin{equation}
  \min_{w \in \mathbb{R}^d} R(w) = \ev{\ell(m_{w}(x),y)}.
\end{equation}
In practice, the distribution $p(x,y)$ is unknown and the computation of the expected risk is not possible. Defining the minimization problem in terms of the sample average also known as the \emph{empirical risk} allow the problem to be solved. For a sample of $n \in \mathbb{N}$ data points $\{(x_i, y_i)\}_{i=1}^n$ the empirical risk is defined as 
\begin{equation}
  \label{eq:emperical_risk}
  \min_{w \in \mathbb{R}^d}  R_n(w) = \frac{1}{n}\sum_{i=1}^n\ell(m_{w}(x_i),y_i).
\end{equation}
A model $m_{w}$ generalizes well if $|R - R_n|$ is small. This notion will become relevant in the following section in the discussion of iterative methods for solving \eqref{eq:emperical_risk}. Whether a model generalizes well both depends on the model architecture and the procedure to obtain the parameters $w$. 
The selection of the loss function is contingent on the kind of data. For regression problems the \emph{mean square loss} is used, for multi-class classification problems the \emph{cross entropy loss} is used.

\subsection{Characterization of minimizers}
First, we introduce the concept of a minimizer to understand how the empirical risk minimization problem can be solved. In the following we consider a more general formulation of the above problem that can be found in optimization textbooks. Consider the unconstrained minimization problem
\begin{equation}
  \min_{w \in \mathbb{R}^d} f(w),
\end{equation}
where $f:\mathbb{R}^d \rightarrow \mathbb{R}$.
For this problem we define two notions of minimizers.
\begin{definition}[Local and global minimizer, \protect{\cite[pp.~12]{nocedalNumericalOptimization2006}}]
  A value $w^\star \in \mathbb{R}^d$ is called \emph{global minimizer} if 
  \begin{equation}
    \label{eq:minimizer}
    f(w^\star) \leq f(w)
  \end{equation} 
  for all $w \in \mathbb{R}^d$. A value $w^\star \in \mathbb{R}^d$ is called \emph{local minimizer} if there exists an $\epsilon > 0$ such that \eqref{eq:minimizer} holds for all $w \in \mathbb{R}^d, \norm{w-w^\star} < \epsilon$.
\end{definition}
Notice that every global minimizer is also a local minimizer. It is difficult to obtain global minimizers in deep learning settings. One key aspect of neural networks is that they are differentiable. If we make some smoothness assumptions on the function $f$, we can obtain sufficient and necessary conditions that provide some insight on local minimizers.
\begin{theorem}[\protect{\cite[pp.~15]{nocedalNumericalOptimization2006}}]
  If $w^\star \in \mathbb{R}^d$ is a local minimizer and $f:\mathbb{R}^d \rightarrow \mathbb{R}$ is continuously differentiable in an open neighborhood of $w^\star$, then $\nabla f(w^\star) = 0$. Further, if additionally $\nabla^2 f$ exists and is continuous in an open neighborhood of $w^\star$, then $\nabla^2f(w^\star)$ is positive semidefinite.
\end{theorem}
\begin{proof}
  The proof can be found in \autocite{nocedalNumericalOptimization2006}.
\end{proof}
The first theorem builds the foundation for all commonly used optimization techniques in deep learning. The gradient can be  efficiently calculated with a procedure called backpropagation which makes use of the recursive structure of neural networks. Finding candidates for local minima boils down to finding $w^\star$ such that $\nabla f(w^\star) = 0$. This leads to the following definition.
\begin{definition}[Stationary point, \protect{\cite[pp.~15]{nocedalNumericalOptimization2006}}]
  A solution $w^\star \in \mathbb{R}^d$ to the equation
  \begin{equation}
  \label{eq:StationaryPoint}
    \nabla f(w^\star) = 0
  \end{equation}
  is called \emph{stationary point}.
\end{definition}
\begin{theorem}[\protect{\cite[pp.~16]{nocedalNumericalOptimization2006}}]
  Let $f:\R^d \rightarrow \R$ be twice differentiable. If $\nabla^2 f$ is continuous in an open neighborhood of $w^\star$ and that $\nabla f(w^\star) = 0$ and $\nabla^2f(w^\star)$ is positive definite. Then $w^\star$ is a strict local minimizer of $f$.
\end{theorem}
\begin{proof}
  The proof can be found in \cite[pp.~16]{nocedalNumericalOptimization2006}.
\end{proof}
It is unclear how to obtain stationary points for neural networks, since $\nabla f$ is usually highly nonlinear. The next section will cover iterative methods to obtain approximate solutions to the stationary point problem.

\subsection{Iterative methods}
For simple functions it is possible to determine the solution to (\eqref{eq:StationaryPoint}) in analytical form. However, state of the art neural networks contain trillions of parameters and in general finding a solution analytically is infeasible. Iterative methods provide a way to start with an initial guess and improve upon that guess gradually. In particular, iterative methods considered in this thesis make use of gradient information for updating the parameters $w \in \mathbb{R}^d$. Before defining iterative descent methods, we introduce the concept of descent directions. 
\begin{definition}[Descent direction, \protect{\cite[pp.~30]{nocedalNumericalOptimization2006}}]
  Let $f: \mathbb{R}^d \rightarrow \mathbb{R}$ be a continuously differentiable function. Then a vector $g \in \mathbb{R}^d$ is called \emph{descent direction} at $w \in \mathbb{R}^d$ if it satisfies 
  \begin{equation}
    g^T \nabla f(w) < 0.
  \end{equation} 
\end{definition}
It is easy to show that given an initial value $w \in \mathbb{R}^d$ and a descent direction $g \in \mathbb{R}^d$ we have $f(w) > f(w + \eta g)$ for $\eta > 0$ sufficiently small. This motivates the definition of one-step iterative methods.
\begin{lemma}
  Let $f : \R^d \rightarrow \R$ be a continuously differentiable function. Then there exists an $\eta > 0$ such that
  \begin{equation}
    f(w) > f(w + \eta g)
  \end{equation}
  for all $w \in \R^d$ and a descent direction $g \in \R^d$ in $w$.
\end{lemma}
\begin{proof}
  Using the mean-value theorem we know that there exists an $\alpha \in (0,1)$ such that 
  \begin{equation}
    f(w) - f(w + \eta g) = \nabla f(w+\alpha\eta g)^Tg.
  \end{equation}
  Now, assuming that $\nabla f (w) \neq 0$, for $0 < \epsilon <
  \frac{\lvert\scp{\nabla f(w)}{g}\rvert}{\norm{g}}$ there exists a $\delta > 0$ such that 
  \begin{equation}
    \label{eq:continuity}
    \norm{\nabla f(w + \delta g) - \nabla f(w)} < \epsilon
  \end{equation}
  by the continuity of the gradients.
  Using \eqref{eq:continuity} we have
  \begin{equation}
    \begin{split}
      \langle\nabla f(w + \delta g), g \rangle &= \langle \nabla f(w + \delta g)- \nabla f(w), g \rangle + \langle \nabla f(w), g \rangle \\
      &\leq \norm{\nabla f(w + \delta g)- \nabla f(w)}\norm{g} + \langle f(w), g \rangle \\
      &< \epsilon \norm{g} + \langle \nabla f(w), g \rangle < 0
    \end{split}
  \end{equation}
  Choosing $\eta < \frac{\delta}{\alpha}$ we have
  \begin{equation}
    f(w) - f(w + \eta g) = \scp{\nabla f(w+\alpha\eta g)}{g} < 0.
  \end{equation}
\end{proof}
\begin{definition}[One-step method, gradient descent, \protect{\cite{nocedalNumericalOptimization2006}}]
  For descent directions $\{g_k\}_{k=1}^\infty$ and an initial value $w_{0}\in \mathbb{R}^d$ a sequence defined by
\begin{equation}
  w_{k+1} = w_{k} + \eta_k g_k,
\end{equation}
  where $\{\eta_k\}_{k=1}^\infty$ is called step size is called \emph{one-step descent method}. The special case with $g_k = \nabla f(w_k)$ such that the sequence of iterates is given by
  \begin{equation}
    \label{eq:gradient_descent}
    w_{k+1} = w_{k} - \eta_k \nabla f(w_{k}),
  \end{equation}
  is called \emph{gradient descent} (GD).
\end{definition}

It is easy to see for the iterates of GD we have ${g_k}^T \nabla f(w_{k}) = -\norm{ f(w_{k}) }^2 < 0$ as long as the iterates have not converged to a stationary point. GD forms the basis for many of the iterative schemes used in deep learning practice. It is important to note that after defining such an iterative scheme it is unclear whether it actually converges to a minimizer. In the later sections of this chapter we will discuss the classes of functions for which convergence can be established and how they relate to deep learning practice. Before diving into the theoretical analysis of descent methods let us consider an extension to the gradient descent method.
Notice that in \eqref{eq:gradient_descent} the computation of each iterate requires the computation of the gradient $\nabla f$. For objective functions of the form $f(w) = \frac{1}{n} \sum_{i=1}^n f_i(w)$ this requires the computation of each $\nabla f_i(w)$ individually. Remember that in deep learning settings $f_i$ represents the loss $\ell(m_w(x_i), y_i)$ with respect to a single data point $(x_i, y_i)$. When the number of data points is large, computing the full gradient $\nabla f$ becomes prohibitively expensive. In fact, ImageNet \cite{dengImageNetLargescaleHierarchical2009}, a benchmark dataset for image classification, contains over 1.2 million images. If we treat each individual gradient $\nabla f_i(w)$ as a realization of a random variable $\nabla f_{\gamma}(w)$ with expected value $\ev{\nabla f_{\gamma}(w)} = \nabla f(w)$, we can estimate $\nabla f(w)$ by calculating the sample average. In particular, this leads to the definition of the following random process:
\begin{definition}[Stochastic gradient descent, \protect{\cite{polyakMethodsSpeedingConvergence1964}}]
  \label{def:sgd}
  For a given initial value $w_{0} \in \mathbb{R}^d$ and i.i.d. random variables $\gamma^{(k)}$ the stochastic process defined by
  \begin{equation}
    \label{eq:stochastic_gradient_descent}
    w_{k+1} = w_{k} - \eta_k \nabla f_{\gamma^{(k)}}(w_{k}),
  \end{equation}
  is called \emph{stochastic gradient descent} (SGD).
  The stochastic process defined by
  \begin{align*}
    w_{k+1} &= \mu v^{(k)} - \eta \nabla f_{\gamma^{(k)}}(w_{k}) \\
    v^{(k+1)} &= w_{k} + v^{(k)},
  \end{align*}
  where $\mu \in (0,1)$ is called \emph{stochastic gradient descent with momentum}. 
\end{definition}
% where $\gamma^{(k)}$ are independently distributed random variables with the same distribution as $\gamma$. This iteration method is called \emph{stochastic gradient descent} (SGD).
In view of \eqref{eq:stochastic_gradient_descent}, let us define a class of one-step stochastic descent methods:
\begin{definition}[Stochastic one-step method, \protect{\cite{liStochasticModifiedEquations2019}}]
  For a given initial value $w_{0} \in \mathbb{R}^d$ and i.i.d. random variables $\gamma^{(k)}$ the stochastic process defined by
\begin{equation}
  w_{k+1} = w_{k} + \eta_k g(w_{k}, \gamma^{(k)}),
\end{equation}
  where $g:\mathbb{R}^d \times \Gamma \rightarrow \mathbb{R}^d$ is a measurable function, is called \emph{stochastic one-step descent method}.
\end{definition}
This class of stochastic iterative methods encompasses many commonly used optimization methods. Note that while introducing stochasticity to the optimization process allows for increased computational speed, it makes analysis of a method more difficult. In fact, for the analysis of stochastic methods it is not sufficient to only consider the expected value $\mathbb{E}[\nabla f_{\gamma}(w)]$ of the gradients. As we will see in the following sections, the variance $\mathbb{V}[\nabla f_{\gamma}(w)] = \mathbb{E}[(\nabla f_{\gamma}(w) - \nabla f(w)){(\nabla f_{\gamma}(w) - \nabla f(w))}^T]$ of the gradients also needs to be considered. (rephrase)
% The SGD method serves as a prototype for a lot of optimization methods. (rephrase) A simple extension is \emph{stochastic gradient descent with momentum}. 
% \begin{align*}
%   w_{k+1} &= \mu v^{(k)} - \eta \nabla f_{\gamma^{(k)}}(w_{k}) \\
%   v^{(k+1)} &= w_{k} + v^{(k)},
% \end{align*}
% where $\mu \in (0,1)$ is called \emph{momentum parameter}. (citation)
In the next section, we will explore the convergence of the stochastic schemes introduced in this section for different classes of well-behaved functions.
\subsection{Convergence analysis}
% \begin{definition}
%   An iterative method ($\{\eta_k\}_{k=1}^\infty$, $\{g_k\}_{k=1}^\infty$) is said to converge linearly if there exists a constant $1 > C > 0$ such that 
%   \begin{equation}
%     \lim_{k \rightarrow \infty} \frac{||w_{k+1} - w^\star||}{||w_{k} - w^\star||} < M
%   \end{equation}
%   and it is said to converge sublinearly if 
%   \begin{equation}
%     \lim_{k \rightarrow \infty} \frac{||w_{k+1} - w^\star||}{||w_{k} - w^\star||} = 1
%   \end{equation}
%   holds.
% \end{definition}
% In this section, we will discuss convergence results for SGD found in literature that make strong assumption about smoothness and convexity of the target function $f : \R^n \rightarrow \R$.
% In recent years, there has been a large body of work investigating the behavior of SGD. Of particular interest is the behavior of over-parametrized deep neural networks that exhibit behavior that is counter to common wisdom: over-parametrization leads to bad generalization. However, deep learning practice shows that a high training accuracy is accompanied by high test accuracy. This leads to the following questions: what is the mechanism of SGD results in good generalization? How do learning rate and batch size affect SGD dynamics?
% First, we approach these questions by discussing convergence under strong smoothness and convexity assumptions. These analyzes provide insight into how batch size, learning rate, smoothness and convexity influence the behavior of SGD iterates. Second, we relax the convexity assumption and present weaker results.
% Finally, whether these results can be applied to modern deep learning architectures. 
%  while others investigate SGD empirically.
In this section, we discuss convergence results for SGD found in modern literature. First, we begin by introducing convergence notions for stochastic processes. Then, we proceed by considering two classes of well-behaved functions, namely convex and smooth functions. These two classes of functions are the foundation for convergence analyses of SGD. Next, we discuss the convergence of SGD under differing assumptions and have a detailed look at one particular analysis. Lastly, we examine how these convergence results can be applied to modern deep learning architectures.

There exists a broad body of literature discussing the convergence of gradient descent. (citations) In SGD, the basic assumption of approximating the full gradient $\nabla f$ by a sampled gradient $\nabla f_{\gamma}$ is that convergence properties of gradient descent persist in some sense. It is not obvious that by gradient sampling we can obtain minimizers of the objective function $f$. Further, it is clear, that we need different notions of convergence, since the iterates of SGD form a stochastic process. In particular, we define two notions of convergence for stochastic processes found in~\autocite{eAppliedStochasticAnalysis2021}.
\begin{definition}[Almost sure convergence, convergence in distribution, \protect{\cite[pp.~16]{eAppliedStochasticAnalysis2021}}]
Let $\{W_k\}_{k=0}^{\infty}$ be a sequence of stochastic variables and $W$ a stochastic variable defined on a probability space $(\Omega, \CF, \BP)$. We say the sequence $\{W_k\}_{k=0}^{\infty}$ converges \emph{almost surely} to $W$ if
\begin{equation}
  \BP(\{\omega \in \Omega : \lim_{n \rightarrow \infty}W_n(\omega) = W(\omega)\}) = 1.
\end{equation}
We say the sequence  $\{W_k\}_{k=0}^{\infty}$ converges to $W$ \emph{in distribution} if
  \begin{equation}
    \lim_{k \rightarrow \infty}\ev{f(W_k) - f(W)} = 0
  \end{equation}
  for all $f \in C_b(\R^n, \R)$.
\end{definition}
% convergence in expectation ist nicht definiert 
% Beweis: Erkläre die einzelnen Schritte 
Note that almost sure convergence implies convergence in distribution. Let $f \in C_b(\R^d, \R)$ and let $\Omega_{\text{conv}} = \{\omega \in \Omega : \lim_{n \rightarrow \infty}W_n(\omega) = W(\omega)\}$ Then, by the dominated convergence theorem we have 
\begin{equation*}
  \lim_{k \rightarrow \infty} \ev{f(W_k) - f(W)} = \int_{\Omega} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP.
\end{equation*}
Now, by splitting $\Omega = \Omega_{\text{conv}} \cup (\Omega \setminus \Omega_{\text{conv}})$ we have
\begin{multline*}
  \int_{\Omega} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP 
  =\int_{\Omega_{\text{conv}}} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP \\
   + \int_{\Omega \setminus \Omega_{\text{conv}}} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP = \int_{\Omega_{\text{conv}}} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP = 0.
\end{multline*}

\begin{example}
  We consider an example of a sequence random variables $\{W_k\}_{k=1}^\infty$ that convergences to a limit $W$ in distribution but not almost surely.
  Let $W,Z$ be two stochastically independent random variables following a normal distribution. We define the sequence $W_k = Z$ for $k=1,2,\dots$. Then, for $f \in \CC_b(\R^d, \R)$ we have
  \begin{equation*}
    \lim_{k \rightarrow \infty} \ev{f(W_k)} = \ev{f(Z)} = \ev{f(W)}.
  \end{equation*}
  However, by stochastic independence we observe
  \begin{equation*}
    \BP(\lim_{k \rightarrow \infty} W_k = W) = \BP(Z = W) = \frac{1}{2\pi}\int_{-\infty}^\infty e^{-x^2} dx = \frac{1}{2\sqrt{\pi}}.
  \end{equation*}
  Therefore, we do not have almost sure convergence.
\end{example}
\subsubsection{Convergence assumptions}

Next, we introduce a strong smoothness condition.
\begin{definition}[Lipschitz continuity, Lipschitz constant]
  \label{def:lipschitz_continuity}
  A function $f : \R^d \rightarrow \R$ is said to be \emph{Lipschitz continuous} with \emph{Lipschtiz constant} $L >0$ if
  \begin{equation*}
    |f(w) - f(v)| \leq L \norm{w - v}
  \end{equation*}
  for all $w,v \in \R^d$.
\end{definition}
\begin{definition}[$L$-smoothness, \protect{\cite{bottouOptimizationMethodsLargeScale2018}}]
  \label{def:l_smooth}
  A continuously differentiable function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is said to be \emph{$L$-smooth} if its gradient is Lipschitz continuous, that is 
  \begin{equation}
    \norm{\nabla f(w) - \nabla f(v) } \leq L \norm{w-v},
  \end{equation}
  for $w,v \in \mathbb{R}^d$.
\end{definition}
From the above definition we have the following lemma.
\begin{lemma}[\protect{\cite{bottouOptimizationMethodsLargeScale2018}}]
  Let $f : \mathbb{R}^d \rightarrow \mathbb{R}$ be a $L$-smooth function. Then, we have 
  \begin{equation}
    f(w) \leq f(v) + \langle \nabla f(w), v - w \rangle + \frac{L}{2} \norm{ v - w }^2,
  \end{equation}
  for all $v, w \in \mathbb{R}^d$.
\end{lemma}
\begin{proof}
  Let $w, v \in \mathbb{R}^d$. Then, by the fundamental theorem of calculus we have
  $$
  f(w) - f(v) = \int_0^1 \nabla f(w_t)^T(v-w)dt 
  $$
  for $w_t = w + t(w-v)$.
  Using this equality we obtain
  \begin{equation}
    \label{eq:Lsmoothproof1}
    f(w) - f(v) = \int_0^1 (\nabla f(w_t) - \nabla f(w))^T(v-w)dt + \nabla f(w)^T(v-w).
  \end{equation}
  Now, using Cauchy-Schwarz inequality we have
  \begin{align}
    \label{eq:Lsmoothproof2}
    \int_0^1 (\nabla f(w_t) - \nabla f(w))^T(v-w)dt &\leq \int_0^1 L \norm{w_t-w} \norm{v-w}dt \\
    &= L \norm{v-w} \int_0^1 t dt = \frac{L}{2} \norm{v-w}
  \end{align}
  Combining \eqref{eq:Lsmoothproof1} and \eqref{eq:Lsmoothproof2} we obtain
  \begin{equation*}
    f(w) \leq f(v) + \langle \nabla f(w), v - w \rangle + \frac{L}{2} \norm{v - w}^2,
  \end{equation*}
\end{proof}
\begin{definition}[Convexity, \protect{\cite[pp.~67]{boydConvexOptimization2004}}]
  A function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is said to be \emph{convex} if 
  \begin{equation}
    f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)
  \end{equation}
  for all $x,y \in \mathbb{R}^n$ and $t \in [0,1]$.
\end{definition}
\begin{lemma}[\protect{\cite[pp.~69]{boydConvexOptimization2004}}]
  \label{lemma:convexity}
  Let $f : \mathbb{R}^d \rightarrow \mathbb{R}$ be a continuously differentiable function. The function $f$ is convex if and only if we have
  \begin{equation*}
    f(v) \geq f(w) + \langle \nabla f(w), v-w \rangle,
  \end{equation*}
  for all $v,w \in \R^d$.
\end{lemma}
\begin{proof}
  The proof can be found in \autocite{boydConvexOptimization2004}.
\end{proof}

Now, (reformulate) we present a notion of convexity that is used in the convergence literature of SGD \autocite{sebbouhAlmostSureConvergence2021}. 
\begin{definition}[Strong convexity, \protect{\cite[pp.~459]{boydConvexOptimization2004}}]
  A continuously differentiable function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is said to be $\mu$-\emph{strongly convex} if
  \begin{equation}
    f(v) \geq f(w) + \langle \nabla f(w), v - w \rangle + \frac{\mu}{2} \norm{v - w}^2
  \end{equation}
  for all $v, w \in \mathbb{R}^d$.
\end{definition}

\begin{definition}[$L$-\emph{smooth in expectation}, \protect{\cite{gowerSGDGeneralAnalysis2019}}]
  A continuously differentiable  function $f_{\gamma}:\mathbb{R}^d \rightarrow \mathbb{R}$ is said to be $L$-\emph{smooth in expectation} with respect to the distribution $\mathcal{D}$ if there exists $L = L(f, \mathcal{D}) > 0$ such that 
  \begin{equation}
    \mathbb{E}[\norm{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)}^2] \leq 2 L(f(w) - f(w^\star)),
  \end{equation}
  for all $w \in \mathbb{R}^d$. 
\end{definition}

\begin{definition}[Robbins and Monro conditions, \cite{robbinsStochasticApproximationMethod1951}]
  A sequence of non-negative real numbers $\{\eta_k\}_{k=0}^\infty$ is said to fulfill the \emph{Robbins and Monro conditions} if
  \begin{equation*}
    \sum_{k=0}^\infty \eta_k = \infty, \quad \sum_{k=0}^\infty \eta_k^2 < \infty.
  \end{equation*}
\end{definition}

\begin{assumption}[\protect{\cite{bottouOptimizationMethodsLargeScale2018}}]
  \label{as:sgd_convergence}
  Let $f : \R^d \rightarrow \R$ be a continuously differentiable function and $\{w_k\}_{k=0}^\infty$ the iterates given by SGD. Then they satisfy the following conditions
  \begin{enumerate}[label=(\roman*)]
    \item There exists a scalar $\mu > 0$ such that 
    \begin{equation*}
      \ev{\nabla f_{\gamma}(w)} \leq \mu,
    \end{equation*}
    for all $w \in \R^d$.
    \item There exist scalars $\mu_1, \mu_2 > 0$ such that 
    \begin{equation*}
      \label{eq:variance_linear_growth}
      \ev{\lVert \nabla f_{\gamma}(w) \rVert^2} \leq \mu_1 + \mu_2 \lVert \nabla f(w) \rVert^2,
    \end{equation*}
    for all $w \in \R^d$.
  \end{enumerate}
\end{assumption}
\subsubsection{Convergence results}
In recent years, there have been developments that build on the initial converge results from Robbins et al.\ \autocite{robbinsStochasticApproximationMethod1951}. There are results for almost sure convergence of the iterates \autocite{zhouStochasticMirrorDescent2017, nguyenSGDHogwildConvergence2018, sebbouhAlmostSureConvergence2021}, convergence of the function values and convergence of the gradients in expectation \autocite{bottouOptimizationMethodsLargeScale2018}.  
In Bottou et al.\ \autocite{bottouOptimizationMethodsLargeScale2018}, the authors present convergence for $L$-smooth objectives that satisfy \autoref{as:sgd_convergence} \ref{eq:variance_linear_growth}. They show convergence in expectation of the function values for strongly convex objectives and convergence of gradients to a stationary point for non-convex objectives. It is important to note that in the non-convex case, the authors only show the convergence of a subsequence to a stationary point. In Gower et al.\ \autocite{gowerSGDGeneralAnalysis2019}, the notion of expected notion of expected smoothness is introduced. Expected smoothness combines $L$-smoothness with a bound on the variance in a novel way. In combination with strong quasi-convexity the authors show convergence of the iterates in expectation for decreasing step sizes.
In the following we have a detailed look at an analysis by Sebbouh et al.\ \autocite{sebbouhAlmostSureConvergence2021} that gives insight into the assumptions and techniques required for a convergence proof.
Combining convexity and $L$-smoothness leads to the following lemma presented by \autocite{sebbouhAlmostSureConvergence2021}.
\begin{lemma}[\protect{\cite{gowerSGDGeneralAnalysis2019}}]
  \label{lemma:gradient_inequality}
  Let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function and $\gamma$ be a random variable defined on $(\Omega, \CF, \BP)$. If $f$ is both $L$-smooth and convex and $w^\star$ is a minimizer of $f$, then we have
  \begin{equation}
    \ev{\norm{\nabla f_{\gamma}(w)}^2} \leq 4 \CL (f(w) - f(w^\star)) + 2 \sigma^2,
  \end{equation}
  where $\sigma^2 \defeq \sup_{w \in \R^d} \ev{\norm{\nabla f(w)}^2}$ and $\CL \defeq \sup_{\gamma} L_{\gamma}$.
\end{lemma}
\begin{proof}
  Let $w, w^\star \in \R^d$ where $w^\star$ is a minimizer of $f$. We use the inequality (2.1.10) from \autocite{nesterovLecturesConvexOptimization2018} to obtain
  \begin{align*}
    \norm{\nabla f_{\gamma}(w) - f_{\gamma}(w^\star)}^2 &\leq 2L_{\gamma}(f_{\gamma}(w) - f_{\gamma}(w^\star) - \langle \nabla f_{\gamma}(w^\star), w - w^\star \rangle) \\
    &\leq 2\CL(f_{\gamma}(w) - f_{\gamma}(w^\star) - \langle \nabla f_{\gamma}(w^\star), w - w^\star \rangle).
  \end{align*}
  Thus, we have
  \begin{align*}
    \ev{\norm{\nabla f_{\gamma}(w) - f_{\gamma}(w^\star)}^2 } &\leq 2\CL(f(w) - f(w^\star) - \langle \nabla f(w^\star), w - w^\star \rangle) \\
    &= 2\CL(f(w) - f(w^\star)).
  \end{align*}
  Now, by using lemma \autoref{lemma:inequality} and the previous inequality we have
  \begin{align*}
    \ev{\norm{\nabla f_{\gamma}(w)}^2} &= \ev{\norm{(\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)) + \nabla f_{\gamma}(w^\star)}^2} \\
    &\leq 2( \ev{\norm{\nabla f_{\gamma}(w) - f_{\gamma}(w^\star)}^2 } + \ev{\norm{\nabla f_{\gamma}(w^\star)}^2}) \\
    &= 4\CL(f(w) - f(w^\star)) + 2\sigma^2.
  \end{align*}
\end{proof}
We now present the following result from \autocite{sebbouhAlmostSureConvergence2021}.
\begin{lemma}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  \label{lemma:sgd_iterates}
  Let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function that is $L$-smooth and convex for each $\gamma \in \Gamma$. Further, let $\{w_{k}\}_{k=1}^{\infty}$ be the iterates given by SGD and let $\eta_k \leq \frac{1}{4 \CL}$ for $k=1,2,\dots$. Then, we have
  \begin{equation*}
    \E_k[\norm{w_{k+1} - w^\star}] + \eta_k (f(w_{k}) - f^\star)) \leq \norm{w_{k} - w^\star}^2 + 2{\eta_k}^2\sigma^2,
  \end{equation*}
  for $k=1,2,\dots$.
\end{lemma}
\begin{proof}
  First, recall that the iterates of SGD are given by
  \begin{equation*}
    w_{k+1} = w_{k} - \eta_k \nabla f_{\gamma_k}(w_{k}).
  \end{equation*}
  We can use this to obtain the following:
  \begin{align*}
    \norm{w_{k+1} - w^\star}^2 &= \norm{w_{k} - w^\star - \eta_k \nabla f_{\gamma_k}(w_{k})}^2 \\
    &= \norm{w_{k} - w^\star}^2 + 2 \eta_k \scp{\nabla f_{\gamma_k}(w_{k})}{ w^\star - w_{k}} + {\eta_k}^2\norm{\nabla f_{\gamma_k}(w_{k})}^2.
  \end{align*}
  Now using \autoref{lemma:gradient_inequality} and \autoref{lemma:convexity} we obtain
  \begin{align*}
    \E_k[\norm{w_{k+1} - w^\star}^2] &= \norm{w_{k} - w^\star}^2 + 2 \eta_k \scp{\nabla f(w_{k})}{w^\star - w_{k}}+ {\eta_k}^2\E_k[\norm{\nabla f_{\gamma_k}(w_{k})}^2] \\
    &\leq \norm{w_{k} - w^\star}^2 + 2 \eta_k(2\CL \eta_k - 1)(f(w_{k}) - f(w^\star))+ {\eta_k}^22\sigma^2 \\
  \end{align*}
  Now, using $\eta_k \leq \frac{1}{4 \CL}$, we have
  \begin{equation*}
    \E_k[\norm{w_{k+1} - w^\star}^2] \leq \norm{w_{k} - w^\star}^2 - \eta_k(f(w_{k}) - f(w^\star))+ {\eta_k}^22\sigma^2.
  \end{equation*}
\end{proof}
We now present a non-asymptotic bound for the convex and $L$-smooth case.
\begin{theorem}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  \label{thm:SGD_bound}
  Let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function that is both $L$-smooth and convex for each $\gamma \in \Gamma$. Then, we have
  \begin{equation*}
    \ev{f(\widetilde{w_{k}}) - f(w^\star)} \leq \frac{\norm{w_{0} - w^\star}^2}{\sum_{i=0}^{k-1}\eta_i} + 2 \sigma^2 \frac{\sum_{i=0}^{k-1}\eta_i^2}{\sum_{i=0}^{k-1}\eta_i},
  \end{equation*}
  for $k = 1, 2, \dots$, where 
  \begin{equation*}
    \widetilde{w}_k = \sum_{i=0}^{k-1}\frac{\eta_i}{\sum_{j=0}^{k-1}\eta_j}w_i.
  \end{equation*}
\end{theorem}
\begin{proof}
  Using lemma \autoref{lemma:sgd_iterates} we have
  \begin{equation*}
    \E_k[\norm{w_{k+1} - w^\star}] + \eta_k (f(w_{k}) - f^\star)) \leq \norm{w_{k} - w^\star}^2 + 2{\eta_k}^2\sigma^2.
  \end{equation*}
  Summing over $k$ and taking the expected value we have
  \begin{equation*}
    \sum_{t=0}^{k-1}\ev{\norm{w^{(t+1)} - w^\star}} + \sum_{t=0}^{k-1} \eta_t \ev{f(w^{(t)}) - f^\star)} \leq \sum_{t=0}^{k-1} \ev{\norm{w^{(t)} - w^\star}^2} + \sum_{t=0}^{k-1} 2{\eta_t}^2\sigma^2.
  \end{equation*}
  Rearranging we have
  \begin{equation*}
    \sum_{t=0}^{k-1} \eta_t \ev{f(w^{(t)}) - f^\star)} \leq \ev{\norm{w_{0} - w^\star}^2} - \ev{\norm{w_{k+1} - w^\star}^2} + \sigma^2\sum_{t=0}^{k-1} 2{\eta_t}^2.
  \end{equation*}
  Now, normalizing with the sum of the learning rates, we use Jensen's inequality to obtain
  \begin{equation*}
    \ev{f(\widetilde{w^{(t)}}) - f^\star)} \leq \sum_{t=0}^{k-1} \frac{\eta_t}{\sum_{i=0}^{k-1}\eta_i} \ev{f(w^{(t)}) - f^\star)} \leq \frac{\ev{\norm{w_{0} - w^\star}^2}}{\sum_{i=0}^{k-1}\eta_i} + \frac{\sigma^2\sum_{t=0}^{k-1} 2{\eta_t}^2}{\sum_{i=0}^{k-1}\eta_i}.
  \end{equation*}
\end{proof}

Theorem \autoref{thm:SGD_bound} shows the relation between learning rate and gradient noise for the optimality gap. Note that for a fixed learning rate $\eta \in \R$ we have
\begin{equation*}
  \E [f(\widetilde{w_{k}}) - f(w^\star)] \leq \frac{\norm{w_{0} - w^\star}^2}{\eta k} + 2 \sigma^2 \eta.
\end{equation*}
While the first term converges to zero as $k \rightarrow \infty$, the second term remains constant. This gap is induced by gradient noise scaled with the learning rate. Intuitively, this is verified by the fact that each gradient step is scaled by the learning rate.
\begin{lemma}
  Let $\{\CF_t\}_{t=0}^\infty$ be a filtration and $\{V_t\}_{t=0}^\infty, \{U_t\}_{t=0}^\infty, \{Z_t\}_{t=0}^\infty$ be $\{\CF_t\}_{t=0}^\infty$-adapted nonnegative processes such that $\sum_{t=0}^\infty Z_t < \infty$ and for all $t \geq 0$
  \begin{equation*}
    \ev{V_{t+1}|\CF_t} + U_{t+1} \leq V_t + Z_t.
  \end{equation*}
  Then, $\{V_t\}_{t=0}^\infty$ converges and $\sum_{t=0}^\infty U_t < \infty$ almost surely.
\end{lemma}

\begin{theorem}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  \label{thm:almost_sure_convergence}
  Let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function that is both $L$-smooth and convex for each $\gamma \in \Gamma$. Then we have 
  \begin{equation*}
    f(\widehat{w_{k}}) - f(w^\star) = o\left(\frac{1}{\sum_{t=0}^{k-1}\eta_t}\right)
  \end{equation*}
  almost surely, where $\widehat{w}_{k+1} = v_k w_k + (1-v_k)\widehat{w}_k, \; \widehat{w}_0 = w_0$ and $v_k = \frac{2\eta_k}{\sum_{j=0}^k\eta_j}$ for $j=0,1,\dots$.
\end{theorem}
\begin{proof}
  The proof can be found in \autocite{sebbouhAlmostSureConvergence2021}.
\end{proof}
Theorem \autoref{thm:almost_sure_convergence} implies the following corollary.
\begin{corollary}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  Let $f_{\gamma}: \R^d \rightarrow \R$ be a $L$-smooth and convex function. Further, $0 < \eta \leq \frac{1}{4\CL}$ and $\epsilon > 0 $. Then, we have
  \begin{enumerate}
    \item If $\sigma^2 \neq 0$ and $\eta_k = \frac{\eta}{k^{1/2+\epsilon}}$
    \begin{equation*}
      f(\widehat{w_{k}}) - f^\star = o\left(\frac{1}{k^{1/2-\epsilon}}\right)
    \end{equation*}
    \item If $\sigma^2 = 0$ and $\eta_k = \eta$
    \begin{equation*}
      f(\widehat{w_{k}}) - f^\star = o\left(\frac{1}{k}\right),
    \end{equation*}
  \end{enumerate}
  where $\widehat{w}_{k+1} = v_k w_k + (1-v_k)\widehat{w}_k, \; \widehat{w}_0 = w_0$ and $v_k = \frac{2\eta_k}{\sum_{j=0}^k\eta_j}$ for $j=0,1,\dots$.
\end{corollary}
% \begin{assumption}
%   \begin{enumerate}
%     \item The sequence of iterates $w_{k}$ is contained in an open set over which $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is bounded from below by a scalar $f_{inf}$.
%     \item There exist scalars $\mu_G \geq \mu > 0$ such that, for all $k \in \mathbb{N}$, \begin{align}
%       \nabla {f(w_{k})}^T\mathbb{E}(g(w_{k}, \gamma^{(k)})) &\leq || \nabla f (w_{k}) ||^2 \\
%       || \mathbb{E}(g(w_{k}, \gamma^{(k)}))|| &\leq \mu_G ||\nabla f (w_{k})||.
%     \end{align}
%     \item There exist scalars $M \leq 0$ and $M_V \leq 0$ such that, for all $k \in \mathbb{N}$, \begin{equation}
%       \mathbb{V}(g(w_{k}, \gamma^{(k)})) \leq M + M_V ||\nabla f(w_{k}) ||^2
%     \end{equation} 
%   \end{enumerate}
% \end{assumption}
% (citation)

\subsection{Summary}
In this section, we introduced iterative methods for solving a general risk minimization problem. We covered gradient descent, stochastic gradient descent and stochastic gradient descent with momentum. Further, we looked at theoretical analyses of SGD and discussed common assumptions. In particular, we highlighted an almost sure analysis that give insight into the behavior of overparameterized models. 
At this point, we note that all analyses in literature employ different techniques and assumptions. This leaves open whether we can find a framework that unifies the convergence analysis. In the next section, we introduce stochastic differential equations which serve as a theoretical framework for general class of one-step methods. In section \autoref{sec:sde_model}, we see that SGD approximates a continuous-time stochastic process given by an SDE. Therefore, we can gain insight into the behavior of SGD by analyzing the behavior of a SDE.

\section{Stochastic differential equations}
\label{sec:BackgroundSDETheory}
In this section, we introduce the concept of stochastic differential equations (SDEs). First, we define a new notion of integration: Itô integration. Then, by reformulating an ordinary differential equation (ODE) as an integral equation, we generalize ODEs to stochastic processes by building on Itô integrals. Next, we discuss the existence and uniqueness of solution to SDEs. Finally, we introduce a framework for approximating solutions to SDEs numerically.
\subsection{Ordinary differential equations}
A system of ordinary differential equations are equations of the form
\begin{equation}
  \label{eq:initial_value_problem}
  w'(t) = b(w(t),t), \quad w(0) = w_0
\end{equation}
where $b : \R^d \times [0,T] \rightarrow \R^d$ is a continuous function. Equivalently, we can formulate the integral equation
\begin{equation}
  w(t) = w_0 + \int_0^t b(w(s),s)ds.
\end{equation}
\begin{theorem}[\protect{\cite[pp.~73]{ahmadTextbookOrdinaryDifferential2015}}]
  Let $b : \R^d \times [0,T] \rightarrow \R$ be a continuous function that fulfills a Lipschitz condition in $w$, i.e.
  \begin{equation*}
    \lvert b(w,t) - b(v,t) \rvert \leq L \norm{w - v}
  \end{equation*}
  for all $w,v \in \R^d$ and $t \in [0,T]$. Then, for every $w_0 \in R^d$ there exists a unique solution $w:[0,T] \rightarrow \R^d$ to \eqref{eq:initial_value_problem}.
\end{theorem}
\begin{definition}[Euler's method]
  Let $0 \leq t_0 < t_1 < \dots < t_N = T, \Delta t_n = t_{n+1} - t_n$. Then, the time-discrete scheme $\{w_n\}_{n=1}^N$ given by
  \begin{equation}
    \label{eq:ode_euler}
    w_{n+1} = w_n + b(w_n, t_n) \Delta t_n
  \end{equation}
  for $n=0,1,\dots,N$ is called \emph{Euler's method}.
\end{definition}
\begin{theorem}
  \label{thm:euler_convergence}
  Let $b : \R^d \times [0,T] \rightarrow \R^d$ be a continuous function statisfying a Lipschitz condition and let $w:[0,T] \rightarrow \R^d$ be the unique solution to \eqref{eq:initial_value_problem}. Further, let $0 \leq t_0 < t_1 < \dots < t_N = T$ be a partition of $[0,T]$ and $\{w_k\}_{k=1}^N$ be the iterates of Euler's method defined in \eqref{eq:ode_euler}. Then, for $h = \max\limits_{k=0,\dots,N-1} t_{k+1} - t_k$ there exists a constant $C > 0$ such that
  \begin{equation*}
    \norm{w(t_k) - w_k} \leq C h.
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof can be obtained as a special case of Theorem 8.3.2 in \cite[pp.~294]{kloedenNumericalSolutionStochastic2013}.
\end{proof}
\subsection{Itô Integral}
\label{subsec:ItoIntegral}
We briefly introduce the notion of Itô integral. Note that further details on the derivation can be found in \autocite{eAppliedStochasticAnalysis2021}. 
First, we introduce a continuous time generalization of a random walk: the Wiener Process.
\begin{definition}[\autocite{durrettProbabilityTheoryExamples2019}]
  A stochastic process $\{B_t\}_{t \geq 0}$ is called \emph{Brownian motion} if it satisfies the following properties
  \begin{enumerate}[label=(\roman*)]
    \item For any $t \geq s > u \geq v \geq 0$, $B_{t+s} - B_t$ and $B_{v+u} - B_v$ are independent.
    \item For any $s,t \geq 0$ $B_{t+s} - B_s \sim N(0, tI_d)$.
    \item The paths $t \rightarrow B_t$ are continuous almost surely.
  \end{enumerate}
\end{definition}
\begin{theorem}
  There exists a stochastic process $(B_t)_{t \geq 0}$ that satisfies the definition of Brownian motion.
\end{theorem}
\begin{proof}
  For the detailed construction of Brownian motion we refer to \autocite{durrettProbabilityTheoryExamples2019}.
\end{proof}
Now, we can define a notion of stochastic integration. Let $f : \R^d \rightarrow \R$ be continuous function. Then, we define a stochastic pathwise integral as a Riemann-Stieltjes integral:
\begin{equation*}
  \int_0^t f(X_s) dB_s = \lim\limits_{|\delta| \rightarrow 0} \sum_j f(X_j)(B_{t_{j+1}} - B_{t_j})
\end{equation*}
The rigorous construction of the Itô integral requires the definition on piece wise constant functions. Then, it can be shown that a general class of functions can be approximated by these functions. This establishes the existence of the integrals as a limit.
\begin{definition}[$\CL(0,T;\CL^2(\Omega))$, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
  We define the class of functions $\CL(0,T;\CL^2(\Omega))$ to be function $f : [0,T] \rightarrow \CL^2(\Omega)$ that satisfy the following properties:
  \begin{enumerate}
    \item $f$ is $(\CR \times \CF)$-measurable
    \item $f$ is $\CF_t$-adapted for all $t \in [0,T]$
    \item The time integral of the $\CL^2(\Omega)$-norm over $[0,T]$ is finite:
    \begin{equation*}
      \int_0^T \ev{f^2(\omega,t)}dt< \infty
    \end{equation*}
  \end{enumerate}
\end{definition}
\begin{theorem}[\autocite{eAppliedStochasticAnalysis2021}]
  \label{thm:ito_isometry}
  For $f \in \CL(0,T;\CL^2(\Omega))$, the Itô integral satisfies
  \begin{equation}
    \label{eq:ito_integral_ev}
    \E \left[ \int_0^T f(\omega,t)dB_t \right] = 0,
  \end{equation}
  and the \emph{Itô isometry}
  \begin{equation}
    \label{eq:ito_isometry}
    \E \left[ \int_0^T f(\omega,t)dB_t \right]^2 = \E \left[ \int_0^T f^2(\omega,t)dt \right].
  \end{equation}
\end{theorem}
Consider the case where $\{\pmb{B}_t\}_{t\geq0}$ is an $m$-dimensional Brownian motion and $\pmb{f},\pmb{g}  \in \R^{d \times m}$, we have
% Mult-line zeilen reduzieren
% \begin{equation}
%   \begin{split}
%     &\ev{\left(\int_S^T \pmb{f}(\omega, t)\cdot d\pmb{B}_t\right)^T\int_S^T \pmb{g}(\omega, t)\cdot d\pmb{B}_t}\\
%      &= \sum_{i=1}^d \ev{\sum_{j,k=1}^m \int_S^T f_{i,j}(\omega, t)dB_t^j\int_S^T g_{i,k}(\omega, t)dB_t^k}\\
%     &= \sum_{i=1}^d \sum_{j=1}^m \ev{\int_S^T f_{i,j}(\omega, t)dB_t^j\int_S^T g_{i,j}(\omega, t)dB_t^j} \\
%     &= \sum_{i=1}^d \sum_{j=1}^m \ev{\int_S^T f_{i,j}(\omega, t)g_{i,j}(\omega, t)dt}\\
%     &= \ev{\int_S^T \sum_{i=1}^d \sum_{j=1}^m f_{i,j}(\omega, t)g_{i,j}(\omega, t)dt}.
%   \end{split}
% \end{equation}
\begin{multline*}
  \ev{\left(\int_S^T \pmb{f}(\omega, t)\cdot d\pmb{B}_t\right)^\mathsf{T}\int_S^T \pmb{g}(\omega, t)\cdot d\pmb{B}_t}\\
     = \sum_{i=1}^d \ev{\sum_{j,k=1}^m \int_S^T f_{i,j}(\omega, t)dB_t^j\int_S^T g_{i,k}(\omega, t)dB_t^k}\\
    \overset{\eqref{eq:ito_integral_ev}}{=} \sum_{i=1}^d \sum_{j=1}^m \ev{\int_S^T f_{i,j}(\omega, t)dB_t^j\int_S^T g_{i,j}(\omega, t)dB_t^j} \\
    \overset{\eqref{eq:ito_isometry}}{=} \sum_{i=1}^d \sum_{j=1}^m \ev{\int_S^T f_{i,j}(\omega, t)g_{i,j}(\omega, t)dt}
    = \ev{\int_S^T \sum_{i=1}^d \sum_{j=1}^m f_{i,j}(\omega, t)g_{i,j}(\omega, t)dt}.
\end{multline*}
When $d = m$ and $g_{i,j}(\omega,t) = g_{j,i}(\omega,t)$, we have
\begin{equation}
  \label{eq:multivariate_ito_isometry}
  \begin{split}
    &\ev{\left(\int_S^T \pmb{f}(\omega, t)\cdot d\pmb{B}_t\right)^\mathsf{T}\int_S^T \pmb{g}(\omega, t)\cdot d\pmb{B}_t}\\
    &= \ev{\int_S^T \Tr\left[\pmb{f}(\omega, t)\pmb{g}(\omega, t)\right]dt}.
  \end{split}
\end{equation}
\begin{lemma}[Proposition 7.3 \protect{\cite{eAppliedStochasticAnalysis2021}}]
  Assume that $f,g \in CV[S,T]$ and $u \in [S,T]$. Then
  \begin{enumerate}[label=(\roman*)]
    \item $\displaystyle\int_S^TfdB_t = \int_S^ufdB_t + \int_u^TfdB_t$
    \item Linearity $\displaystyle\int_S^T(f+cg)dB_t =  \int_S^TfdB_t + c \int_S^TgdB_t$
    \item $\displaystyle\int_S^TfdB_t$ is $\CV_T^B$-measurable
  \end{enumerate}
\end{lemma}
\begin{example}
  \begin{equation}
    \int_0^t B_s dB_s = \frac{B_t^2}{2} - \frac{t}{2}
  \end{equation}
\end{example}
\begin{definition}[Itô process, \cite{eAppliedStochasticAnalysis2021}]
  A stochastic process is called \emph{Itô process} if it is given by
  \begin{equation}
    \label{eq:ito_process}
    W_t = W_0 + \int_0^tb(s, \omega)ds + \int_0^t \sigma(s, \omega)dB_s,
  \end{equation}
  where $\sigma \in \CV[0,T]$, b is $\CF_t$-adapted, and $\int_0^T |b(t, \omega)|dt < \infty$ almost surely.
\end{definition}
\begin{theorem}[Itô's formula, \protect{\cite[pp.~147]{eAppliedStochasticAnalysis2021}}]
  \label{thm:ito_formula}
  Let $f : \R^d \rightarrow \R$ be a twice differentiable function, and let $V_t = f(W_t)$ where $W_t$ is an Itô process defined in \eqref{eq:ito_process}. Then $V_t$ is also an Itô process and
  \begin{equation}
    \begin{split}
    V_t = f(W_0) &+ \int_0^t \nabla f(W_s)^\T b(s)+ \frac{1}{2}\Tr\left[\sigma(s) \nabla^2 f(W_s) \sigma(s)^\T\right]ds \\
    &+ \int_0^t \nabla f(W_s)^\T \sigma(s) dB_s.
    \end{split}
  \end{equation}
\end{theorem}

\begin{definition}[Drift, diffusion, SDE, \protect{\cite[pp.~19]{freidlinRandomPerturbationsDynamical1998}}]
  We call the function $b : \R^d \times [0,T] \rightarrow \R^d$ \emph{drift} and the function $\sigma : \R^d \rightarrow \R^{d \times m}$ \emph{diffsion}. Then a \emph{stochastic differential equation} is defined as the relation
  \begin{equation}
    \label{eq:sde}
    W_t = W_0 + \int_0^t b(W_s, s) dt + \int_0^t \sigma(W_s)dB_s, \quad t \in [0,T].
  \end{equation}
  We call a stochastic process $\{W_t\}_{t \in [0,T]}$ a \emph{solution} to \eqref{eq:sde} if the relation holds almost surely for all $t \in [0,T]$.
\end{definition}


\subsection{Existence and uniqueness}
\label{subsec:SDEExistenceUniqueness}
In this section, we cover the conditions for the existence and uniqueness of solutions to SDEs. We present a theorem by Li et al. \cite{liStochasticModifiedEquations2019} that generalizes results presented in \cite{oksendalStochasticDifferentialEquations2003}. 
% \begin{theorem}
%   \label{thm:sde_existence_uniqueness}
%   Let $T > 0$ and $b(\cdot,\cdot):[0,T] \times \mathbb{R}^n \rightarrow \mathbb{R}^n$, $\sigma(\cdot,\cdot):[0,T] \times \mathbb{R}^n \rightarrow \mathbb{R}^{n \times m}$ be measurable functions satisfying 
%   \begin{equation}
%     |b(t,x)| + |\sigma(t,x)| \leq C(1+|x|); x \in \mathbb{R}^n, t \in [0,T]
%   \end{equation}
%   for some constant $C$, and such that 
%   \begin{equation}
%     |b(t,x) - b(t,y)| + |\sigma(t,x) - \sigma(t,y)| \leq D(|x-y|); x,y \in \mathbb{R}^n, t \in [0,T]
%   \end{equation}
%   for some constant $D$. Let $Z$ be a random variable which is independent of the $\sigma$-algebra $\mathcal{F}_{\infty}^{m}$ generated by $B_s(\cdot)$, $s\geq 0$ and such that 
%   \begin{equation}
%     E[|Z|^2] < \infty
%   \end{equation}
% Then the stochastic differential equation 
% \begin{equation}
%   dX_t = b(t,X_t)dt + \sigma(t, X_t)dB_t, 0 \leq t \leq T, X_0 = Z
% \end{equation}
% has a unique t-continuous solution $X_t(\omega)$ with the property that $X_t(\omega)$ is adapted to the filtration $\mathcal{F}_t^Z$ generated by $Z$ and $B_s(\cdot)$; $s \leq t$
% and 
% \begin{equation}
%   \ev{\int_0^T|X_t|^2dt} < \infty.
% \end{equation}

% \end{theorem}
Let $T > 0$ and $Q$ be a subset of Euclidean space. For $(x,t,q) \in \R^d \times [0,T] \times Q$, let $B(x,t,q)$ be a $d$-dimensional random vector and $S(x,t,q)$ be a $d \times d$-dimensional random matrix. Then we assume: 
\begin{assumption}[\protect{\cite[pp. 30]{liStochasticModifiedEquations2019}}]
  \label{as:sde_existence}
  The random functions $B,S$ satisfy the following:
  \begin{enumerate}[label=(\roman*)]
    \item $B,S$ are $B_t$-adapted and continuous in $(x,t) \in \R \times [0,T]$ almost surely.
    \item $B,S$ satisfy a unifrom linear growth condition, i.e. there exists a non-random constant $L > 0$ such that
    \begin{equation*}
      \norm{B(x,t,q)}^2 + \normf{S(x,t,q)}^2 \leq L^2 (1 + \norm{x}^2) \text{ almost surely}
    \end{equation*}
    for all $x \in \R^d, \; t \in [s,T], \; q \in Q$.
    \item $B,S$ satisfy a uniform Lipschitz condition in $x$, i.e.
    \begin{equation*}
      \norm{B(x,t,q) - B(y,t,q)} + \normf{S(x,t,q) - S(y,t,q)} \leq L \norm{x - y}  \text{ almost surely}
    \end{equation*}
    for all $x,y \in \R^d, \; t \in [s, T], \; q \in Q$.
  \end{enumerate}
\end{assumption}

\begin{theorem}
  \label{thm:sde_existence}
  Let $s \in [0,T)$ and for each $q \in Q$, let $\{\phi_t^q : t \in [s,T]$ be a $\R^d$-valued, $B_t$-adapted random process that is continuous in $t \in [s,T]$ almost surely, with
  \begin{equation*}
    \sup_{q \in Q} \ev{\sup_{t \in [s,T]} \norm{\phi^q_t}^2} < \infty.
  \end{equation*}
  Then, for each $q \in Q$ the stochastic differential equation
  \begin{equation*}
    \xi^q_t = \phi_t^q + \int_s^t B(\xi^q_v,v,q)dv + \int_s^tS(\xi_v^q, v, q)dB_v
  \end{equation*}
  admits a unique solution $\{\xi_t^q : t \in [s,T]\}$ which is continuous for $t\in [s,T]$ almost surely and satisfies
  \begin{equation*}
    \sup_{q \in Q} \ev{\sup_{t \in [s,T]} \norm{\xi^q_t}^2} \leq C \left( 1 + \sup_{q \in Q} \ev{\sup_{t \in [s,T]} \norm{\phi^q_t}^2}\right)
  \end{equation*}
  for some constant $C > 0$ that depends only on $L,T$.
\end{theorem}
% \begin{theorem}
%   Let us assume the same conditions as in \autoref{thm:sde_existence} and let $q^\star \in Q$ be fixed. Suppose futher that the following holds for any $t \in [s,T],\; R > 0$ and $\epsilon > 0$:
%   \begin{enumerate}
%     \item 
%   \end{enumerate}
% \end{theorem}
\begin{proof}
  The proof for this theorem can be found in \cite{liStochasticModifiedEquations2019}.
\end{proof}
\begin{example}[Ornstein-Uhlenbeck process, \protect{\cite{uhlenbeckTheoryBrownianMotion1930}}]
  \label{ex:ornstein_uhlenbeck}
  The \emph{Ornstein-Uhlenbeck process} is defined by the equation
  \begin{equation}
    dW_t = -\gamma W_t dt + \sigma dB_t, W_0 = w,
  \end{equation}
  for $\gamma, \sigma > 0$ and $x \in \R^d$.
  Its solution is given by 
  \begin{equation}
    \label{eq:ornstein_solution_1d}
    W_t = e^{-\gamma t}w + \sigma \int_0^t e^{-\gamma (t - s)}dB_s.
  \end{equation}
  We see this by applying \hyperref[thm:ito_formula]{Itô's formula} to the function $f(t,w) = e^{\gamma t}w$:
  \begin{align*}
    df(t,W_t) &= \left(\gamma e^{\gamma t}W_t -\gamma W_t e^{\gamma t} + \frac{1}{2} \sigma^2 \cdot 0 \right)dt + \sigma e^{\gamma t} dB_t \\
    &= \sigma e^{\gamma t} dB_t.
  \end{align*}
  Thus, we have 
  \begin{equation*}
    e^{\gamma t}W_t - w = \int_0^t \sigma e^{\gamma s} dB_s
  \end{equation*}
  and finally by rearranging we obtain \eqref{eq:ornstein_solution_1d}.
  
\end{example}
\subsection{Numerical approxmations}
\label{subsec:SdeNumericalMethods}
There are two classes of numerical approximations for SDEs: Strong and weak approximations. 
\begin{definition}[Time discrete approximation;]
  A time discrete approximation $\widehat{X}_h$ with step size $h$ is a right continuous process with left-hand limits. The approximation $\widehat{X}_{n,h} = \widehat{X}_h(t_n)$ is $\mathcal{F}_{t_n}$ measurable w.r.t. a time discretization $\mathcal{T}^M_h$ and recursively defined by a function $\psi$ such that for $n=0,1,\dots,M-1$ holds
  \begin{equation}
    \widehat{X}_{n+1, h} = \Phi(\widehat{X}_{0,h}, \dots, \widehat{X}_{n,h}, t_0, \dots, t_n, Z^1_n,\dots, Z_n^k)
  \end{equation}
  for some finite number $k$ of $\mathcal{F}_{t_{n+1}}$ measurable random variables $Z^j_n, 1 \leq j \leq k$.
\end{definition}
\begin{definition}
  A time discrete approximation $\widehat{X}$ with maximum step size $h$ \emph{converges strongly} to $X$ at time $T$ as $h \rightarrow 0$ if 
  \begin{equation}
    \lim_{h \rightarrow 0} \mathbb{E}(\norm{X_T - \widehat{X}(T)}) = 0.
  \end{equation}
  The time discrete approximation $\widehat{X}$ converges strongly with order $p>0$ to $X$ at time $T$ as $h \rightarrow 0$ if there exists a constant $K > 0$, which does not depend on $h$, and a $\delta_0 > 0$ such that 
  \begin{equation}
    \mathbb{E}(\norm{X_T - \widehat{X}(T)}) \leq K h^p
  \end{equation}
  holds for each $h \in ]0, \delta_0[$.
\end{definition}

\begin{definition}
  A time discrete approximation $\widehat{X}$ converges weakly to $X$ at time $T$ as $h \rightarrow 0$ with respect to a class $\mathcal{C}$ of test functions $f: \mathbb{R}^n \rightarrow \mathbb{R}$ if 
  \begin{equation}
    \lim_{h \rightarrow 0} |\mathbb{E}(f(X_T)) - \mathbb{E}(f(\widehat{X}(T)))| = 0
  \end{equation}
  holds for all $f \in \mathcal{C}$.
  A time discrete approximation $\widehat{X}$ converges weakly with order $p$ to $X$ at time $T$ as $h \rightarrow 0$ if for each $f \in C^{2(p+1)}(\mathbb{R}^n, \mathbb{R})$ there exists a constant $K_f$ and a finite $\delta_0$ such that 
  \begin{equation}
     |\mathbb{E}(f(X_T)) - \mathbb{E}(f(\widehat{X}(T)))| = K_f h^p
  \end{equation}
  for each $h \in ]0, \delta_0[$.
\end{definition}
\begin{definition}[Euler-Maruyama scheme, \protect{\cite[pp.~305]{kloedenNumericalSolutionStochastic2013}}]
  Let $0 \leq t_0 < t_1 < \dots < t_N = T$, $\Delta t_k = t_{k+1} - t_k$ and let $B_k \sim \CN(0,\Delta t_k)$. Then, the time-discrete scheme $\{W_k\}_{k=0}^N$ given by
\begin{equation}
  \label{eq:euler_maruyama}
  W_{k+1} = W_k + b(W_k, t_k)\Delta t + \sigma(W_k, t_k) \Delta B_k
\end{equation}
for $0 \leq k \leq N$ is called \emph{Euler-Maruyama scheme}.
\end{definition}

\begin{theorem}[\protect{\cite[Proposition 7.22.]{eAppliedStochasticAnalysis2021}}]
  The Euler-Maruyama scheme is of strong order~$1/2$.
\end{theorem}
\section{Stochastic modified differential equations}
\label{sec:sde_model}
In this section, we introduce a continuous-time model for SGD. First, we consider the non-deterministic case of GD and derive an ODE system to model the limiting behavior of GD. Then, we establish a similar model for SGD building on the results of section \autoref{sec:BackgroundSDETheory}. Lastly, we establish a rigorous framework to show the convergence of SGD and the continuous-time model in a weak sense.
Note that in section \autoref{sec:sdeModelApplicatios} we detail analyses of the continuous-time process and make a case for the usefulness of the SDE model.

\subsection{Gradient descent}
\label{sec:gradient_flow}
% To motivate the continuous-time model for GD we consider the following example.
Recall the iterates $\{ w_k\}_{k=1}^N$ of GD given in \eqref{eq:gradient_descent} for a continuously differentiable function $f : \R^d \rightarrow \R$ are given by
\begin{equation*}
  w_{k+1} = w_{k} - \eta_k \nabla f(w_{k}),
\end{equation*}
for $k=1,\dots,N$. Now, we want to highlight the following observation: GD descent has the same structure as Euler's method for ordinary differential equations. If we interpret the learning rate $\eta$ as a time step, GD is a first order approximation to a system of ODEs. In fact, this follows from \autoref{thm:euler_convergence} if $\nabla f$ satisfies a Lipschitz condition. This leads to the following definition.
% In fact, if $\nabla f$ satisfies a Lipschitz condition we know by \autoref{thm:euler_convergence} that the iterates form a first order approximation to a system of ODEs. 
\begin{definition}[Gradient flow, \protect{\cite{santambrogioEuclideanMetricWasserstein2017}}]
  Let $f : \R^n \rightarrow \R$ be a continuously differentiable function. Then the system of ODEs
  \begin{equation}
  w'(t) = - \nabla f(w(t))
  \end{equation}
  with the initial condition $w(0) = w_{0}$ is called \emph{gradient flow}.
\end{definition}
Next, we consider the gradient flow of a linear regression problem for which the continuous-time model reduces to a system of linear ordinary differential equations.
\begin{example}[Linear regression]
  \label{ex:linear_regression}
  Let $\{x_i\}_{i=1}^n$ be a sequence of values with $x_i \in \R$ for $i = 1,2,\dots,n$. We define the sequence $\{y_i\}_{i=1}^n$ by
  \begin{equation*}
    y_i = wx_i + b + \epsilon_i,
  \end{equation*}
  where $w,b \in \R$ are fixed values and $\epsilon_i \sim \CN(0,1)$ for $i = 1, 2, \dots, n$.
  Now, given a sample of $\{(x_i, y_i)\}_{i=1}^n$ for unknown parameters $w, b \in R$ we want to find $\hat{w}, \hat{b} \in \R$ that optimally represent the data. In order to characterize optimality we define a loss function $\CL : \R^2 \rightarrow \R$ with 
  \begin{equation}
    \CL (w, b) = \frac{1}{n} \sum_{i=1}^n(wx_i + b - y_i)^2.
  \end{equation}
  In \autoref{fig:quadratic_loss_function} we see the loss value depending on the choice of $w,b \in \R$. The gradient of $\CL$ is given by
  \[
    \nabla \CL (w,b) = \frac{1}{n}
    \begin{pmatrix}
      2\left(\displaystyle\sum_{i=1}^n x_i^2\right)w + 2 \left(\displaystyle\sum_{i=1}^n x_i\right)b - 2 \displaystyle\sum_{i=1}^n x_i y_i \\
      2 \left(\displaystyle\sum_{i=1}^n x_i\right) w + 2nb - 2 \displaystyle\sum_{i=1}^n y_i
    \end{pmatrix}.
  \]
  For a learning rate $\eta > 0$ and an initial values $w_{0},b^{(0)} \in \R$ the iterates of gradient descent are given by
  \begin{equation}
    \begin{pmatrix}
      w_{k+1} \\
      b^{(k+1)}
    \end{pmatrix}
    =
    \begin{pmatrix}
      w_{k} \\
      b^{(k)}
    \end{pmatrix}
    - \frac{\eta}{n}
    \begin{pmatrix}
      2\left(\displaystyle\sum_{i=1}^n x_i^2\right)w + 2 \left(\displaystyle\sum_{i=1}^n x_i\right)b - 2 \displaystyle\sum_{i=1}^n x_i y_i \\
      2 \left(\displaystyle\sum_{i=1}^n x_i\right) w + 2nb - 2 \displaystyle\sum_{i=1}^n y_i
    \end{pmatrix}.
  \end{equation}
  In \autoref{fig:learning_rates} we show the behavior of the iterates for different values of $\eta >0$. We observe that a large learning rate leads to a faster convergence to the optimal value. This observation is in agreement with the results from \autoref{sec:Optimization}.
  %  Fig size reduzieren
  % Bounding boxes tight
  % Eta in den figures, konsistente schreibweise der kommazahlen

  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/learning_rates.pdf}
    \caption{Evolution of $w$ and $b$ for GD}
    \label{fig:learning_rates}
  \end{figure}

  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/quadratic_loss.pdf}
    \caption{Loss function}
    \label{fig:quadratic_loss_function}
  \end{figure}
\end{example}


\begin{example}[\autoref{ex:linear_regression} continued]
  Now, we apply this view to the linear regression example. In \autoref{fig:scaled_weights_biases} we observe that for each curve scaled according to the learning rate $\eta$ follows the same trajectory. This follows directly from the fact that they all approximate the solution of the gradient flow equation. The gradient flow system is given by
  \begin{equation*}
    \begin{pmatrix}
      w'(t)      \\
      b'(t)     
  \end{pmatrix}
  = 
  \begin{pmatrix}
    -2  \displaystyle\sum_{i=1}^n x_i^2  &  -2  \displaystyle\sum_{i=1}^n x_i      \\
      -2  \displaystyle\sum_{i=1}^n x_i  &  -2n      
  \end{pmatrix}
  \begin{pmatrix}
    w(t)     \\
    b(t)    
  \end{pmatrix}
  +
  \begin{pmatrix}
    2  \displaystyle\sum_{i=1}^n x_i y_i     \\
    2  \displaystyle\sum_{i=1}^n y_i    
  \end{pmatrix},
  \end{equation*}
  where $w(0) = w_{0}$ and $b(0) = b^{(0)}$. This system is linear in the parameters $w$ and $b$.
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/scaled_weights_biases.pdf}
    \caption{Evolution of $w$ and $b$ scaled for SGD}
    \label{fig:scaled_weights_biases}
  \end{figure}
\end{example}
We saw that gradient descent is a first order approximation to the gradient flow equation. We can analyze the asymptotic behavior of gradient descent by studying the gradient flow equation. However, gradient flow does not capture the phenomena induced by discretization. 
A technique to study the asymptotic behavior of numerical schemes is backward analysis. The idea is to construct a modified equation that more closely matches the behavior of the iterative scheme.
\begin{definition}[Modified equation, \protect{\cite[pp.~337]{hairerGeometricNumericalIntegration2013}}]
  Let $b: \R^d \rightarrow \R^d$ be a function with continuous derivatives of up to order $\alpha \in \N$. Let the $w:[0,T] \rightarrow \R^d$ be the solution of the differential equation
  \begin{equation*}
    w'(t) = b(w(t)), \quad w(0) = w_0
  \end{equation*}
  and let $\{w_k\}_{k=0}^N$ be the iterates of the iterative scheme $\Phi_h : \R^d \rightarrow \R^d$ given by
  \begin{equation*}
    w_{k+1} = w_k + \Phi_h(w_k).
  \end{equation*}
  Then, the \emph{modified differential equation} is given by
  \begin{equation*}
    \dot{\tilde{w}} = b_h(\tilde{w}),
  \end{equation*}
  where $b_h(\tilde{w}) = b_0(\tilde{w}) + hb_1(\tilde{w}) + h^2b_2(\tilde{w})+\dots$
  for some $b_0, b_1, \dots : \R^d \rightarrow \R^d$ such that $w_k = \tilde{w}(kh)$ for $k=1,\dots,N$.
\end{definition}
If we apply Euler's method to gradient flow, we have $\Phi_h(w) = h b(w) = - h \nabla f(w)$. Let $\widetilde{w}$ denote the solution of the modified differential equation. Then, if we use Taylor's expansion we have
\begin{align*}
  \widetilde{w}(t+h) &= \widetilde{w}(t) + b_h(\widetilde{w}(t))h + \frac{1}{2}b_h(\widetilde{w}(t))b_h'(\widetilde{w}(t))h^2 + \CO(h^3) \\
  &= \widetilde{w}(t) + (b_0(\widetilde{w}(t)) + b_1(\widetilde{w}(t))h + \dots)h \\
  &+ \frac{1}{2}(b_0(\widetilde{w}(t)) + b_1(\widetilde{w}(t))h + \dots)(b_0'(\widetilde{w}(t)) + b_1'(\widetilde{w}(t))h + \dots)h^2 + \CO(h^3).
\end{align*}
Rearanging by the powers of $h$ yields
\begin{equation*}
  \widetilde{w}(t+h) = \widetilde{w}(t) + b_0(\widetilde{w}(t))h + \left( \frac{1}{2}b_0(\widetilde{w}(t))b_0'(\widetilde{w}(t)) + b_1(\widetilde{w}(t))\right)h^2 + \CO(h^3).
\end{equation*}
By comparing the coefficients of the first and second powers of the iterative scheme $\Phi_h$ and the modified solution $\widetilde{w}$ we obtain the system
\begin{equation*}
  \begin{cases}
    b_0(\widetilde{w}(t)) = - \nabla f(w(t)) \\
    \frac{1}{2}b_0(\widetilde{w}(t))b_0'(\widetilde{w}(t)) + b_1(\widetilde{w}(t)) = 0
  \end{cases}.
\end{equation*}
Solving for $b_0$ and $b_1$ yields $b_0(w) = -\nabla f(w), \; b_1 = - \frac{1}{2}\nabla^2f(w)\nabla f(w) = -\frac{1}{4}\nabla\norm{\nabla f(w)}^2$. Therefore, the second order modified differential equation for gradient flow is given by
\begin{equation}
  \label{eq:second_order_ode}
  w'(t) = -\nabla(f(w) + \frac{\eta}{4}\norm{\nabla f(w)}^2),\quad w(0) = w_0.
\end{equation}
We see that GD with a fixed finite learning rate leads to a gradient flow for the loss function $\widetilde{f}(w) = f(w) + \frac{\eta}{4}\norm{\nabla f(w)}^2$. Barret et al. characterize the norm term As an implicit regularization to the gradients of the loss function \cite{barrettImplicitGradientRegularization2021}.
\begin{definition}[Positive definite, negative definite]
  Let $A \in \R^{d \times d}$ be a symmetric matrix. Then, we call the matrix $A$ \emph{positive definite} if
  \begin{equation*}
    x^\T A x > 0
  \end{equation*}
  for all $x \in \R^d \setminus \{0\}$. Further, we call $A$ \emph{positive semi-definite} if
  \begin{equation*}
    x^\T A x \geq 0
  \end{equation*}
  for all $x \in \R^d$. We call the matix $A$ \emph{negative (semi)-definite} if $-A$ is positive (semi)-definite.
\end{definition}
\begin{lemma}
  Let $A \in \R^{d \times d}$ be a symmetric, positive definite matrix. Then, the matrix $A$ has strictly positive eigenvalues, i.e.
  \begin{equation*}
    \lambda_i(A) > 0
  \end{equation*}
  for $i = 1, \dots, d$.
\end{lemma}
\begin{proof}
  Let $A \in \R^{d \times d}$ be a symmetric, positive definite matrix. Further, let $v \in \R^d \setminus \{0\}$ be an eigenvector to the eigenvalue $\lambda \in \sigma(A) = \{\lambda \in \R | \exists v \in \R^d \setminus \{0\}: Av = \lambda v \}$, then we have
  \begin{equation*}
    0 < v^\T A v = v^\T (\lambda v) = \lambda \norm{v}^2.
  \end{equation*}
  Clearly, this implies $\lambda > 0$.
\end{proof}
\begin{example}[\protect{\autoref{ex:linear_regression}} continued]
  Let us extend the linear regresssion example to the $d$-dimensional case, i.e. $f(w)=\frac{1}{2n} \norm{Xw - y}$, where $X \defeq [x_1\;x_2\;\dots\;x_n]^\T \in \R^{d \times n}$ and $y \defeq [y_1\;y_2\;\dots\;y_n]^\T$. 
  The gradient is given by $\nabla f(w) = \frac{1}{n}\left(X^\T Xw - X^\T y\right)$. 
  Notice that the stationary points of $f$ are given by the equation
  \begin{equation*}
    X^\T Xw^\star = X^\T y.
  \end{equation*}
  If we assume $d < n$ and $x_i \neq x_j$ for $i \neq j$, then the matrix $X^\T X$ has full rank and is positive definite. Thus, we have the unqiue minimizer $w^\star = (X^\T X)^{-1}X^\T y$.
  Now, we analyze the dynamics of gradient descent using the first and second order modified equations. First, we consider the gradient flow equation.
  For an intial value $w_0 \in \R^n$ we have 
  \begin{equation}
    \label{eq:linear_regression_gradient_flow}
    w'(t) = -\frac{1}{n}\left(X^\T Xw - X^\T y\right), \quad w(0) = w_0.
  \end{equation}
  Equation \eqref{eq:linear_regression_gradient_flow} is a system of inhomogenous linear equations. Under our assumptions on $X$ it has the solution
  \begin{equation*}
    w(t) = e^{-\frac{1}{n}X^\T Xt}(w_0 - (X^\T X)^{-1}X^\T y) + (X^\T X)^{-1}X^\T y = e^{-X^\T Xt}(w_0 - w^\star) + w^\star.
  \end{equation*}
  We see that since $X^\T X$ is positive definite, we have $\lambda_i(-X^\T X) < 0$, $i = 1, \dots, d$ for all eigenvalues of $-X^\T X$. Thus, the solution to the gradient flow equation converges to the minimizer:
  \begin{equation*}
    \lim_{t \rightarrow \infty} w(t) = \lim_{t \rightarrow \infty} e^{-X^\T Xt}(w_0 - w^\star) + w^\star = w^\star.
  \end{equation*}
  The rate of convergence is determined by smallest eigenvalue $\min\limits_{i=1,\dots,d}\lambda_i$. Next, we derive the second order modified equation and compare the solution with the first order solution. In the case of linear regression the second order modified equation defined in \eqref{eq:second_order_ode} is given by
  \begin{equation*}
    \widetilde{w}'(t) = -\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)\widetilde{w}(t) + \frac{1}{n}\left(I_d + \frac{\eta}{2n} X^\T X\right)X^\T y, \quad \widetilde{w}(0) = w_0.
  \end{equation*}
  Once again this is a linear system of inhomogenous equations. We note that the matrix $-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)$ has full rank since the sum of two negative definite matrices is negative definite by \autoref{lem:sum_positive_definite}. Thus, we have the solution
  \begin{align*}
    \widetilde{w}(t) &= e^{-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)}\Bigl(w_0 - \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}(X^\T X)^{-1}X^\T y\Bigr) \\
    &+ \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}(X^\T X)^{-1}X^\T y \\
    &=e^{-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)}\Bigl(w_0 - \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}w^\star\Bigr) + \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}w^\star.
  \end{align*}
  Now, consider the eigenvalue decomposition $X^\T X = Q^\T \Lambda Q$, where $Q^\T Q = I_d$ and $\Lambda = \diag(\lambda_1, \dots, \lambda_d)$. If we denote the limiting value of $\widetilde{w}$ as $\widetilde{w}^\star \defeq \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}w^\star$, we have
  \begin{align*}
    \norm{\widetilde{w}^\star}^2 &= {w^\star}^\T (I_d + \frac{\eta}{2 n}X^\T X)^{-2} w^\star = {Qw^\star}^\T (I_d + \frac{\eta}{2 n}\Lambda)^{-2} Qw^\star \\
    &= \sum_{j=1}^d \frac{1}{(1 + \frac{\eta}{2 n}\lambda_i)^2} (Qw^\star)_i^2 \leq \sum_{j=1}^d (Qw^\star)_i^2 = \norm{Qw^\star}^2 = \norm{w^\star}.
  \end{align*}
  This result agrees with the intuition that gradient descent for finite learning rates regularizes the loss function. Indeed, the gap between the stationary first and second order solution is driven by $\frac{\eta}{2 n} \min_{i=1,\dots,d}\lambda_i(X^\T X)$. This means that the strength of the regularization is proportional to the learning rate and smallest eigenvalue of $X^\T X$ and inversely proportional to the sample size $n$. 
  We conclude that for sufficiently small learning rates gradient flow captures the dynamics of GD. However, for larger learning rates gradient flow fails to model regularization phenomena induced by the discretization.
\end{example}
\subsection{Stochastic gradient descent}
Now, similar to the modified equations for GD we derive stochastic modified equations for SGD. 
Recall from \autoref{sec:Optimization} that the iterates are given by
\begin{equation*}
  w_{k+1} = w_{k} - \eta \nabla f_{\gamma^k}(w_{k})
\end{equation*}
for $k = 0,1,\dots$. We observe that the iterates can be decomposed as follows
\begin{equation}
  \label{eq:sgd_decomposition}
  w_{k+1} = w_{k} - \eta \nabla f(w_{k}) + \eta N^{(k)},
\end{equation}
where $N^{(k)} \defeq \nabla f(w_{k}) - \nabla f_{\gamma^k}(w_{k})$.
In the following, we make three assumptions that guarantee the existence of the first and second moment.
\begin{assumption}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{as:sde_model}
  The random variable satisfies 
  \begin{enumerate}[label=(\roman*)]
    \item $f_{\gamma}(w) \in \mathcal{L}^1(\Omega)$ for all $w \in \mathbb{R}^d$
    \item \label{as:bounded_gradient} $f_{\gamma}(w)$ is continuously differentiable in $w$ almost surely and for each $R > 0$, there exists a random variable $M_{R,\gamma}$ such that $\max_{\norm{x} \leq R} \norm{ \nabla f_{\gamma}(w) } \leq M_{R,\gamma}$ almost surely, with $\mathbb{E} |M_{R,\gamma}| < \infty$
    \item $\nabla f_{\gamma}(w) \in \mathcal{L}^2(\Omega)$ for all $w \in \mathbb{R}^d$
  \end{enumerate}
\end{assumption}

The first term in \eqref{eq:sgd_decomposition} is the full gradient update from GD. The second term is the \emph{gradient noise}. It is easy to see that 
\begin{align*}
  &\ev{\eta N^{(k)}|w_{k}} = 0 \\
  &\var{\eta N^{(k)}|w_{k}} = \eta^2 \ev{N^{(k)}{N^{(k)}}^T|w_{k}}.
\end{align*}
\begin{example}[\autoref{ex:linear_regression} continued]
  Now, we apply SGD to the linear regression problem from \autoref{ex:linear_regression}. In \autoref{fig:sgd_linear_fit} we observe different results for different sample trajectories of SGD.
  Further, we visualize the distribution of the parameter $w$ during several iterations of SGD in \autoref{fig:sgd_weight_histogram}. For this particular example the weights seem to follow a normal distribution. To confirm normality empirically we construct a quantile-quantile plot of the sampled distribution and the normal distribution in \autoref{fig:sgd_weight_qq}.
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/sgd_linear_fit.pdf}
    \caption{Linear regression fit with SGD}
    \label{fig:sgd_linear_fit}
  \end{figure}
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/sgd_weight_histogram.pdf}
    \caption{Histogram of parameter $w$ for SGD sample trajectories}
    \label{fig:sgd_weight_histogram}
  \end{figure}
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/sgd_weight_qq.pdf}
    \caption{Quantile-quantile plot for normal distribution and sampled values of $w$ at the last iteration}
    \label{fig:sgd_weight_qq}
  \end{figure}
\end{example}
To derive the SDE model we make the following assumption.
\begin{assumption}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{as:normality}
  Let $f_{\gamma}, f : \R^n \rightarrow \R$ be a continuously differentiable functions for $\gamma \in \Gamma$. Then we assume 
  \begin{equation*}
    \nabla f_{\gamma}(w) - \nabla f(w) \sim \CN(0, \Sigma(w)).
  \end{equation*}
\end{assumption}
From \autoref{as:normality} and \eqref{eq:sgd_decomposition} we have
\begin{equation}
  \label{eq:sgd_normal_decomposition}
  w_{k+1} = w_{k} - \eta \nabla f(w_{k}) + (\eta \Sigma(w))^{\frac{1}{2}} Z_k,
\end{equation}
where $Z_k \sim \CN(0,I_n)$ and $k=0,1,\dots$. Comparing \eqref{eq:sgd_normal_decomposition} with the Euler-Maruyama scheme in \eqref{eq:euler_maruyama}, we see that SGD approximates the SDE
\begin{equation}
  dW_t = -\nabla f(W_t)dt + (\eta \Sigma(W_t))^{\frac{1}{2}}dB_t.
\end{equation}
Next, we introduce a framework by \autocite{liStochasticModifiedEquations2019} to establish weak approximation of the SGD iterates to the solution of the SDE model. We generalize the results of \autoref{sec:gradient_flow} to the case of SGD and introduce both a first and second order modified equation.
We consider the general class of equation of the form
\begin{equation}
  \label{eq:general_sde}
  dW^{\eta, \epsilon}_t = b(W^{\eta, \epsilon}_t, \eta, \epsilon)dt + \sqrt{\eta}\sigma(W^{\eta, \epsilon}_t, \eta, \epsilon)dB_t, \quad W_0 = w_0, \quad t \in [0,T],
\end{equation}
where $\epsilon \in (0,1)$ is a mollification parameter. This is a technique introduced Li et al. \cite{liStochasticModifiedEquations2019} to relax the smoothness assumption on the target function $f$. In particular, both the first and second order convergence theorems introduced in this section only require the smoothness that is necessary for defining the modified equations.
To understand this technique we introduce two concepts: weak derivatives and mollifiers. Weak derivatives generalize differentiation to locally integrable functions. Mollifiers are useful for creating a sequence of smooth functions that approximate a particular nonsmooth function. We briefly introduce both concepts in the following and refer to \cite{evansPartialDifferentialEquations2010} for a more detailed introduction.
\begin{definition}[Weak derivatives, \protect{\cite[pp.~242]{evansPartialDifferentialEquations2010}}]
  Let $f,g \in \CL^1_{\text{loc}}(\R^d)$ and let $\alpha = (\alpha_1, \dots, \alpha_n)$ be a multiindex. Then, $g = D^{\alpha} f$ is called \emph{$\alpha$ order weak derivative of $f$}, if
  \begin{equation}
    \label{eq:weak_derivative}
    \int_U f D^{\alpha} \phi dx = (-1)^{|\alpha|} \int_U g \phi dx
  \end{equation}
  for all test functions $\phi \in C_c^{\infty}(U)$, where $U \subset \R^d$ is an open set.
\end{definition}
\begin{example}
  Let us consider the function $f : \R \rightarrow \R^+$, $f(x) = |x|$ which is continuous in every $x \in \R$. For $x \neq 0$ the absolute value is differentiable. However, for $x = 0$ we have
  \begin{align*}
    \lim_{h\rightarrow 0^+} \frac{\lvert x + h \rvert - \lvert x \rvert}{h} &= \lim_{h\rightarrow 0^+} \frac{h}{h} = 1 \\
    \lim_{h\rightarrow 0^-} \frac{\lvert x + h \rvert - \lvert x \rvert}{h} &= \lim_{h\rightarrow 0^+} \frac{-h}{h} = -1.
  \end{align*}
  Thus, $f$ is not differentiable in $x = 0$.
  Now, let $\phi \in C_c^{\infty}(\R^d)$ be a test function. Then, we have
  \begin{equation*}
    I \defeq \int_{-\infty}^{\infty}|x|\phi'(x)dx = \int_{-\infty}^{0}-x\phi'(x)dx + \int_{0}^{\infty}x\phi'(x)dx
  \end{equation*}
  Using partial integration we obtain
  \begin{equation*}
    \begin{split}
      I &= \lim_{x \rightarrow -\infty} -x \phi(x) - \int_{-\infty}^{0}-\phi(x)dx + \lim_{x \rightarrow \infty} x \phi(x) - \int_{0}^{\infty}\phi(x)dx \\
      &= -\int_{-\infty}^{\infty}\sign(x)\phi(x)dx.
    \end{split}
  \end{equation*}
  Therefore, the first order weak derivative of $f$ is the sign function. Notice, that the weak derivative is unique in a pointwise sense. Modifications on sets of zero measure do not change the integral in \eqref{eq:weak_derivative}. We understand uniqueness of the weak derivative in an almost sure sense.
\end{example}

\begin{definition}[Mollifier, \protect{\cite{evansPartialDifferentialEquations2010}}]
  Let us denote by $\nu : \R^d \rightarrow \R$, $\nu \in \CC^{\infty}_c(\R^d)$ the \emph{standard mollifier}
  \begin{equation*}
    \nu(w) \defeq \begin{cases}
      C \exp\left(-\frac{1}{1 - \norm{w}^2}\right), &\norm{w} < 1 \\
      0, &\norm{w} \geq 1,
    \end{cases}
  \end{equation*}
  where $C \defeq (\int_{\R^d}\nu(w)dw)^{-1}$ is chosen so that the integral of $\nu$ is 1. Further, define $\nu^{\epsilon}(w) = \epsilon^{-d}(w/\epsilon)$. Let $\psi \in \CL^1_{\text{loc}}(\R^d)$ be locally integrable, then we may define its mollification by
  \begin{equation*}
    \psi^{\epsilon}(w) \defeq (\nu^{\epsilon}*\psi)(w) = \int_{\R^d}\nu^{\epsilon}(w-v)\psi(v)dv = \int_{\CB(0,\epsilon)}v^{\epsilon}(v)\psi(w-v)dv,
  \end{equation*}
  where $\CB(0,\epsilon)$ is the $d$-dimensional ball of radius $\epsilon$ centered at 0. 
\end{definition}
\begin{lemma}[\protect{\cite[pp. 630]{evansPartialDifferentialEquations2010}}]
  \label{lemma:mollifiers}
  Let $\psi \in \CL^1_{\text{loc}}(\R^d)$. Then we have
  \begin{enumerate}[label=(\roman*)]
    \item $\psi^{\epsilon} \in \CC^{\infty}(\R^d)$
    \item $\psi^{\epsilon}(w) \rightarrow \psi(w)$ as $\epsilon \rightarrow 0$ for almost every $w \in \R^d$
    \item If $\psi$ is continuous, then $\psi^{\epsilon}(w) \rightarrow \psi(w)$ as $\epsilon \rightarrow 0$ uniformly on compact subset of $\R^d$
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof can be found in \cite[pp. 630]{evansPartialDifferentialEquations2010}.
\end{proof}
\begin{lemma}[\protect{\cite[Lemma 30]{liStochasticModifiedEquations2019}}]
  \label{lemma:mollifier_bound}
  Let $\epsilon \in (0,1)$ and $\psi : \R^d \rightarrow \R$ be continuous with its weak derivative $D\psi$ belonging to $G_w$. Denote by $\psi^{\epsilon}  = \moll * \psi$ the mollification of $\psi$. Then, there exists a $K \in G$ independent of $\epsilon$ such that
  \begin{equation*}
    \lvert\psi^{\epsilon}(w) - \psi(w)\rvert \leq \epsilon K(w)
  \end{equation*}
  for all $w \in \R^d$.
\end{lemma}
\begin{proof}
  The proof can be found in 
  \cite{liStochasticModifiedEquations2019}.
\end{proof}

To proof the approximation result we need to make an assumption on the growth of the drift and diffusion term of \eqref{eq:general_sde}. 
\begin{definition}[Polynomial growth, \protect{\cite{liStochasticModifiedEquations2019}}]
  Let $G$ denote the set of continuous functions $g : \R^d \rightarrow \R$ of at most \emph{polynomial growth}, i.e. $g \in G$ if there exist positive integers $\kappa_1, \kappa_2 > 0$ such that
  \begin{equation*}
    |g(x)| \leq \kappa_1(1 + \norm{x}^{2\kappa_2})
  \end{equation*} 
  for all $x \in \R^d$. Moreover, for each integer $\alpha \geq 1$ we denote by $G^{\alpha}$ the set of $\alpha$-times continuously differentiable functions $g : \R^d \rightarrow \R$ which, together with its partial derivatives up to and including order $\alpha$, belong to $G$. By $G^{\alpha}_w$ we denote the class of functions with polynomial growth and partial weak derivatives of up to order $\alpha$. 
\end{definition}
Next, let we introduce some notation for the one-step error of both SGD and the SDE:
\begin{equation*}
  \Delta(w) \defeq w_{1}^{w,0} - w, \quad \widetilde{\Delta}(w) \defeq \widetilde{W}^{w,0}_1 - w,
\end{equation*}
where $\{w_k^{w,0}: k = 0,1,\dots, N\}$ denotes the iterates of SGD with $w_0 = w$ and $\{\widetilde{W}_k^{w,0}: k = 0,1,\dots, N\}$ the solution of the SDE with initial value $W_0 = w$ at the time steps corresponding to the SGD iterates: $\widetilde{W}_k^{w,0} = W_{k\eta}$.
We introduce two lemmas to characterize the moments of the one-step error for both SGD and the SDE solution. These characterizations are required to verify the assumptions of \autoref{thm:approximation} which is the main ingredient to proof the approximation results \autoref{thm:second_order} and \autoref{cor:first_order}. We assume that the drift of and diffusion term of \eqref{eq:general_sde} have the form 
\begin{equation*}
  b(w,\eta, \epsilon) = b_0(w, \epsilon) + \eta b_1(w, \epsilon), \quad \sigma(w,\eta, \epsilon) s= \sigma_0(w, \epsilon).
\end{equation*}
\begin{lemma}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{lem:sgd_one_step}
  We define $\Delta(w) \defeq w_{1} - w$, where $w, w_{1} \in \R^d$ are the first two iterates of SGD defined in \autoref{def:sgd}. Suppose that for each $w \in \R^d$, $f \in G^1$. Then,
  \begin{enumerate}[label=(\roman*)]
    \item $\ev{\Delta_{(i)}(w)} = -\partial_{(i)} f(w) \eta$, $i = 1,\dots,d$
    \item $\ev{\Delta_{(i)}(w)\Delta_{(j)}(w)} = \partial_{(i)} f(w)\partial_{(j)} f(w) \eta^2 + \Sigma(w)_{(i,j)}\eta^2$, $i,j = 1,\dots,d$
    \item $\ev{\prod_{j=1}^3\left\lvert \Delta_{(i_j)}(w)\right\rvert} = \mathcal{O}(\eta^3)$
  \end{enumerate}
\end{lemma}
\begin{proof}
  By the definition of the SGD iterates we have
  \begin{equation*}
    \Delta(w) = W_{1} - w = w - \eta \nabla f_{\gamma}(w) - w = - \eta \nabla f_{\gamma}(w).
  \end{equation*}
  By \autoref{as:sde_model} \ref{as:bounded_gradient} and the \hyperref[thm:dominated_convergence]{dominated convergence theorem} for $i \in \{1,\dots,d\}$ we follow
  \begin{equation*}
    \ev{\Delta_{(i)}(w)} = - \eta \partial_{(i)} f(w).
  \end{equation*}
  Further, for  $i,j \in \{1,\dots,d\}$
  \begin{align*}
    \ev{\Delta_{(i)}(w)\Delta_{(j)}(w)} &= \eta^2 \ev{\partial_{(i)}f_{\gamma}(w)\partial_{(j)}f_{\gamma}(w)} \\
    &= \eta^2 \mathbb{E}\left[(\partial_{(i)}f_{\gamma}(w)- \partial_{(i)}f(w))(\partial_{(j)}f_{\gamma}(w) - \partial_{(j)}f(w)) \right. \\
    &+ \left. \partial_{(i)}f_{\gamma}(w)\partial_{(j)}f(w) + \partial_{(i)}f(w)\partial_{(j)}f_{\gamma}(w) - \partial_{(i)}f(w)\partial_{(j)}f(w) \right] \\
    &= \partial_{(i)} f(w)\partial_{(j)} f(w) \eta^2 + \Sigma(w)_{(i,j)}\eta^2.
  \end{align*}
  Lastly, for $i,j,k \in \{1,\dots,d\}$ we have
  \begin{equation*}
    \ev{\Delta_{(i)}(w)\Delta_{(j)}(w)\Delta_{(k)}(w)} = \eta^3 \ev{\partial_{(i)}f_{\gamma}(w)\partial_{(j)}f_{\gamma}(w)\partial_{(k)}f_{\gamma}(w)} = \mathcal{O}(\eta^3)
  \end{equation*}
\end{proof}
\begin{lemma}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{lem:sde_one_step}
  We define $ \widetilde{\Delta}(w) \defeq \widetilde{W}^{w,0}_1 - w$. Suppose that $b_0, b_1, \sigma \in G^3$. Then we have
  \begin{enumerate}[label=(\roman*)]
    \item  $\ev{\widetilde{\Delta}_{(i)}(w)} = b_0(w,\epsilon)_{(i)}\eta + \left[\frac{1}{2}b_0(w,\epsilon)_{(j)}\partial_{(j)}b_0(w,\epsilon)_{(i)} + b_1(w,\epsilon)_{(i)}\right] \eta^2 + \mathcal{O}(\eta^3)$, $i,j= 1,\dots,d$,
    \item $\ev{\widetilde{\Delta}_{(i)}(w)\widetilde{\Delta}_{(j)}(w)} = \left[b_0(w, \epsilon)_{(i)}b_0(w,\epsilon)_{(j)} + \sigma_0(w, \epsilon)_{(i,k)}\sigma_0(w,\epsilon)_{(j,k)}\right]\eta^2 + \mathcal{O}(\eta^3)$,
    \item $\ev{\prod_{j=1}^3\left\lvert\widetilde{\Delta}_{(i_j)}(w)\right\rvert} = \mathcal{O}(\eta^3)$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof can be found in \cite{liStochasticModifiedEquations2019}.
\end{proof}
\begin{theorem}[\protect{\cite[pp.~9]{liStochasticModifiedEquations2019}}]
  \label{thm:approximation}
  Let $T > 0, \; \eta \in (0,\min\{1,T\}), \: \epsilon \in (0,1)$ and $N = \lfloor \frac{T}{\eta} \rfloor$. Let $\alpha \geq 1$ be an integer. Suppose further that the following conditions hold:
  \begin{enumerate}[label=(\roman*)]
    \item There exists a function $\rho : (0,1) \rightarrow \R_+$ and $K_1 \in G$ independent of $\eta, \epsilon$ such that
    \begin{equation*}
      \left\lvert \ev{\prod_{j=1}^s \Delta_{(i_j)}(x)} - \ev{\prod_{j=1}^s \tilde{\Delta}_{(i_j)}(x)}\right\rvert \leq K_1(x)(\eta \rho(\epsilon) + \eta^{\alpha+1}),
    \end{equation*}
    for $s=1,2,\dots,\alpha$ and
    \begin{equation*}
      \ev{\prod_{j=1}^{\alpha+1}\left\lvert \Delta_{(i_j)}(x)\right\rvert} \leq K_1(x)\eta^{\alpha+1},
    \end{equation*}
    for all $i_j \in \{1,\dots,d\}$.
    \item For each $m \geq 1$, the $2m$-moment of $x_k^{x,0}$ is uniformly bounded with respect to $k$ and $\eta$, i.e. there exists a $K_2 \in G$, independent of $\eta, k$, such that
    \begin{equation*}
      \ev{\left\lvert x_k^{x,0} \right\rvert^{2m}} \leq K_2(x),
    \end{equation*}
    for all $k = 0, \dots, N \equiv \lfloor \frac{N}{\eta} \rfloor$.
  \end{enumerate}
  Then, for each $g \in G^{\alpha + 1}$, there exists a constant $C > 0$, independent of $\eta, \epsilon$, such that
  \begin{equation*}
    \max_{k=0,\dots,N}\norm{\ev{g(x_k)} - \ev{g(X_{k\eta})}} \leq C(\eta^{\alpha} + \rho(\epsilon))
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof of this theorem can be found in \cite{liStochasticModifiedEquations2019}.
\end{proof}
\begin{lemma}[\protect{\cite[Lemma 29]{liStochasticModifiedEquations2019}}]
  \label{lemma:moment_bound}
  Let $\{w_k: k \geq 0 \}$ be the SGD iterations defined in \eqref{eq:stochastic_gradient_descent}. Suppose for some random variable $L_{\gamma} > 0$ almost surely and 
  \begin{equation*}
    \ev{L_{\gamma}^m} < \infty
  \end{equation*}
  for all $m \geq 1$. Then, for fixed $T > 0$ and any $m \geq 1$, $\ev{\norm{w_k}^m}$ exists and is uniformly bounded in $\eta$ and $k=0,\dots,N\equiv \lfloor T / \eta \rfloor$.
\end{lemma}
\begin{proof}
  The proof can be found in \cite[Lemma 29]{liStochasticModifiedEquations2019}.
\end{proof}
\begin{theorem}[\autocite{liStochasticModifiedEquations2019}]
  \label{thm:second_order}
  Let, $T > 0$, $\eta \in (0, \min\{1,T\})$ and set $N = \lceil T/N \rceil$. Let $\{w_k:k\geq 0\}$ be the iterations defined in \eqref{eq:stochastic_gradient_descent}. Suppose the following conditions are met:
  \begin{enumerate}[label=(\roman*)]
    \item $f \equiv \E f_{\gamma}$ is twice continuously differentiable, $\nabla |\nabla f |^2$ is Lipschitz, and $f \in G^4_w$.
    \item $\nabla f_{\gamma}$ satisfies a Lipschitz condition:
    \begin{equation*}
      |\nabla f_{\gamma}(w) - \nabla f_{\gamma}(v)| \leq L_{\gamma} |w - v| \quad \text{a.s.}
    \end{equation*}
    for all $w,v \in \R^d$, where $L_{\gamma}$ is a random variable which is positive a.s. and $\ev{L_{\gamma}^m} < \infty$ for each $m \geq 1$.
  \end{enumerate}
  Define $\{W_t:t\in [0,T] \}$ as the stochastic process statisfying the SDE
  \begin{equation}
    \label{eq:second_order_sde}
    d W_t = -\nabla\left(f(W_t) + \frac{1}{4}\eta \norm{\nabla f(W_t)}^2\right)dt + \sqrt{\eta}\Sigma(W_t)^{\frac{1}{2}}dB_t
  \end{equation}
  with $W_0 = w_0$. Then, $\{W_t:t\in [0,T] \}$ is an order-2 weak approximation of SGD, i.e. for each $g \in G^3$, there exists a constant $C > 0$ independent of $\eta$ such that
  \begin{equation}
    \max_{k=0,\dots,N} \norm{\ev{g(w_k)} - \ev{g(W_{k\eta})}} \leq C \eta^2.
  \end{equation}
\end{theorem}

\begin{proof}
  The proof of the approximation result follows the following four steps
  \begin{enumerate}
    \item Using the smoothness and Lipschitz assumption we show existence and uniqueness of the solution to \eqref{eq:second_order_sde}
    \item We construct a mollified equation with coefficients that satisfy the assumption of \autoref{lem:sgd_one_step} and \autoref{lem:sde_one_step} to obtain moment estimates of the one-step errors
    \item Using \autoref{thm:approximation} we show the bound for the mollified equation and SGD
    \item Using the convergence of the mollified process we show the weak approximation inequality
  \end{enumerate}
  To show existence and uniqueness of a stochastic process $\{W_t\}_{t\geq 0}$ that satisfies equation \eqref{eq:second_order_sde} we need to show the conditions of \autoref{thm:sde_existence}.
  By assumption \autoref{as:sde_model}~\ref{as:bounded_gradient} and by \autoref{thm:dominated_convergence} we have
  \begin{align*}
    \ev{L_{\gamma}} &\geq \ev{\norm{\nabla f_{\gamma}(x) - \nabla f_{\gamma}(y)}} \\
    &\geq \norm{\ev{\nabla f_{\gamma}(x) - \nabla f_{\gamma}(y)}}
    = \norm{\nabla f(x) - \nabla f(y)}.
  \end{align*}
  Next, we show that $\Sigma(x)^{\frac{1}{2}}$ is Lipschitz continuous. For this we define $u(x) \defeq \nabla f_{\gamma}(x) - \nabla f(x)$. We see that for $x,y \in \R^d$, we have
  \begin{equation}
    \label{eq:u_lipschitz}
    \norm{u(x) - u(y)} \leq \norm{\nabla f(x) - \nabla f(y)} + \norm{\nabla f_{\gamma}(x) - \nabla f_{\gamma}(y)} \leq \left(\ev{L_{\gamma}} + L_{\gamma}\right)\norm{x-y}.
  \end{equation}
  Next, we observe that
  \begin{equation}
    \label{eq:sigma_inequality}
    \begin{split}
      \normf{\Sigma(x)^{\frac{1}{2}} - \Sigma(y)^{\frac{1}{2}}} &= \lvert \norml{[u(x)u(x)^\T]^{\frac{1}{2}}} - \norml{[u(y)u(y)^\T]^{\frac{1}{2}}} \rvert \\
      &\leq \norml{[u(x)u(x)^\T]^{\frac{1}{2}} - [u(y)u(y)^\T]^{\frac{1}{2}}}.
    \end{split}
  \end{equation}
  Further, the mapping $u \rightarrow (uu^\T)^{\frac{1}{2}} = uu^\T/\norm{u}$ is Lipschitz continuous. Let $u,v \in \R^d$. Then, we have
  \begin{equation}
    \label{eq:mapping_lipschitz}
    \begin{split}
    \normf{(uu^\T)^{\frac{1}{2}} - (vv^\T)^{\frac{1}{2}}}^2 &= \normf{uu^\T/\norm{u} - vv^\T/\norm{v}}^2 = \sum_{i,j = 1}^d (u_iu_j/\norm{u} - v_iv_j/\norm{v})^2 \\
    &= \norm{u}^2 - 2 \frac{\scp{u}{v}^2}{\norm{u}\norm{v}} + \norm{v}^2 = \norm{u}^2 - 2 \lvert \scp{u}{v} \rvert+ \norm{v}^2 \\
    &\leq \norm{u}^2 - 2 \scp{u}{v} + \norm{v}^2 = \norm{u-v}^2.
    \end{split}
  \end{equation}
  Combining equations \eqref{eq:sigma_inequality}, \eqref{eq:mapping_lipschitz}, and \eqref{eq:u_lipschitz} we follow
  \begin{align*}
    \normf{\Sigma(x)^{\frac{1}{2}} - \Sigma(y)^{\frac{1}{2}}} \leq \norm{u(x) - u(y)} \leq \left(\ev{L_{\gamma}} + L_{\gamma}\right)\norm{x-y}.
  \end{align*}
  From the arguments above and \autoref{lemma:linear_growth} we see that $b(x) = -\nabla \left( f(x) - \frac{1}{4}\eta \norm{\nabla f(x)}^2\right)$ and $\sigma(x) = \eta \Sigma(x)^{\frac{1}{2}}$ fulfill a linear growth condition. Thus, $b$ and $\sigma$ fulfill \autoref{as:sde_existence} and by \autoref{thm:sde_existence} equation \eqref{eq:second_order_sde} has a unique solution.

  Further, let $\epsilon \in (0,1)$. We define the mollified functions
  \begin{equation*}
    b_0(w, \epsilon) = - \moll * \nabla f(w), \quad b_1(w, \epsilon) = -\frac{1}{4}\moll * (\nabla\norm{\nabla f(w)}^2), \quad \sigma_0(w, \epsilon) = \moll * \Sigma(w)^{\frac{1}{2}}.
  \end{equation*}
  Next, we show that $b_0 + \eta b_1$, $\sigma_0$ satisfy a Lipschitz condition in $w \in \R^d$ uniformly in $\eta, \epsilon$.
  For that we note that a Lipschitz continuous function $\psi \R^d \rightarrow \R$ preserves Lipschitz continuity through mollificaiton. Let $w, v \in \R^d$. Then, by Jensen's inequality we have
  \begin{equation*}
    \lvert\moll*\psi(w) - \moll*\psi(v) \rvert \leq \int_{\CB(0,\epsilon)}\moll(z)\lvert\psi(w-z) - \psi(v-z)\rvert dz \leq L \norm{w-v}.
  \end{equation*}
  Thus, we have that $b_0 + \eta b_1$, $\sigma_0$ are uniformly Lipschitz. 
  % Proof linear growth condition here
  By \autoref{thm:sde_existence} the family of stochastic processes $\{W_t^{\epsilon}: \epsilon \in (0,1)\}$ satisfying 
  \begin{equation*}
    dW_t^{\epsilon} = b_0(W_t^{\epsilon}, \epsilon) + \eta b_1(W_t^{\epsilon}, \epsilon)dt + \sqrt{\eta} \sigma_0(W_t^{\epsilon}, \epsilon)dB_t, \quad W_0^{\epsilon} = w,
  \end{equation*}
  admits a unique solution for each $\epsilon \in (0,1)$.
  Next, we show that $b_0(\cdot, \epsilon),b_1(\cdot, \epsilon),\sigma_0(\cdot, \epsilon) \in G^3$ uniformly in $\epsilon$.
  We note that we have $f \in G^4_w \subset \CL^1_{\text{loc}}(\R^d)$. Thus, we have $\nabla f, \nabla \norm{\nabla f}^2, \Sigma^{\frac{1}{2}} \in \CL^1_{\text{loc}}(\R^d)$. By \autoref{lemma:mollifiers} we obtain smoothness for $b_0, b_1$ and $\sigma_0$. Further, polynomial growth is satisfied since for $\psi \in G$ and a multiindex $J$ we have
  \begin{align*}
    \moll * D^J \psi (w) &= \int_{\R^d}\moll(v-w) D^J \psi (v)dv = \nabla^J\moll(w-v)\psi (v) dv \\
    &= \nabla^J(\moll * \psi)(w),
  \end{align*}
  for $w \in \R^d$.
  Moreover, by linear growth and \autoref{lemma:inequality} we have
  \begin{align*}
    \lvert \psi^{\epsilon}(w) \rvert &\leq \int_{\CB(0, \epsilon)} \moll(v) \lvert \psi(w-v) \rvert dv \leq \int_{\CB(0, \epsilon)} \moll(v) C_1 (1 + \norm{x-y}^{C_2})dv\\
    &\leq C_1\left(1 + 2^{C_2 - 1} \norm{w}^{C_2} + 2^{C_2 - 1}\frac{1}{\epsilon^d}\int_{\CB(0,\epsilon)}\norm{v}^{C_2}dv\right).
  \end{align*}
  Notice that $\int_{\CB(0,\epsilon)}\norm{v}^{C_2} \leq \int_{\CB(0,\epsilon)}dv = \vol(\CB(0,\epsilon)) = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2}+1)}\epsilon^d$, where $\Gamma$ denotes the Gamma function. Therefore, $\psi^{\epsilon} \in G$ uniformly in $\epsilon$.
  This implies that $b_0(\cdot,\epsilon), b_1(\cdot,\epsilon), \sigma_0(\cdot, \epsilon) \in G^3$.
  Then, by \autoref{lemma:mollifier_bound}, \autoref{lemma:moment_bound}, \autoref{lem:sde_one_step} and \autoref{lem:sgd_one_step} we have all conditions of \autoref{thm:approximation} and conclude for each $g \in G^3$
  \begin{equation*}
    \max_{k=0,\dots,N}\norm{\ev{g(w_k)} - \ev{g(W^{\epsilon}_{k\eta})}} \leq C(\eta^2 + \rho(\epsilon)),
  \end{equation*}
  where $C$ is independent of $\eta$ and $\epsilon$ and $\rho(\epsilon) \rightarrow 0$ as $\epsilon \rightarrow 0$. Moreover, since $b_0(w, \epsilon) \rightarrow b_0(w, 0)$ uniformly on compact sets, we apply Theorem 20 from \cite{liStochasticModifiedEquations2019} to conclude that
  \begin{equation*}
    \sup_{t \in [0,T]}\ev{\norm{W^{\epsilon}_t - W_t}^2} \rightarrow 0 \text{ as } \epsilon \rightarrow 0.
  \end{equation*}
  Thus, we have
  \begin{align*}
    \norm{\ev{g(W_{k\eta})} - \ev{g(w_k)}} &\leq  \norm{\ev{g(W_{k\eta}^\epsilon)} - \ev{g(w_k)}} + \norm{\ev{g(W_{k\eta}^{\epsilon})} - \ev{g(W_{k\eta})}} \\
    &\leq C(\eta^2 + \rho(\epsilon)) \\
    &+ \ev{\norm{W^{\epsilon}_{k\eta} - W_{k\eta}}^2}^{\frac{1}{2}} \left( \int_0^1 \ev{\normf{\nabla^2g(\lambda W^{\epsilon}_{k\eta} + (1-\lambda)W_{k\eta})}^2}d\lambda\right)^{\frac{1}{2}}.
  \end{align*}
  Using Theorem 19 from \cite{liStochasticModifiedEquations2019} and the assumption that $\nabla^2g \in G$, the last expectation is finite and hence taking the limit $\epsilon \rightarrow 0$ yields the result.
\end{proof}
\begin{corollary}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{cor:first_order}
  Assume the same conditions as in \autoref{thm:second_order}, except that we replace (i) with $f \equiv \E f_{\gamma}$ is continuously differentiable, and $f \in G^3_w$.
  Define $\{W_t:t\in [0,T] \}$ as the stochastic process satisfying the SDE
  \begin{equation}
    \label{eq:first_order_sde}
    d W_t = -\nabla f(W_t) dt + \sqrt{\eta}\Sigma(W_t)^{\frac{1}{2}}dB_t
  \end{equation}
  with $W_0 = w_0$. Then, $\{W_t:t\in [0,T] \}$ is an order-1 weak approximation of SGD, i.e. for each $g \in G^3$, there exists a constant $C > 0$ independent of $\eta$ such that
  \begin{equation}
    \max_{k=0,\dots,N} |\ev{g(w_k)} - \ev{g(W_{k\eta})}| \leq C \eta.
  \end{equation}
\end{corollary}
Now, we want to consider an example where the analytical solution to the order-1 and order-2 is known.
\begin{example}[\autocite{liStochasticModifiedEquations2019}]
  Let $H \in \mathbb{R}^{d\times d}$ be a symmetric, positive matrix. Define the sample objective 
\begin{equation*}
  f_{\gamma}(w) = \frac{1}{2} (w - \gamma)^T H (w - \gamma) - \frac{1}{2} \text{Tr}(H)
\end{equation*}
for $\gamma \sim N(0,I)$. The total objective is 
\begin{equation*}
  \label{eq:objective_function}
  f(w) = \ev{f_{\gamma}(w)} = \frac{1}{2} w^T H w.
\end{equation*}
The stochastic differential equation becomes 
\begin{equation*}
  dW_t = -H W_t dt + \sqrt{\eta}H \cdot d\pmb{B}_t.
\end{equation*}
This process is the \emph{Ornstein-Uhlenbeck} process from \autoref{ex:ornstein_uhlenbeck} and posses the analytical solution
\begin{equation}
  \label{eq:ornstein_solution}
  W_t = e^{-t H}\left(W_0 + \sqrt{\eta}\int_0^te^{s H}H d\pmb{B}_s\right).
\end{equation}
Substituting \eqref{eq:ornstein_solution} into \eqref{eq:objective_function} we obtain
\begin{equation*}
  \begin{split}
    f(W_t) &= \frac{1}{2} W_0^Te^{-tH}He^{-tH}W_0 +
     \frac{1}{2} \sqrt{\eta}W_0e^{-tH}H\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s \\ 
    &+ \frac{1}{2} \sqrt{\eta}\left(\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s\right)^T He^{-tH}W_0 \\
    &+ \frac{1}{2} \eta \left(\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s\right)^T H\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s.
  \end{split}
\end{equation*}
By \autoref{thm:ito_isometry}, we have 
\begin{equation*}
  \begin{split}
  \ev{f(W_t)} &= \frac{1}{2}W_0^THe^{-2tH}W_0 \\
  &+ \frac{1}{2} \eta \ev{ \left(\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s\right)^T H\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s}.
  \end{split}
\end{equation*}
Now, using \eqref{eq:multivariate_ito_isometry}, we follow
\begin{equation}
  \begin{split}
    \label{eq:analytical_expected_value}
    \ev{f(W_t)} &= \frac{1}{2}W_0^THe^{-2tH}W_0 + \frac{1}{2}\eta \int_0^t\Tr\left[H^3e^{2(s-t)H}\right]ds \\
    &= \frac{1}{2}W_0^THe^{-2tH}W_0 + \frac{1}{2}\eta \sum_{i=1}^d \int_0^t\lambda_i(H)^3e^{2(s-t)\lambda_i(H)}ds \\
    & = \frac{1}{2}W_0^THe^{-2tH}W_0 + \frac{1}{4}\eta \sum_{i=1}^d \lambda_i(H)^2\left(1 - e^{-2t\lambda_i(H)}\right).
  \end{split}
\end{equation}
Using the analytical expression we have derived for the expected value of the objective function, we compare the average trajectories of 1000 SGD samples with the SDE solution. In \autoref{fig:sde_sgd} we can see that for decreasing learning rate $\eta$ the average SGD trajectory and the SDE solution approach each other.
To confirm \autoref{cor:first_order} we look at the maximum absolute difference between the expected value and the average SGD trajectories in \autoref{fig:convergence_rate}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../seminar_talk/plots/sde_sgd.pdf}
  \caption{Loss value of SDE and SGD}
  \label{fig:sde_sgd}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../seminar_talk/plots/convergence_rate.pdf}
  \caption{Maximum of the absolute difference of analytical expected value and average SGD trajectories}
  \label{fig:convergence_rate}
\end{figure}
\end{example}
% \subsection{Will be removed soon}
% A class of loss function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ relevant to machine learning applications is usually given in the form $f(x,y) = \sum_{i=1}^n f_i(x,y)$.
% \begin{equation}
%   x^{(k+1)} = x^{(k)} - \psi^k \nabla f(x^{(k)}) h +  \psi(t)\sqrt{h/b^{(k)}} \sigma_{MB}(x^{(k)})Z^{(k)}
% \end{equation}
% \begin{equation}
%   dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t))dB(t)
% \end{equation}
% \begin{equation}
%   dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t), X(t-\xi(t)))dB(t)
% \end{equation}
% A simplified model SDE model is given by 
% \begin{equation}
%   dX_t = -\nabla f(X_t)dt + (\eta \Sigma(X_t))^{\frac{1}{2}}dB_t
% \end{equation}
% where
% \begin{equation}
%   \Sigma(X_t) = \frac{1}{N} \sum_{i=1}^N (\nabla f(x) - \nabla f_i(x))(\nabla f(x) - \nabla f_i(x))^T.
% \end{equation}
% One key observation is that the model includes the full gradient as well as a covariance term that requires the evaluation of each individual gradient term. Many commonly used numerical schemes  to obtain samples of the solution of this SDE require the evaluation of the bias $b$ and the drift $\sigma$. For example, the Euler-Maruyama scheme introduced in section (\ref{subsec:SdeNumericalMethods}) requires the evaluation of $b$ and $\sigma$ in each time step. 
% Additionally, the evaluation of $\sigma$ requires the storage of $n^2$ entries, where $n \in \mathbb{N}$ is the number of parameters in the model.
% In \autocite{liValidityModelingSGD2021}, the authors introduce the time discrete approximation called stochastic variance amplified gradient (SVAG). This method does need evaluations of the full gradient. It is given the following recurrence relation
% \begin{equation}
%   X_{k+1} = X_k - \frac{\eta}{l} \nabla f^l(X_k)
% \end{equation}
% where $f^l$ is defined as
% \begin{equation}
%   f^l_{i,j}(x) = \frac{1+\sqrt{2l - 1}}{2}f_i(x) + \frac{1-\sqrt{2l - 1}}{2}f_j(x)
% \end{equation}
% for independently sampled $i,j$.
\begin{definition}[Stochastic variance amplified gradient, \cite{liValidityModelingSGD2021}]
  Let $f, f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable functions with $\ev{f_{\gamma}} \equiv f$. Further, let $\{(\gamma_{k,1}, \gamma_{k,2})\}_{k=1}^N$ be a sequence of i.i.d. random variables and define the function $f^l_{\bar{\gamma}} : \R^d \rightarrow \R$ with
  \begin{equation*}
    f^l_{\bar{\gamma}}(w) =  \frac{1+\sqrt{2l - 1}}{2}f_{\gamma_{k,1}}(w) + \frac{1-\sqrt{2l - 1}}{2}f_{\gamma_{k,2}}(w)
  \end{equation*}
  for $w \in \R^d$.
    For a given $w_{0} \in \R^d$ the iterates defined by the recursive relation
  \begin{equation*}
    w_{k+1} = w_{k} - \eta f_{\bar{\gamma}}(w_{k})
  \end{equation*}
  are called \emph{stochastic variance amplified gradient} (SVAG) iterates.
\end{definition}
\begin{theorem}[\cite{liValidityModelingSGD2021}]
  Suppose the following conditions are met:
  \begin{enumerate}[label=(\roman*)]
    \item $f \equiv \ev{f_{\gamma}}$ is $C^{\infty}$-smooth, and $f \in G^4$.
    \item $\norm{\nabla f_{\gamma}(x) - \nabla f_{\gamma}(y)} \leq L_{\gamma} \norm{x - y}$, for all $x, y \in \R^d$, where $L_{\gamma}$ is a random variable with finite moments, i.e. $\ev{L_{\gamma}^k}$ is bounded for $k \in \N$.
    \item $\Sigma^{\frac{1}{2}}(x)$ is $C^{\infty}$-smooth.
  \end{enumerate}
  Let $T > 0$ be a constant and $l$ be the SVAG hyperparameter. Define $\{W_t : t \in [0,T] \}$ as the stochastic process satisfying equation \eqref{eq:first_order_sde} and $\{w_k^{\eta/l}: 1 \leq k \leq \left\lfloor Tl/\eta \right\rfloor \}$ as the trajectory of SVAG where $w_0 = W_0$. Then SVAG $\{w_k^{\eta/l}\}$ is an order-1 approximation of the SDE solution $\{W_t\}_{t \in [0,T]}$, i.e. for each $g \in G^4$, there exists a constant $C > 0$ independent of $l$ such that
  \begin{equation*}
    \max_{k=0,\dots, \lfloor \frac{lT}{\eta} \rfloor} \left\lvert \ev{g(w^{\eta/l}_k)} - \ev{g(W_{\frac{k\eta}{l}})} \right\rvert \leq C l^{-1}.
  \end{equation*}
\end{theorem}
\section{Stochastic modified equations in deep learning}
\label{sec:smdedl}
\section{Applications}
\label{sec:applications}
\section{Conclusion}

\nomenclature{\(\nabla f\)}{Gradient of a function $f$}
\nomenclature{\(\Tr A\)}{Trace of the matrix $A \in \R^{d \times d}$, $\Tr A = \sum_{i=1}^d a_{i,i}$}
\nomenclature{\(||\cdot||_2\)}{$||\cdot||_2 : \mathbb{R}^d \rightarrow \mathbb{R}, \quad ||x||_2 = \sqrt{\sum_{i=1}^d |x_i|^2}$ for $x \in \mathbb{R}^d$}
\nomenclature{\(||\cdot||_F\)}{$||\cdot||_F : \mathbb{R}^{d \times d} \rightarrow \mathbb{R}, \quad ||A||_F = \sqrt{\sum_{i,j=1}^d |a_{i,j}|^2}$ for $A \in \mathbb{R}^{d \times d}$}


\printbibliography

\appendix
\section{Inequalities}
\begin{lemma}
  \label{lemma:inequality}
  Let $a,b \geq 0$ be non-negative real values. Then we have
  \begin{equation}
    \label{eq:inequality}
    \left(a+b\right)^n \leq 2^{n-1} \left( a^n + b^n \right),
  \end{equation}
  for $n \in \N$.
\end{lemma}
\begin{proof}
  Let $a \geq b \geq 0$ be given without loss of generality. Then for $n = 1$ the inequality holds trivially.
  Assuming \eqref{eq:inequality} holds for $n = k-1 \geq 1$, we have
  \begin{equation*}
    (a+b)^k = (a+b)^{k-1}(a+b) \leq 2^{k-2} (a^{k-1} + b^{k-1}) (a+b) = 2^{k-1}(a^k+b^k + ab^{k-1} + a^{k-1}b).
  \end{equation*}
  We see that
  \begin{align*}
    &a^k+b^k + ab^{k-1} + a^{k-1}b \leq 2 (a^k + b^k) \\
    &\iff a^k + b^k -ab^{k-1} - a^{k-1}b \geq 0 \\
    &\iff (a^{k-1} - b^{k-1}) (a-b) \geq 0,
  \end{align*}
  which holds by assumption. Thus, we have
  \begin{equation*}
    (a+b)^k \leq 2^{k-1}(a^k+b^k + ab^{k-1} + a^{k-1}b) \leq 2^{k-1} (a^k + b^k) \qedhere
  \end{equation*}
\end{proof}
\section{Convergence theorems}
\begin{theorem}[Dominated convergence theorem, \protect{\cite{evansPartialDifferentialEquations2010}[pp. 648]}]
  \label{thm:dominated_convergence}
  Assume the functions $\{f_k\}_{k=1}^{\infty} \subset \CL^1(\R^d)$ are integrable and
  \begin{equation*}
    f_k \rightarrow f \text{ almost surely.}
  \end{equation*}
  Suppose
  \begin{equation*}
    |f_k| \leq g \text{ almost surely,}
  \end{equation*}
  for some integrable function $g \in \CL^1(\R^d)$. Then, we have
  \begin{equation*}
    \int_{\R^d} f_k(x) dx \rightarrow \int_{\R^d} f(x) dx.
  \end{equation*}
\end{theorem}
\section{Auxiliary results}
\begin{lemma}
  \label{lemma:linear_growth}
  Let $f : \R^m \rightarrow \R^n$ be a Lipschitz continuous function. Then, $f$ fulfills a linear growth condition, i.e. there exists a $C > 0$ such that 
  \begin{equation*}
    \norm{f(x)}^2 \leq C (1 + \norm{x}^2).
  \end{equation*}
\end{lemma}
\begin{proof}
  Let $x \in \R^m$. Then, we have
  \begin{align*}
    \norm{f(x)}^2 &= \norm{f(x) - f(0) + f(0)}^2 \leq 2 \left( \norm{f(x) - f(0)}^2 + \norm{f(0)}^2\right) \\
    &\leq C(1+\norm{x}),
  \end{align*}
  where $C \defeq 2\max\{L^2, \norm{f(0)}^2\}$.
\end{proof}
\begin{lemma}
  \label{lem:sum_positive_definite}
  Let $A,B \in R^{d \times d}$ be symmetric, positive definite matrices. Then, the matrix $A+B$ is positive definite.
\end{lemma}
\begin{proof}
  Let $A,B \in R^{d \times d}$ be symmetric, positive definite matrices. For $x \in \R^d \setminus \{0\}$ we have
  \begin{equation*}
    x^\T(A+B)x = x^\T A x + x^\T B x > 0.
  \end{equation*}
  Thus, $A+B$ is positive definite.
\end{proof}
\end{document}
