% LTeX: enabled=false
\documentclass[12pt]{article}

\usepackage{nomencl}
\makenomenclature

% Layout
\usepackage[a4paper,includeheadfoot,margin=2.54cm]{geometry}

% Spracheinstellungen, alle Sprachen laden, letzte ist aktiv
\usepackage[english]{babel}
\usepackage{csquotes}
% hilfreiche Pakete der AMS (American Mathematical Society) laden
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{unicode-math}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{bm}
\usepackage{amsbsy}
\usepackage{aliascnt}
% Sätze, Lemmata, ..
\usepackage{amsthm}


\newtheorem{theorem}{Theorem}[section]

\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\providecommand*{\lemmaautorefname}{Lemma}

\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}

\newaliascnt{example}{theorem}
\newtheorem{example}[example]{Example}
\aliascntresetthe{example}
\providecommand*{\exampleautorefname}{Example}

\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Definition}
\aliascntresetthe{definition}
\providecommand*{\definitionautorefname}{Definition}


\newaliascnt{assumption}{theorem}
\newtheorem{assumption}[assumption]{Assumption}
\aliascntresetthe{assumption}
\providecommand*{\assumptionautorefname}{Assumption}
% Formelnummerierung
\numberwithin{equation}{section}

% For different enumeration styles
\usepackage{enumitem}
% moderne Literaturverwaltung mittels Biber, erstzt BibLaTeX,
% kann UTF-8
\usepackage{biblatex}
\addbibresource{thesis.bib}

% Einbinden von Grafik, neue Version
\usepackage{graphicx}

% Unterabbildungen
\usepackage{subcaption}

% TikZ ist kein Zeichenprogramm (doch)
\usepackage{tikz}

% listings bindet Code ein
\usepackage{listings}
\definecolor{hellgrau}{rgb}{0.90,0.90,0.90}
\definecolor{commentcol}{rgb}{0.0823,.4902,0.0}
\lstset{language=Python,
        basicstyle={\footnotesize\ttfamily},
        keywordstyle={\sffamily\bfseries},
        tabsize=2,
        numbers=left,
        numberstyle=\tt,
        stepnumber=1,
        numbersep=7pt,
        breaklines=true,
        frame=single,
        frameround=ffff,
        commentstyle=\color{commentcol},
        backgroundcolor=\color{hellgrau},
        literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {Ã}{{\~A}}1 {ã}{{\~a}}1 {Õ}{{\~O}}1 {õ}{{\~o}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
  {·}{{$\cdot$}}1
}

% Hyperlinks in Texten
\usepackage{hyperref}
\hypersetup{%
  pdftitle     = {Modeling of stochastic optimization algorithms with stochastic differential equations},
  pdfsubject   = {Masterarbeit von Jonathan Hellwig},
  pdfkeywords  = {Masterarbeit, neuronale Netze},
  pdfauthor    = {\textcopyright\ Jonathan Hellwig 2022},
  linkcolor    = red,     % links to same page
  urlcolor     = blue,     % links to URLs
  citecolor    = green!50!black,     % links to citations
  breaklinks   = true,       % links may (line) break
  colorlinks   = true,
  citebordercolor=0 0 0,  % color for \cite
  filebordercolor=0 0 0,
  linkbordercolor=0 0 0,
  menubordercolor=0 0 0,
  urlbordercolor=0 0 0,
  pdfhighlight=/P,   % moeglich /I, /P, ...
  pdfborder=0 0 0,   % keine Box um die Links!
}
\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
}
% nützliche Kurzkommandos für natürliche, ..., reelle, .. Zahlen
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\BP}{\mathbb{P}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\K}{\mathbb{K}}

\newcommand{\CF}{\mathcal{F}}
\newcommand{\CL}{\mathcal{L}}
\newcommand{\CR}{\mathcal{R}}
\newcommand{\CN}{\mathcal{N}}
\newcommand{\CV}{\mathcal{V}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\CO}{\mathcal{O}}
\newcommand{\CH}{\mathcal{H}}
\newcommand{\CG}{\mathcal{G}}
% ein paar dick gedruckte Buchstaben
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}

% Typesetting for transpose
\newcommand{\T}{\mathsf{T}}

% Mollifier 
\newcommand{\moll}{\nu^{\epsilon}}

\newcommand{\ev}[1]{\mathbb{E}\left[{#1}\right]}
\newcommand{\var}[1]{\mathbb{V}\left[{#1}\right]}
\newcommand{\norm}[1]{\lVert{#1}\rVert_2}
\newcommand{\normf}[1]{\lVert{#1}\rVert_F}
\newcommand{\norml}[1]{\lVert{#1}\rVert_{\CL^2(\Omega)}}
\newcommand{\scp}[2]{\langle{#1}, {#2}\rangle_2}

% Custom math operators
\usepackage{mathtools}
\newcommand{\defeq}{\vcentcolon=}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\vol}{Vol}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\ReLU}{ReLU}
\DeclareMathOperator{\Cov}{Cov}

% Beginn des Dokumentes
\begin{document}

% Titelseite
\thispagestyle{empty}

\begin{center}
  \includegraphics[height=2.3cm]{pics/MATH_de}
  \hfill
  \includegraphics[height=2.3cm]{pics/TUHH_de}
\end{center}

\vspace*{5em}

\begin{center}
  {\Huge
    \textsc{Relations between variants of stochastic gradient descent and stochastic differential equations}\\[2em]
  }
  {\LARGE
    Masterarbeit
  }

  \vspace*{2em}

  {\Large
    von\\
    Jonathan Hellwig\\
    aus Hamburg\\
    Matrikelnummer: 7381194\\
    Studiengang: Technomathematik\\
  }
\end{center}

\vfill
\begin{center}
  \today
\end{center}
\vfill

\begin{tabbing}
  % längste Zeile zuerst duplizieren, mit kill löschen
  Zweitprüfer:  \= Prof. Dr. Matthias Schulte\kill
  Erstprüfer: \> Dr. Jens-Peter M. Zemke\\
  Zweitprüfer:  \> Prof. Dr. Matthias Schulte\\
  Betreuer:     \> Dr. Jens-Peter M. Zemke\\
\end{tabbing}
\newpage
% this page intentionally left blank
\thispagestyle{empty}
\mbox{}
\newpage

\section*{Eidestattliche Erklärung}

Hiermit versichere ich an Eides statt, dass ich die vorliegende Arbeit im Masterstudiengang Technomathematik selbstständig verfasst und keine anderen als die angegebenen Hilfsmittel – insbesondere keine im Quellenverzeichnis nicht benannten Internet-Quellen – benutzt habe. Alle Stellen, die wörtlich oder sinngemäß aus Veröffentlichungen entnommen wurden, sind als solche kenntlich gemacht. Ich versichere weiterhin, dass ich die Arbeit vorher nicht in einem anderen Prüfungsverfahren eingereicht habe und die eingereichte schriftliche Fassung der auf dem elektronischen Speichermedium entspricht.


\vspace*{3em}

\begin{tabbing}
  \rule{.4\textwidth}{1pt} \hspace*{.2\textwidth}
  \= \rule{.4\textwidth}{1pt} \\
  Ort, Datum \> Unterschrift
\end{tabbing}

\newpage
\mbox{}
\newpage
% TOC - Table of Contents
\tableofcontents
\newpage
\listoffigures
\newpage
% LTeX: enabled=true
\section{Motivation}
\label{sec:Motivation}
In recent years, large-scale machine learning using deep neural networks has been shown to be applicable across domains. Models like GPT-3 for text generation \autocite{brownLanguageModelsAre2020}, DALL-E2 for image generation \autocite{rameshHierarchicalTextConditionalImage2022}, MuZero for solving games \autocite{schrittwieserMasteringAtariGo2020} and AlphaFold for predicting protein geometry \autocite{jumperHighlyAccurateProtein2021} are prominent examples.

% One challenging area of research remains in understanding why these models work as well as they do. What allows these models to perform well on unseen data? 
% Researchers approach this question by studying these models emperically and by creating mathematical frameworks to understand which model properties lead to good generalizations. 
While the practical usefulness of deep neural networks is beyond dispute, theoretical results that explain how and why these models work have been lacking. Researchers want to understand the following question: how does the performance of a model depend on the choice of initialization, training algorithm and hyperparameters. Empirical studies can provide insights into initialization that ensure fast convergence \cite{glorotUnderstandingDifficultyTraining2010, heDelvingDeepRectifiers2015a}. Further, adaptive variants of stochastic gradient descent, namely Ada-Grad \cite{duchiAdaptiveSubgradientMethods2011}, RMSProp \cite{geoffreyhintonnitishsrivastavaandkevinswer-NeuralNetworksMachine2012} and Adam \cite{kingmaAdamMethodStochastic2017} have been well studied for different datasets and model classes \cite{wilsonMarginalValueAdaptive2017}. Goyal et al.\ \cite{goyalAccurateLargeMinibatch2018} observe that for stochastic gradient descent training dynamics are preserved when linearly scaling both the learning rate and the batch size. 
However, the underlying mechanisms for these empirical observations are not well understood. Ideally, for an initialization $w_0 \in \R^d$, a set of hyperparameters $\CH$ and an update rule $g : \R^d \times \CH \rightarrow \R^d$ we want to have a closed form solution of the difference equation
\begin{equation*}
  w_{k+1} = g(w_k, \CH), \quad k=1,\dots, N-1,
\end{equation*}
for $N \in \N$.
By studying the solution $w_N = h(w_0, g, \CH)$, we can understand how initialization, training algorithm and hyperparameters influence the final model. Clearly, for large non-linear models explicit solutions to this difference equation are intractable. 
% Nonetheless, we might infer some properties of the its dynamics by studying
% Some theoretical research focuses on dense models in the infinite width limit \cite{leeWideNeuralNetworks2019, leeDeepNeuralNetworks2022, matthewsGaussianProcessBehaviour2022}. These works show that the trained model can be understood as a Gaussian process and investiagte properties of the trained model. 
% At this point authors make the assumption that the parameters follow an ordinary differential equation of the form
% \begin{equation*}
%   w'(t) = b(w(t),t).
% \end{equation*}

% For smooth and convex models the training process has been scrutinized extensively and we cover some basic convergence analyses in \autoref{sec:stochastic_optimization} of this thesis. However, state-of-the-art models are neither smooth nor convex and classic literature does not answer questions about how the dynamics of a given optimization algorithm influence the training process. 
For this thesis we started with the works of Orvieto et al.\ \cite{orvietoContinuoustimeModelsStochastic2019a} and Li et al.\ \cite{liValidityModelingSGD2021} which present a continuous-time model for stochastic gradient descent (SGD) and an efficient algorithm to simulate it in a deep learning setting respectively. The model is presented as a stochastic modified differential equation (SMDE) of the form
\begin{equation*}
  d W_t = -\nabla f(W_t) dt + \sqrt{\eta}\Sigma(W_t)^{1/2}d\mathbf{B}_t, \quad W_0 = w_0,\quad t \in [0, \lfloor N / \eta \rfloor],
\end{equation*}
where $f: \R^d \rightarrow \R$ is the target function, $\Sigma$ the covariance matrix of the gradients, $\eta > 0$ the learning rate and $\{ \mathbf{B}_t: t \in [0,\lfloor N / \eta \rfloor]\}$ an $m$-dimensional Brownian motion.
We set out to assess the validity of the results presented in these papers, with the goal of understanding how the continuous-time model can be used to gain insights about SGD and its variants.
For this purpose, we provide an overview of the classical analyses of SGD that make strong regularity assumptions in \autoref{sec:stochastic_optimization}. We highlight one particular analysis and show the proofs presented by Sebbouh et al.\ \cite{sebbouhAlmostSureConvergence2021} in more detail.
In \autoref{sec:BackgroundSDETheory}, we provide an introduction to stochastic differential equations following textbooks by E et al.\ \cite{eAppliedStochasticAnalysis2021} and Kloeden et al.\ \cite{kloedenNumericalSolutionStochastic2013}. Subsequently, we introduce the well-known gradient flow model and frame it as a modified differential equation following \cite{smithOriginImplicitRegularization2021}. We derive the first and second order modified equations of a linear regression model and analyze its dynamics.
In \autoref{sec:sde_model}, we present the works of \cite{liStochasticModifiedEquations2019} and explain the main approximation theorem of SMDE and its proof more extensively. This work highlights the convergence of the SMDE to the SGD iterates as the learning rate approaches zero.
In \autoref{sec:smdedl}, we summarize the works by Wu et al.\ \cite{wuNoisyGradientDescent2020a} and Li et al.\ \cite{liValidityModelingSGD2021} that describe methods to simulate SMDEs. We performed a series of experiments to validate the findings of Li et al.\ \cite{liValidityModelingSGD2021} and extend the scope of the investigation by examining different model classes.  
In \autoref{sec:applications}, we outline some applications of SMDE in improving the performance and stability of deep learning algorithms found in \cite{fontaineConvergenceRatesApproximation2021, zhouTheoreticallyUnderstandingWhy2020, liValidityModelingSGD2021}.

%  In this thesis,we want to study the dynamics of gradient descent and its stochastic version by approximating the discrete optimization process by a continuous process. First, we study the gradient flow equation and see that this equation belongs to a more general class of equations called modified equations. These equations allow us to model the dynamics of gradient descent more closely. By analyzing higher order modified equations we see the regularizing effect of finite learning rate gradient descent. Next, we generalize to the case of stochastic gradient descent which requires stochastic calculus to formulate a continuous time model. By assuming a certain noise structure we construct a continuous model that captures the dynamics of stochastic gradient descent in a distributional sense.
% However, the theorems that provide these approximation results are asymptotic and it is not clear if they hold in practice. Further, we need to make strong assumptions on the regularity of the gradients. In \autoref{sec:smdedl}, we investigate how closely the solution of the continuous process matches the discrete process. Moreover, we discuss how the assumption of the noise class influences the validity of the continuous model.
% Finally, in \autoref{sec:applications} we highlight some insights we can gain from the dynamics of the continuous model.
% Underlying the construction of these models is a class of optimization problems.
% Conventionally, models with more parameters than data points were avoided because they lead to overfitting.


% - conventional wisdom says large models lead to overfitting
% - the most successful models are overparameterized models
% - loss functions are non-convex and even not differentiable
% - traditional convergence analyses cannot be applied
% - we want to understand the dynamics of the parameters during training
% - one approach is to study networks emperically: loss landscape, noise
% - another approach: approximate the discrete process by a continuous process determined by a differential equation
% - introduce flexibilty to the differential equation to reduce gap between discrete and continuous process
% - the dynamics of a stochastic optimizer cannot be modeled well by a deterministic process
% - by asserting a certain structure to the noise one can derive stochastic differential equations
% - in this thesis we introduce theorems that describe the how discrete and continuous process are related
% - the error between continuous and discrete is made up of discretization error and noise error
% - we discuss the influence of both errors
% - we discuss that the constants in the approximation theorems can be large
% - using the dynamic model we discuss special cases, and see how the discrete process relates to generalization
% - we discuss what conclusions we can draw from the continous model
% - we explain convergence for convex functions
% - we discuss that the assumption of the approximation theorems are not fulfilled by modern neural network architecture and that the SDE approximation is heuristic

\section{Notation}
\label{sec:notation}
In this section, we introduce notation used throughout this thesis.
\subsection{Derivatives}
In the case of scalar function $f: \R \rightarrow \R$, we write $f'$ to denote the derivative.
For $n,d\in \N$ let $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_n)$ be a multiindex with $\alpha_1,\dots,\alpha_n \in \{1,\dots,d\}$ and $|\alpha| = \alpha_1 + \dots + \alpha_n$ be its \emph{order}. For a function $f : \R^d \rightarrow \R$ with partial derivatives of up to order $|\alpha|$ we write
\begin{equation*}
  \nabla^{\alpha} f = \frac{\partial^{\alpha^1}}{\partial x_1^{\alpha_1}}\dots\frac{\partial^{\alpha^n}}{\partial x_n^{\alpha_n}} f
\end{equation*}
to denote the strong partial derivative with respect to $\alpha$. For a function $f:\R^d \rightarrow \R$  with weak derivatives of up to order $|\alpha|$ (see \autoref{def:weak_derivatives}) we use $D^{\alpha}f$ to denote weak partial derivatives for the multiindex $\alpha$. 
If $f$ is once differentiable, we use $\nabla f : \R^d \rightarrow \R^d$ without a multiindex to denote its gradient which maps a $w \in \R^d$ to a vector with the entries
\begin{equation*}
  \nabla f(w)_{(i)} = \frac{\partial}{\partial w_i} f(w)  
\end{equation*}
 for $w \in \R^d$ and $i=1,\dots,d$. Further, if $f$ is twice continuously differentiable, we use $H_f : \R^d \rightarrow \R^{d \times d}$ to denote the \emph{Hessian} which maps a $w \in \R^d$ to matrix with the entries 
 \begin{equation*}
  H_f(w)_{(i,j)} = \frac{\partial^2}{\partial w_i \partial w_j} f(w)
 \end{equation*}
 for $i,j=1,\dots,d$. Let $m \in \N$. For vector valued functions $\mathbf{f} : \R^d \rightarrow \R^m$ we write 
 \begin{equation*}
  \nabla^{\alpha}\mathbf{f} = (\nabla^{\alpha}f_1, \dots, \nabla^{\alpha}f_m)^\T \in \R^{m}
 \end{equation*}
 to denote the application of the partial derivative to each component of $\mathbf{f}$ for a multiindex $\alpha$. Additionally, we write 
 \begin{equation*}
  \nabla \mathbf{f} = (\nabla f_1, \dots, \nabla f_m) \in \R^{d \times m}
 \end{equation*}
 to denote the gradient applied to each component of $\mathbf{f}$, i.e., the transposed Jacobian matrix.
% For a differentiable function $f : \R^d \rightarrow \R^m$ we write
% \begin{equation*}
%   \nabla f (w)_{i,j} = \frac{\partial}{\partial w_i} f_j(w) 
%   % \begin{bmatrix}
%   %   \frac{\partial}{\partial w_1} f_1(w) & \frac{\partial}{\partial w_1} f_2(w) & \cdots & \frac{\partial}{\partial w_1} f_n(w) \\
%   %   \frac{\partial}{\partial w_2} f_1(w) & \frac{\partial}{\partial w_2} f_2(w) & \cdots & \frac{\partial}{\partial w_2} f_n(w) \\
%   %   \vdots  & \vdots  & \ddots & \vdots  \\
%   %   \frac{\partial}{\partial w_m} f_1(w) & \frac{\partial}{\partial w_m} f_2(w) & \cdots & \frac{\partial}{\partial w_m} f_n(w) 
%   % \end{bmatrix}
% \end{equation*}
% for $w \in \R^m$ and $i = 1, \dots, d$, $j = 1, \dots, m$.
% By $H_f(w)$ we denote the \emph{Hessian} which is defined as
\subsection{Function spaces}
Let $\CC^k(\R^d, \R)$ be the space of functions $f: \R^d \rightarrow \R$ with continuous derivatives of up to order $k \in \N \cup \{\infty\}$. Further, by $\CC_c^k(\R^d, \R)$ and $\CC_b^k(\R^d, \R)$ we denote the subset of functions of $\CC^k(\R^d, \R)$ which are compactly supported and bounded respectively.
For a probability space $(\Omega, \CF, \BP)$ (see \autoref{subsec:stochastic_processes}) we use $\CL^p(\Omega, \CF, \BP)$ to denote the \emph{Lebesgue space} (see, e.g., \cite[pp.~636]{evansPartialDifferentialEquations2010})  for $p \in (1,\infty)$ and use $\CL^p(\Omega)$ if the $\sigma$-algebra $\CF$ and probability measure $\BP$ are clear. If we have $f \in\CL^p(\Omega, \CF, \BP)$, then
\begin{enumerate}[label=(\roman*)]
  \item $f$ is $\CF$-measurable,
\end{enumerate}
and
\begin{enumerate}[resume, label=(\roman*)]
  \item the norm 
  \begin{equation*}
    \lVert f \rVert_{\CL^p(\Omega, \CF, \BP)} \defeq \int_{\Omega} |f|^p d\BP < \infty
  \end{equation*}
  is finite.
\end{enumerate}
For a Lebesgue space $\CL^p(\Omega, \CF, \BP)$ we write $\CL^p_{\text{loc}}(\Omega, \CF, \BP)$ to denote the space of functions $f \in \CL^p(\widetilde{\Omega}, \CF, \BP)$ for all compact subsets $\widetilde{\Omega} \subset \Omega$.
\subsection{Matrices}
Let $d,m,n \in \N$. We use $I_d \in \R^{d \times d}$ to denote the $d$-dimensional \emph{identity} matrix and for a matrix $A \in \R^{m \times n}$ we use $A^\T \in \R^{n \times m}$ to denote its \emph{transpose}.
For a symmetric matrix $A \in \R^{d \times d}$ we write $\lambda_i(A)$ to denote the $i$-th eigenvalue for $i=1,\dots,d$, where $\lambda_1(A) \geq \lambda_2(A) \geq \dots \geq \lambda_d(A)$. Further, for $A \in \R^{n \times m}$ we write its \emph{singular value decomposition} (see, e.g., \cite{lycheNumericalLinearAlgebra2020}) as $A = V^\T D U$, where $V \in \R^{m \times m}$, $U \in \R^{n \times n}$ are orthonormal matrices, i.e., $V^\T V = I_n$ and $U^\T U = I_m$. The matrix $D \in \R^{m \times n}$ is a diagonal matrix with non-negative entries $\sigma_1(A) \geq \dots \geq \sigma_{\min\{m,n\}}(A)$ called \emph{singular values}. We use $\Tr A \defeq \sum_{i=1}^d a_{i,i}$ to denote the \emph{trace} of a matrix $A \in \R^{d \times d}$. We write $\det(A)$ to denote the determinant of a matrix $A \in \R^{d\times d}$.
\subsection{Miscellaneous}
For $g,f \in \CL^1(\R^d, \R)$ we write $f*g$ to denote the \emph{convolution} of $f$ and $g$ with 
\begin{equation*}
  (f*g)(z) = \int_{\R^d} f(x)g(z-x)dx = \int_{\R^d} g(x)f(z-x)dx,
\end{equation*}
for all $z \in \R^d$.
\section{Introduction to stochastic gradient descent}
\label{sec:stochastic_optimization}
% This chapter covers the background of optimization theory. It goes into the formal definition of optimization problems, the necessary and sufficient conditions for minimizers, and covers iterative methods for obtaining such minimizers.
%  This section covers the mathematical basis for constructing and solving optimization problems in the context of neural networks.
In this section, we begin by introducing stochastic processes and risk minimization problems in the context of deep learning applications. Next, we present results that characterize minimizers and follow with a discussion of gradient descent and its stochastic variant. In the final parts of this section, we give an overview of assumptions for the convergence analysis of SGD and extend the proofs of one particular analysis from Sebbouh et al.\ \cite{sebbouhAlmostSureConvergence2021} to give insight into the proof strategy.
 \subsection{Stochastic processes}
 \label{subsec:stochastic_processes}
 Before we introduce the formal problem definition, we cover some fundamental concepts for stochastic processes and refer to the textbook by E et al.\ \cite{eAppliedStochasticAnalysis2021} for further details. We define three concepts that are fundamental to the construction of stochastic processes: sample spaces, $\sigma$-algebras and probability measures.
 \begin{definition}[Sample space,  \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   The \emph{sample space} $\Omega$ is the set of all possible outcomes. Each element $\omega \in \Omega$ is called a \emph{sample point}.
 \end{definition}
 \begin{definition}[$\sigma$-algebra, $\sigma(B)$, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   A $\sigma$-algebra $\CF$ is a collection of subsets of $\Omega$ that satisfies
   \begin{enumerate}[label=(\roman*)]
     \item $\Omega \in \CF$,
     \item if $A \in \CF$, then $A^C \in \CF$, where $A^C = \Omega \setminus A$ is the complement of $A$ in $\Omega$,
     \item if $A_n \in \CF$ for $n \in \N$, then $\bigcup_{n=1}^{\infty} A_n \in \CF$.
   \end{enumerate}
   We write $\sigma(B)$ to denote the smallest $\sigma$-algebra generated by $B$, where $B$ is a set of subsets of $\Omega$. Further, we write $\mathcal{R}$ to denote the \emph{Borel} $\sigma$-algebra on $\R$, the smallest $\sigma$-algebra containing all open sets.
 \end{definition}
 \begin{definition}[Probability measure, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   Let $\Omega$ be a sample space and $\CF$ a $\sigma$-algebra defined on $\Omega$. The mapping $\BP : \CF \rightarrow [0,1]$ is called \emph{probability measure} if it satisfies
   \begin{enumerate}[label=(\roman*)]
     \item $\BP(\Omega) = 1$,
     \item if $A_n \in \CF$ are pairwise disjoint for $n\in\N$, i.e., $A_i \cap A_j = \emptyset$ for $i \neq j$, then
     \begin{equation*}
       \BP(\bigcup_{n=1}^{\infty}A_n) = \sum_{n=1}^{\infty}\BP(A_n).
     \end{equation*}
   \end{enumerate}
 \end{definition}
 \begin{definition}[Probability space, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
   The triplet $(\Omega, \CF, \BP)$ of sample space, $\sigma$-algebra and probability measure is called \emph{probability space}.
 \end{definition}
 One of the most important concepts in probability theory are random variables. We use independent random variables in \autoref{subsec:gd_sgd} to define the iterates of stochastic gradient descent. 
 \begin{definition}[Random variable, stochastic independence, \protect{\cite[pp.~12]{eAppliedStochasticAnalysis2021}}]
  For a probability space $(\Omega, \CF, \BP)$ a mapping $W : \Omega \rightarrow \R^d$ is called \emph{random variable}, if it is measurable, i.e., for each $A \in \mathcal{R}^d$ we have $W^{-1}(A) \in \CF$.
  Two random variables $W, V$ are called \emph{independent} if for all $A,B \in \CF$ we have
  \begin{equation*}
    \BP(\{W \in A\} \cap \{V \in B\}) = \BP(\{W \in A\})\BP(\{V \in B\}).
  \end{equation*}
  For a set of random variables $\{W_i\}_{i=1}^n$ we use the abbreviation i.i.d.\ to say that they are independent and identically distributed.
 \end{definition}
 Throughout this thesis we need the concept of expected value and covariance which characterize a probability distribution. 
 \begin{definition}[Expected value, covariance, \protect{\cite[pp.~9]{eAppliedStochasticAnalysis2021}}]
  Let $(\Omega, \CF, \BP)$ be a probability space and $W:\Omega \rightarrow \R^d$ a random variable. Then, its \emph{expected value} is defined as
  \begin{equation*}
    \ev{W} = \int_\Omega W(\omega) \BP(d\omega)
  \end{equation*}
  and its \emph{covariance} is defined as 
  \begin{equation*}
    \Cov (W) = \ev{(W - \ev{W})^\T (W - \ev{W})}.
  \end{equation*}
 \end{definition}
 In particular, we also need the conditional expectation in \autoref{subsec:convergence_analysis_of_sgd}.
 \begin{definition}[Conditional expectation, \protect{\cite[pp.~14]{eAppliedStochasticAnalysis2021}}]
  Let  $(\Omega, \CF, \BP)$ be a probability space. Then, for a random variable $W$ with $\ev{|W|} < \infty$ the \emph{conditional expectation} $Z \defeq \ev{W|\CG}$ of $W$ given a $\sigma$-algebra $\CG \subset \CF$ satisfies
  \begin{enumerate}[label=(\roman*)]
    \item $Z$ is measurable with respect to $\CG$,
    \item for any set $A \in \CG$
    \begin{equation*}
      \int_A Z(\omega)\BP(d\omega) = \int_A W(\omega)\BP(d\omega).
    \end{equation*}
    For two random variables $W,V$ with $\ev{|W|},\ev{|V|} < \infty$ we write $\ev{W|V} \defeq \ev{W|\sigma(V)}$ to denote the conditional expectation with respect to the $\sigma$-algebra generated by $V$.
  \end{enumerate}
 \end{definition}
 \begin{theorem}[\protect{\cite[pp.~14]{eAppliedStochasticAnalysis2021}}]
  \label{thm:conditional_expectation}
  Let $W, V$ be a random variables defined on a probability space $(\Omega, \CF, \BP)$ with $\ev{|W|},\ev{|V|} < \infty$. Then, for $\CG \subset \CF$ we have
  \begin{enumerate}[label=(\roman*)]
    \item for a measurable function $f$ and if $V$ is $\CG$-measurable
    \begin{equation*}
      \ev{f(V)W|\CG} = f(V)\ev{W|\CG},
    \end{equation*}
    \item the tower property
    \begin{equation*}
      \ev{\ev{W|\CG}} = \ev{W},
    \end{equation*}
    \item if $W$ is independent of $\CG$
    \begin{equation*}
      \ev{W|\CG} = \ev{W}.
    \end{equation*}
    
  \end{enumerate}
 \end{theorem}
 \begin{proof}
  The proof to (i), (ii) and (iii) can be found in the textbook from Durrett \cite{durrettProbabilityTheoryExamples2019} in Theorem 4.1.14, Theorem 4.1.13 for the special case $\CF_1 = \{\emptyset, \Omega\}$ and Example 4.1.4 respectively.
 \end{proof}
 Next, we define normal random variables. They are an important class of random variables and are required for the definition of stochastic differential equations in \autoref{sec:BackgroundSDETheory}.
 \begin{example}[\protect{\cite[pp.~11]{eAppliedStochasticAnalysis2021}}]
  Let $(\Omega, \CF, \BP)$ be a probability space. A random variable $W : \Omega \rightarrow \R^d$ is called \emph{normally distributed} if its probability measure is given by
  \begin{equation*}
    \BP(W \in A) = \frac{1}{(2 \pi)^\frac{d}{2} \det(S)^\frac{1}{2}} \int_A \exp\left(-\frac{1}{2}(x-\mu)S^{-1}(x-\mu)\right)dx, \quad A \in \mathcal{R}^d,
  \end{equation*}
  where $S \in \R^{d \times d}$ is a symmetric, positive definite matrix (see \autoref{def:pos_neg_def}) and $\mu \in \R^d$. We write $W \sim \CN(\mu, S)$ to denote that a random variable is normally distributed with parameters $S \in \R^{d \times d}$, $\mu \in \R^d$.
 \end{example}
 Stochastic gradient descent is defined as a sequence of random variables where each iterate depends on the previous iterate. We model time varying random variables by stochastic processes.
 \begin{definition}[Stochastic process, \protect{\cite[pp.~103]{eAppliedStochasticAnalysis2021}}]
  Let $(\Omega, \CF, \BP)$ be a probability space. A parametrized random variable $\{W_t\}_{t \in \bm{T}}$ defined on $(\Omega, \CF, \BP)$ for an index set $\bm{T}$ is called \emph{stochastic process}. When $\bm{T} = \N_0$, we write $\{W_n\}_{n=0}^\infty$ and for $\bm{T} = [0,\infty)$ we write $\{W_t\}_{t \geq 0}$.
 \end{definition}
 An important concept for stochastic processes are filtrations. They encode all the available information up to a certain point in time.
 \begin{definition}[Filtration, $\CF_t$-adapted, \protect{\cite[pp.~104]{eAppliedStochasticAnalysis2021}}]
   Given the probability space $(\Omega, \CF, \BP)$, a \emph{filtration} is a nondecreasing family of $\sigma$-algebras $\{\CF_t\}_{t\in \mathbf{T}}$, i.e., $\CF_s \subset \CF_t \subset \CF$ for any $s,t \in \mathbf{T}$ with $s < t$. Further, a $\R^d$-valued stochastic process $\{W_t\}_{t \in \mathbf{T}}$ on the probability space  $(\Omega, \CF, \BP)$ is called $\CF_t$-\emph{adapted} if $W_t$ is $\CF_t$-measurable, i.e., $W_t^{-1}(B) \in \CF_t$ for any $t \in \mathbf{T}$ and $B \in \mathcal{R}^d$.
 \end{definition}
 To gain more insight into filtrations we consider the following example from E et al.\cite{eAppliedStochasticAnalysis2021}.
 \begin{example}[\protect{\cite[pp.~103]{eAppliedStochasticAnalysis2021}}]
  E et al.\ consider the stochastic process $\{W_n\}_{n=0}^\infty$ of repeated coin tossing. Let $\Omega = \{T,H\}^\N$ be the sample space, where $T$ denotes tails and $H$ denotes heads. They define the initial $\sigma$-algebra as $\CF_0 = \{\emptyset, \Omega\}$. Next, they define $\CF_1 = \{\emptyset, \Omega, \{T\}, \{H\}\}$ and $\CF_2 = \sigma(\{\emptyset, \Omega, \{T\}, \{H\}, \{HH\},\{HT\}, \{TH\},\{TT\}\})$. We see that we have $\CF_0 \subset \CF_1 \subset \CF_2$. The filtration $\CF_t$ contains all possible events up to the time $t \in \N$.
 \end{example}
\subsection{Risk minimization}
% Risk minimization
% Formal definition of data distribution
% In this section, we present a general optimization problem that is often approximated in practice. The goal is to construct a model that represents a set of data in an optimal way. Further, a model should generalize well, i.e., deliver good performance on unseen data. More formally, a model is a mapping $M$ that depends on some set of parameters $w \in \R^d$ and maps input features $x \in X$ to output features $y \in Y$. The data points $(x,y) \in X \times Y$ follow a joint probability distribution $p(x,y)$. The mapping $\ell : Y \times Y \rightarrow \R$ quantifies the distance between a prediction $M(x;w)$ and a target feature and is called loss function. We follow \cite{bottouOptimizationMethodsLargeScale2018} and say a model generalizes well if it minimizes the \emph{expected risk}
% \begin{equation}
%   \min_{w \in \R^d} R(w) = \ev{\ell(M(x;w),y)}.
% \end{equation}
% In practice, the distribution $p(x,y)$ is unknown and the computation of the expected risk is not possible. Defining the minimization problem in terms of the sample average also known as the \emph{empirical risk} allow the problem to be solved. For a sample of $n \in \mathbb{N}$ data points $\{(x_i, y_i)\}_{i=1}^n$ the empirical risk is defined as 
% \begin{equation}
%   \label{eq:emperical_risk}
%   \min_{w \in \R^d}  R_n(w) = \frac{1}{n}\sum_{i=1}^n\ell(M(x_i;w),y_i).
% \end{equation}
We present the well-known risk minimization problem (see, e.g., \cite{bottouOptimizationMethodsLargeScale2018}) that is fundamental to train deep learning models. We use the risk minimization formulation to motivate the construction of SGD in \autoref{subsec:gd_sgd} and look at concrete examples in \autoref{sec:smdedl}. Now, we start by formally defining models and loss functions.
\begin{definition}[Model, loss function]
  Let $\mathcal{X}$ be the feature space and $\mathcal{Y}$ be the target space. Then, we call the mapping $M : \mathcal{X} \times \R^d \rightarrow \mathcal{Y}$ \emph{model}. The mapping $\ell : \mathcal{Y} \times \mathcal{Y} \rightarrow \R$ is called \emph{loss function}.
\end{definition}
Ideally, we want to construct a model that captures the structure of some data well. In practice, we only have access to a sample of data, but the model should yield good predictions on data not contained in the sample. Bottou et al.\ \cite{bottouOptimizationMethodsLargeScale2018} define two concepts that distinguish between the theoretical and practical optimization problem.
\begin{definition}[Expected risk, empirical risk, \protect{ \cite{bottouOptimizationMethodsLargeScale2018}}]
  Let $M : \mathcal{X} \times \R^d \rightarrow \mathcal{Y}$ be a model and $\ell : \mathcal{Y} \times \mathcal{Y} \rightarrow \R$ a loss function. Then, we call
  \begin{equation}
  \min_{w \in \R^d} R(w) = \ev{\ell(M(X;w),Y)}
\end{equation}
\emph{expected risk}, where $X,Y$ are a $\mathcal{X}$-valued and $\mathcal{Y}$-valued random variables respectively. Further, for a set of samples $\{(x_i, y_i)\}_{i=1}^n \subset (\mathcal{X} \times  \mathcal{Y})^n$ we call
\begin{equation}
  \label{eq:empirical_risk}
  \min_{w \in \R^d}  R_n(w) = \frac{1}{n}\sum_{i=1}^n\ell(M(x_i;w),y_i)
\end{equation}
\emph{empirical risk}.
\end{definition}
A model $M$ generalizes well if the difference between its empirical and expected risk $|R - R_n|$ is small. This notion will become relevant in the following sections in the discussion of gradient descent for solving \eqref{eq:empirical_risk}. Whether a model generalizes well both depends on the model architecture and the procedure to obtain the parameters $w \in \R^d$. 
The choice of the loss function depends on the type of data. For regression problems, the \emph{mean squared loss} (see \autoref{ex:linear_regression}) is typically used, while for multi-class classification problems, the \emph{cross entropy loss} is often used.

\subsection{Characterization of minimizers}
First, we introduce the concept of minimizers to understand how the empirical risk minimization problem can be solved. In the following, we consider a general formulation of the minimization problem \eqref{eq:empirical_risk} that is found in optimization textbooks (see, e.g., \cite{nocedalNumericalOptimization2006}). For a function $f:\R^d \rightarrow \R$ we define the \emph{unconstrained minimization problem}
\begin{equation}
  \min_{w \in \R^d} f(w).
\end{equation}
Next, we introduce two notions of minimizers.
\begin{definition}[Local and global minimizer, \protect{\cite[pp.~12]{nocedalNumericalOptimization2006}}]
  A value $w^\star \in \R^d$ is called (strict) \emph{global minimizer} if 
  \begin{equation}
    \label{eq:minimizer}
    f(w^\star) \leq (<) \; f(w)
  \end{equation} 
  for all $w \in \R^d$. A value $w^\star \in \R^d$ is called (strict) \emph{local minimizer} if there exists an $\epsilon > 0$ such that \eqref{eq:minimizer} holds for all $w \in \R^d$ with $\norm{w-w^\star} < \epsilon$.
\end{definition}
Notice that every global minimizer is also a local minimizer. It is difficult to obtain global minimizers in deep learning settings. One key aspect of neural networks is that we can efficiently calculate the gradient with respect to their parameters. 

If we make some assumptions on the smoothness of the function $f$, we can obtain necessary (see \autoref{thm:necessary_condition}) and sufficient (see \autoref{thm:sufficient_condition}) conditions that provide some insight on local minimizers.
\begin{theorem}[\protect{\cite[pp.~15]{nocedalNumericalOptimization2006}}]
  \label{thm:necessary_condition}
  If $w^\star \in \R^d$ is a local minimizer and $f:\R^d \rightarrow \R$ is continuously differentiable in an open neighborhood of $w^\star$, then $\nabla f(w^\star) = 0$. Further, if additionally $H_f$ exists and is continuous in an open neighborhood of $w^\star$, then $H_f(w^\star)$ is positive semidefinite (see \autoref{def:pos_neg_def}).
\end{theorem}
\begin{proof}
  The proof can be found in \autocite{nocedalNumericalOptimization2006}.
\end{proof}
This first theorem is the foundation for all commonly used optimization techniques in deep learning. The gradient can be  efficiently calculated with a procedure called backpropagation which makes use of the recursive structure of neural networks (see \autoref{sec:smdedl}). 
Using the gradient, we can find candidates for local minima by solving $\nabla f(w^\star) = 0$. This leads to the following definition.
\begin{definition}[Stationary point, \protect{\cite[pp.~15]{nocedalNumericalOptimization2006}}]
  A solution $w^\star \in \R^d$ to the equation
  \begin{equation}
  \label{eq:StationaryPoint}
    \nabla f(w^\star) = 0
  \end{equation}
  is called \emph{stationary point}.
\end{definition}
Further, if we have a stationary point $w^\star \in \R^d$, we can check whether it is a minimizer using the Hessian.
\begin{theorem}[\protect{\cite[pp.~16]{nocedalNumericalOptimization2006}}]
  \label{thm:sufficient_condition}
  Let $f:\R^d \rightarrow \R$ be twice continuously differentiable in an open neighborhood of $w^\star \in \R^d$. Suppose $w^\star$ is a stationary point and $H_f(w^\star)$ is positive definite. Then $w^\star$ is a strict local minimizer of $f$.
\end{theorem}
\begin{proof}
  The proof can be found in \cite[pp.~16]{nocedalNumericalOptimization2006}.
\end{proof}

At this point, it is unclear how to obtain stationary points for the risk minimization problem \eqref{eq:empirical_risk}, since $\nabla R_n$ is usually highly nonlinear. The next section will cover iterative methods to obtain approximate solutions to the stationary point problem.

\subsection{Gradient descent and stochastic gradient descent}
\label{subsec:gd_sgd}
For some functions it is possible to determine the solution to \eqref{eq:StationaryPoint} in analytical form. However, state of the art neural networks contain billions of parameters and in general finding a solution analytically is infeasible (see, e.g., \cite{brownLanguageModelsAre2020}). Iterative methods provide a way to start with an initial guess and improve upon that guess gradually. In particular, iterative methods considered in this thesis make use of gradient information for updating the parameters $w \in \R^d$. Before defining iterative descent methods, we introduce the well-known concept of descent directions (see, e.g., \cite{nocedalNumericalOptimization2006}.) 
\begin{definition}[Descent direction, \protect{\cite[pp.~30]{nocedalNumericalOptimization2006}}]
  Let $f: \R^d \rightarrow \R$ be a continuously differentiable function. Then a vector $g \in \R^d$ is called \emph{descent direction} at $w \in \R^d$ if it satisfies 
  \begin{equation}
    \label{eq:descent_direction}
    g^T \nabla f(w) < 0.
  \end{equation} 
\end{definition}
It is easy to show that given an initial value $w \in \R^d$ and a descent direction $g \in \R^d$ we have $f(w) > f(w + \eta g)$ for $\eta > 0$ sufficiently small. We include \autoref{lem:descent_direction} and a short proof because it illustrates why the gradient descent method leads to a decrease in function values.
\begin{lemma}
  \label{lem:descent_direction}
  Let $f : \R^d \rightarrow \R$ be a continuously differentiable function. Then there exists an $\eta > 0$ such that
  \begin{equation}
    f(w) > f(w + \eta g)
  \end{equation}
  for all $w \in \R^d$ and a descent direction $g \in \R^d$ in $w$.
\end{lemma}
\begin{proof}
  Using the mean-value theorem \cite[pp.~629]{nocedalNumericalOptimization2006} we know that there exists an $\alpha \in (0,1)$ such that 
  \begin{equation*}
    f(w + \eta g) - f(w) = \nabla f(w+\alpha\eta g)^T\eta g.
  \end{equation*}
  Now, assuming that $\nabla f (w) \neq 0$, for $0 < \epsilon <
  \frac{-\scp{\nabla f(w)}{g}}{\norm{g}}$ there exists a $\delta > 0$ such that 
  \begin{equation}
    \label{eq:continuity}
    \norm{\nabla f(w + \delta g) - \nabla f(w)} < \epsilon
  \end{equation}
  by the continuity of the gradients.
  Using \eqref{eq:descent_direction} and \eqref{eq:continuity} we have
  \begin{equation*}
    \begin{split}
     \scp{\nabla f(w + \delta g)}{\eta g} &= \scp{\nabla f(w + \delta g)- \nabla f(w)}{\eta g}+ \scp{\nabla f(w)}{\eta g}\\
      &\leq \eta \norm{\nabla f(w + \delta g)- \nabla f(w)}\norm{g} + \eta \scp{\nabla f(w)}{g} \\
      &< \epsilon \eta \norm{g} + \eta \scp{\nabla f(w)}{g} < 0.
    \end{split}
  \end{equation*}
  Choosing $\eta < \frac{\delta}{\alpha}$ we have
  \begin{equation*}
    f(w + \eta g) - f(w) = \scp{\nabla f(w+\alpha\eta g)}{g} < 0. \qedhere
  \end{equation*}
\end{proof}
This motivates the definition of one-step iterative methods.
\begin{definition}[One-step method, gradient descent, step size, learning rate \protect{\cite{nocedalNumericalOptimization2006}}]
  For descent directions $\{g_k\}_{k=1}^\infty$ and an initial value $w_{0}\in \R^d$ a sequence defined by
\begin{equation*}
  w_{k+1} = w_{k} + \eta_k g_k,
\end{equation*}
  where the sequence of non-negative real numbers $\{\eta_k\}_{k=1}^\infty$ is called \emph{step size} or \emph{learning rate} is called \emph{one-step descent method}. The special case with $g_k = -\nabla f(w_k)$ such that the sequence of iterates is given by
  \begin{equation}
    \label{eq:gradient_descent}
    w_{k+1} = w_{k} - \eta_k \nabla f(w_{k}),
  \end{equation}
  is called \emph{gradient descent} (GD).
\end{definition}

It is easy to see that for the iterates of GD we have ${g_k}^T \nabla f(w_{k}) = -\norm{ \nabla f(w_{k}) }^2 < 0$ as long as the iterates have not converged to a stationary point. GD forms the basis for many of the iterative schemes used in deep learning practice. It is important to note that after defining such an iterative scheme it is unclear whether it actually converges to a minimizer. In \autoref{subsec:convergence_analysis_of_sgd}, we discuss the classes of functions for which convergence can be established and how they relate to deep learning practice. Before diving into the theoretical analysis of descent methods let us consider an extension to the gradient descent method.
Notice that in \eqref{eq:gradient_descent} the computation of each iterate requires the computation of the gradient $\nabla f$. For objective functions of the form $f(w) = \frac{1}{n} \sum_{i=1}^n f_i(w)$, i.e., the empirical risk minimization setting, this requires the computation of each individual $\nabla f_i(w)$ for $i = 1, \dots, n$. In deep learning settings, $f_i$ is the loss $\ell(M(x_i;w), y_i)$ with respect to a single data point $(x_i, y_i)$, $i=1,\dots,n$. When each model evaluation is slow and the number of data points is large, computing the full gradient $\nabla f$ becomes prohibitively expensive. In fact, ImageNet \cite{dengImageNetLargescaleHierarchical2009}, a benchmark dataset for image classification, contains over 1.2 million images. 
% If we treat each individual gradient $\nabla f_i(w)$ as a realization of a random variable $\nabla f_{\gamma}(w)$ with expected value $\ev{\nabla f_{\gamma}(w)} = \nabla f(w)$, we can estimate $\nabla f(w)$ by calculating a sampled average of individual gradients. In particular, this leads to the definition of the following random process:
Instead of calculating the full gradient $\nabla f$ we calculate a sampled gradient $\nabla f_{\gamma}$, where $\gamma : \Omega \rightarrow \{1,\dots,n\}$ is a random variable defined on a probability space $(\Omega, \CF, \BP)$ that select a random index. We define the well-known stochastic process:
\begin{definition}[Stochastic gradient descent, \protect{\cite{polyakMethodsSpeedingConvergence1964}}]
  \label{def:sgd}
 Let $(\Omega, \CF, \BP)$ be a probability space. For a given initial value $w_{0} \in \R^d$, a sequence of step sizes $\{\eta_k\}_{k=0}^\infty$ and i.i.d.\ random variables $\{\gamma_{k}\}_{k=0}^\infty$ defined on $(\Omega, \CF, \BP)$ the stochastic process defined by
  \begin{equation}
    \label{eq:stochastic_gradient_descent}
    w_{k+1} = w_{k} - \eta_k \nabla f_{\gamma_{k}}(w_{k}),
  \end{equation}
  is called \emph{stochastic gradient descent} (SGD).
  The stochastic process defined by
  \begin{align*}
    w_{k+1} &= \mu v_{k} - \eta_k \nabla f_{\gamma_{k}}(w_{k}) \\
    v_{k+1} &= w_{k} + v_{k},
  \end{align*}
  where $\mu \in (0,1)$ is called \emph{stochastic gradient descent with momentum}. 
\end{definition}
% where $\gamma^{(k)}$ are independently distributed random variables with the same distribution as $\gamma$. This iteration method is called \emph{stochastic gradient descent} (SGD).
% In view of \eqref{eq:stochastic_gradient_descent}, let us define a class of one-step stochastic descent methods:
% \begin{definition}[Stochastic one-step method, \protect{\cite{liStochasticModifiedEquations2019}}]
%   For a given initial value $w_{0} \in \R^d$ and i.i.d. random variables $\gamma^{(k)}$ the stochastic process defined by
% \begin{equation}
%   w_{k+1} = w_{k} + \eta_k g(w_{k}, \gamma^{(k)}),
% \end{equation}
%   where $g:\R^d \times \Gamma \rightarrow \R^d$ is a measurable function, is called \emph{stochastic one-step descent method}.
% \end{definition}
% This class of stochastic iterative methods encompasses many commonly used optimization methods. 
Note that while introducing stochasticity to the optimization process allows for increased computational speed, it makes the analysis of a method more difficult. In fact, for the analysis of stochastic methods it is not sufficient to only consider the expected value $\ev{\nabla f_{\gamma}(w)}$ of the gradients. As we will see in the following sections, the covariance matrix of the gradients $\Sigma(w) = \Cov(\nabla f_\gamma(w)) = \ev{(\nabla f_{\gamma}(w) - \nabla f(w)){(\nabla f_{\gamma}(w) - \nabla f(w))}^T}$ influences the dynamics of the loss function nontrivially.
% The SGD method serves as a prototype for a lot of optimization methods. (rephrase) A simple extension is \emph{stochastic gradient descent with momentum}. 
% \begin{align*}
%   w_{k+1} &= \mu v^{(k)} - \eta \nabla f_{\gamma^{(k)}}(w_{k}) \\
%   v^{(k+1)} &= w_{k} + v^{(k)},
% \end{align*}
% where $\mu \in (0,1)$ is called \emph{momentum parameter}. (citation)


In the next section, we explore the convergence of the stochastic gradient descent for different classes of well-behaved functions.
\subsection{Convergence analysis of SGD}
\label{subsec:convergence_analysis_of_sgd}
% \begin{definition}
%   An iterative method ($\{\eta_k\}_{k=1}^\infty$, $\{g_k\}_{k=1}^\infty$) is said to converge linearly if there exists a constant $1 > C > 0$ such that 
%   \begin{equation}
%     \lim_{k \rightarrow \infty} \frac{||w_{k+1} - w^\star||}{||w_{k} - w^\star||} < M
%   \end{equation}
%   and it is said to converge sublinearly if 
%   \begin{equation}
%     \lim_{k \rightarrow \infty} \frac{||w_{k+1} - w^\star||}{||w_{k} - w^\star||} = 1
%   \end{equation}
%   holds.
% \end{definition}
% In this section, we will discuss convergence results for SGD found in literature that make strong assumption about smoothness and convexity of the target function $f : \R^n \rightarrow \R$.
% In recent years, there has been a large body of work investigating the behavior of SGD. Of particular interest is the behavior of over-parametrized deep neural networks that exhibit behavior that is counter to common wisdom: over-parametrization leads to bad generalization. However, deep learning practice shows that a high training accuracy is accompanied by high test accuracy. This leads to the following questions: what is the mechanism of SGD results in good generalization? How do learning rate and batch size affect SGD dynamics?
% First, we approach these questions by discussing convergence under strong smoothness and convexity assumptions. These analyzes provide insight into how batch size, learning rate, smoothness and convexity influence the behavior of SGD iterates. Second, we relax the convexity assumption and present weaker results.
% Finally, whether these results can be applied to modern deep learning architectures. 
%  while others investigate SGD empirically.
In this section, we summarize convergence results for SGD found in modern literature. First, we begin by introducing convergence notions for stochastic processes. Then, we proceed by considering two classes of well-behaved functions, namely convex and smooth functions. These two classes of functions are the foundation for convergence analyses of SGD. Next, we discuss the convergence of SGD under differing assumptions and have a detailed look at one particular analysis from Sebbouh et at.\ \cite{sebbouhAlmostSureConvergence2021} and expand upon its proofs. 

Unlike the iterates of GD, the iterates of SGD form a stochastic process. Hence, we need a notion of stochastic convergence. In particular, we define two notions of convergence for stochastic processes found in~\autocite{eAppliedStochasticAnalysis2021}.

\begin{definition}[Almost sure convergence, convergence in distribution, \protect{\cite[pp.~16]{eAppliedStochasticAnalysis2021}}]
Let $\{W_k\}_{k=0}^{\infty}$ be a sequence of random variables and $W$ a random variable defined on a probability space $(\Omega, \CF, \BP)$. We say the sequence $\{W_k\}_{k=0}^{\infty}$ converges \emph{almost surely} to $W$ if
\begin{equation*}
  \BP(\{\omega \in \Omega : \lim_{k \rightarrow \infty}W_k(\omega) = W(\omega)\}) = 1.
\end{equation*}
We say the sequence  $\{W_k\}_{k=0}^{\infty}$ converges to $W$ \emph{in distribution} if
  \begin{equation*}
    \lim_{k \rightarrow \infty}\ev{f(W_k) - f(W)} = 0
  \end{equation*}
  for all $f \in \CC_b(\R^n, \R)$.
\end{definition}
% convergence in expectation ist nicht definiert 
% Beweis: Erkläre die einzelnen Schritte 
Note that almost sure convergence implies convergence in distribution. Let $f \in \CC_b(\R^d, \R)$ and assume that $\{W_k\}_{k=0}^\infty$ converges to a random variable $W$ almost surely. Further, we write $\Omega_{\text{conv}} \defeq \{\omega \in \Omega : \lim_{k \rightarrow \infty}W_k(\omega) = W(\omega)\}$. Then, by the dominated convergence theorem (see \autoref{thm:dominated_convergence}) we have 
\begin{equation*}
  \lim_{k \rightarrow \infty} \ev{f(W_k) - f(W)} = \int_{\Omega} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP.
\end{equation*}
Now, by splitting $\Omega = \Omega_{\text{conv}} \cup (\Omega \setminus \Omega_{\text{conv}})$ we have
\begin{multline*}
  \int_{\Omega} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP 
  =\int_{\Omega_{\text{conv}}} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP \\
   + \int_{\Omega \setminus \Omega_{\text{conv}}} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP = \int_{\Omega_{\text{conv}}} \lim_{k \rightarrow \infty} f(W_k) - f(W) d\BP = 0.
\end{multline*}
Therefore, $\{W\}_{k=0}^\infty$ converges to $W$ in distribution.
\begin{example}
  We consider an example of a sequence of random variables $\{W_k\}_{k=1}^\infty$ that convergences to a limit $W$ in distribution but not almost surely.
  Let $W,Z$ be two stochastically independent random variables following a standard normal distribution. We define the sequence $W_k = Z$ for $k=1,2,\dots$. Then, for $f \in \CC_b(\R^d, \R)$ we have
  \begin{equation*}
    \lim_{k \rightarrow \infty} \ev{f(W_k)} = \ev{f(Z)} = \ev{f(W)}.
  \end{equation*}
  However, by stochastic independence we observe
  \begin{equation*}
    \BP(\lim_{k \rightarrow \infty} W_k = W) = \BP(Z = W) = \frac{1}{2\pi}\int_{-\infty}^\infty e^{-x^2} dx = \frac{1}{2\sqrt{\pi}}.
  \end{equation*}
  Therefore, we do not have almost sure convergence.
\end{example}
\subsubsection{Convergence assumptions}
There exists a broad body of literature discussing the convergence of stochastic gradient descent~\cite{allen-zhuConvergenceTheoryDeep2019,mertikopoulosAlmostSureConvergence2020,vaswaniFastFasterConvergence2019,gowerSGDGeneralAnalysis2019,liConvergenceStochasticGradient2019,sebbouhAlmostSureConvergence2021,bottouOptimizationMethodsLargeScale2018}.
The basic assumption for SGD to work is that approximating the sampled gradient $\nabla f_{\gamma}$ leads to a decrease of function values in expectation. Now, we note that it is not obvious that by using sampled gradients we can obtain stationary points of the objective function $f$ and it is not true in the general case. We consider several classes of functions commonly found in literature.
% In fact, in this section and in \autoref{sec:sde_model} we come to understand that SGD has additional properties induced by stochasticity. 
First, we introduce Lipschitz continuity. 
\begin{definition}[Lipschitz continuity, Lipschitz constant, \protect{\cite[pp.~10]{nesterovLecturesConvexOptimization2018}}]
  \label{def:lipschitz_continuity}
  A function $f : \R^d \rightarrow \R$ is said to be \emph{Lipschitz continuous} with \emph{Lipschitz constant} $L >0$ if
  \begin{equation*}
    |f(w) - f(v)| \leq L \norm{w - v}
  \end{equation*}
  for all $w,v \in \R^d$.
\end{definition}
Next, we define the notation of $L$-smoothness. This condition is both a common assumption to prove the convergence of SGD and the existence of solutions to the modified stochastic differential equations in \autoref{sec:sde_model}.
\begin{definition}[$L$-smoothness, \protect{\cite{bottouOptimizationMethodsLargeScale2018}}]
  \label{def:l_smooth}
  A continuously differentiable function $f : \R^d \rightarrow \R$ is said to be \emph{$L$-smooth} if its gradient is Lipschitz continuous, that is 
  \begin{equation*}
    \norm{\nabla f(w) - \nabla f(v) } \leq L \norm{w-v},
  \end{equation*}
  for $w,v \in \R^d$.
\end{definition}
\begin{example}
  In this example, we investigate $L$-smoothness of three functions. 

  Let $A \in \R^{d \times d},\; b \in \R^d,\; c \in \R$, then the quadratic function
  \begin{equation*}
    f(x) = x^\T A x + b^\T x + c
  \end{equation*}
  is $L$-smooth with constant $L = \norm{A}$. The gradient is given by $\nabla f(x)  = Ax + b$ and thus
  \begin{equation*}
    \norm{\nabla f(x) - \nabla f(y)} = \norm{A(x-y)} \leq \norm{A} \norm{x-y}.
  \end{equation*}
  The function $f(x) = x^3$ is not $L$-smooth. Let $x = n \in \N$ and $y = 0$, then we have
  \begin{equation*}
    \frac{\lvert f'(x) - f'(y) \rvert}{\lvert x - y \rvert} = \frac{\lvert3x^2 - 3 y^2 \rvert}{\lvert x-y \rvert} = 3n \rightarrow \infty
  \end{equation*}
  for $n \rightarrow \infty$.

  Lastly, the function $f(x) = \ln(x)$ is not $L$-smooth. Let $x = 1$ and $y = \frac{1}{n}$, then we have
  \begin{equation*}
    \frac{\lvert f'(x) - f'(y) \rvert}{\lvert x - y \rvert} = \frac{\lvert \frac{1}{x}- \frac{1}{y} \rvert}{\lvert x-y \rvert} = \frac{\lvert 1- n\rvert}{\lvert 1-\frac{1}{n} \rvert} = n \rightarrow \infty
  \end{equation*}
  for $n \rightarrow \infty$. 
  % This function is relevant for the construction of the cross-entropy loss which is not $L$-smooth. Therefore, we cannot apply the convergence theorems in \autoref{sec:convergence_results} and guarantee the existence of solutions to stochastic modified equations in \autoref{sec:sde_model}.
\end{example}
We restate a characterization of $L$-smoothness found in Bottou et al.\ \cite{bottouOptimizationMethodsLargeScale2018} that is needed for the following convergence theorems.
\begin{lemma}[\protect{\cite{bottouOptimizationMethodsLargeScale2018}}]
  Let $f : \R^d \rightarrow \R$ be a $L$-smooth function. Then, we have 
  \begin{equation}
    f(w) \leq f(v) + \langle \nabla f(w), v - w \rangle + \frac{L}{2} \norm{ v - w }^2,
  \end{equation}
  for all $v, w \in \R^d$.
\end{lemma}
\begin{proof}
  Let $w, v \in \R^d$. Then, by the fundamental theorem of calculus we have
  $$
  f(w) - f(v) = \int_0^1 \nabla f(w_t)^T(v-w)dt 
  $$
  for $w_t = w + t(w-v)$.
  Using this equality we obtain
  \begin{equation}
    \label{eq:Lsmoothproof1}
    f(w) - f(v) = \int_0^1 (\nabla f(w_t) - \nabla f(w))^T(v-w)dt + \nabla f(w)^T(v-w).
  \end{equation}
  Now, using Cauchy-Schwarz inequality and $L$-smoothness we have
  \begin{equation}
    \label{eq:Lsmoothproof2}
    \begin{split}
      \int_0^1 (\nabla f(w_t) - \nabla f(w))^T(v-w)dt &\leq \int_0^1 L \norm{w_t-w} \norm{v-w}dt \\
      &\leq \int_0^1 L \norm{w + t(w - v) -w} \norm{v-w}dt \\
      &= L \norm{v-w}^2 \int_0^1 t dt = \frac{L}{2} \norm{v-w}^2.
    \end{split}
  \end{equation}
  Combining \eqref{eq:Lsmoothproof1} and \eqref{eq:Lsmoothproof2} we obtain
  \begin{equation*}
    f(w) \leq f(v) + \langle \nabla f(w), v - w \rangle + \frac{L}{2} \norm{v - w}^2. \qedhere
  \end{equation*}
\end{proof}
Gower et al.\ \cite{gowerSGDGeneralAnalysis2019} present an extension of $L$-smoothness which combines a condition on the sampling noise and the smoothness of the target function.
\begin{definition}[$L$-\emph{smooth in expectation}, \protect{\cite{gowerSGDGeneralAnalysis2019}}]
  A continuously differentiable  function $f_{\gamma}:\R^d \rightarrow \R$ is said to be $L$-\emph{smooth in expectation} with respect to the distribution $\mathcal{D}$ if there exists $L = L(f, \mathcal{D}) > 0$ such that 
  \begin{equation}
    \mathbb{E}[\norm{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)}^2] \leq 2 L(f(w) - f(w^\star)),
  \end{equation}
  for all $w \in \R^d$. 
\end{definition}
% After discussing smoothness conditions, we  convexity. Convex function are well-behaved in the numerical optimization setting.
We restate the definition of convex functions and present some of their properties found in the textbook by Boyd et al.\ \cite{boydConvexOptimization2004}.
\begin{definition}[Convexity, \protect{\cite[pp.~67]{boydConvexOptimization2004}}]
  A function $f : \R^d \rightarrow \R$ is said to be \emph{convex} if 
  \begin{equation}
    f(tw+(1-t)v) \leq tf(w)+(1-t)f(v)
  \end{equation}
  for all $w,v \in \R^d$ and $t \in [0,1]$.
\end{definition}
For continuously differentiable functions we have the following equivalent characterization of convex functions:
\begin{lemma}[\protect{\cite[pp.~69]{boydConvexOptimization2004}}]
  \label{lemma:convexity}
  Let $f : \R^d \rightarrow \R$ be a continuously differentiable function. The function $f$ is convex if and only if we have
  \begin{equation*}
    f(v) \geq f(w) + \langle \nabla f(w), v-w \rangle,
  \end{equation*}
  for all $v,w \in \R^d$.
\end{lemma}
\begin{proof}
  The proof can be found in \autocite{boydConvexOptimization2004}.
\end{proof}

Next, we present a notion of strong convexity that is used in the convergence literature of SGD \autocite{sebbouhAlmostSureConvergence2021,moulinesNonAsymptoticAnalysisStochastic2011,bottouOptimizationMethodsLargeScale2018}. 
\begin{definition}[Strong convexity, \protect{\cite[pp.~459]{ gowerSGDGeneralAnalysis2019, boydConvexOptimization2004}}]
  A continuously differentiable function $f : \R^d \rightarrow \R$ is said to be $\mu$-\emph{strongly convex} if
  \begin{equation*}
    f(v) \geq f(w) + \langle \nabla f(w), v - w \rangle + \frac{\mu}{2} \norm{v - w}^2
  \end{equation*}
  for all $v, w \in \R^d$. Further, if $f : \R^d \rightarrow \R$ has a unique global minimizer $w^\star \in \R^d$ we call $f$ \emph{strongly quasi-convex} if
  \begin{equation*}
    f(w^\star) \geq f(w) + \langle \nabla f(w), w^\star - w \rangle + \frac{\mu}{2} \norm{w^\star - w}^2
  \end{equation*}
  for all $w \in \R^d$.
\end{definition}
Note that for continuously differentiable functions \autoref{lemma:convexity} immediately implies that convexity follows from strong convexity.

 Additionally, Bottou et al.\ \cite{bottouOptimizationMethodsLargeScale2018} present two assumptions for the SGD iterates: a bound on the first moment of the gradient and a growth condition for the second moment.
\begin{assumption}[\protect{\cite{bottouOptimizationMethodsLargeScale2018}}]
  \label{as:sgd_convergence}
  Let $f : \R^d \rightarrow \R$ be a continuously differentiable function and let $\{w_k\}_{k=0}^\infty$ be the iterates given by SGD in \eqref{eq:stochastic_gradient_descent}. Then they satisfy the following conditions:
  \begin{enumerate}[label=(\roman*)]
    % \item There exists a scalar $\mu > 0$ such that 
    % \begin{equation*}
    %   \norm{\ev{\nabla f_{\gamma}(w_k)}} \leq \mu,
    % \end{equation*}
    % for all $w \in \R^d$.
    \item The sequence of iterates $\{w_k\}_{k=0}^\infty$ is contained in an open set over which $f$ is bounded from below by a scalar $f_{\text{inf}}\in \R$.
    \item There exist scalars $\mu_1, \mu_2 > 0$ such that 
    \begin{equation*}
      \label{eq:variance_linear_growth}
      \ev{\lVert \nabla f_{\gamma}(w_k) \rVert^2} \leq \mu_1 + \mu_2 \lVert \nabla f(w_k) \rVert^2,
    \end{equation*}
    for all $k=1,2,\dots$.
  \end{enumerate}
\end{assumption}

Finally, we restate the famous Robbins and Monro conditions for the learning rate. They have the following intuitive interpretation: If the learning rate is decreased too strongly, the steps towards the minimum are too small and the iterates never reach a stationary point. On the other hand, if the learning rate is not decreased sufficiently, the iterates oscillate in a neighborhood of the stationary point.
\begin{definition}[Robbins and Monro conditions, \cite{robbinsStochasticApproximationMethod1951}]
  A sequence of non-negative real numbers $\{\eta_k\}_{k=0}^\infty$ is said to fulfill the \emph{Robbins and Monro conditions} if
  \begin{equation*}
    \sum_{k=0}^\infty \eta_k = \infty, \quad \sum_{k=0}^\infty \eta_k^2 < \infty.
  \end{equation*}
\end{definition}


\subsubsection{Convergence results}
\label{sec:convergence_results}
In recent years, there have been a number of developments in the field of stochastic approximation, building on the initial convergence results from Robbins et al.\ \autocite{robbinsStochasticApproximationMethod1951}. These developments include almost sure convergence of the iterates \autocite{zhouStochasticMirrorDescent2017, nguyenSGDHogwildConvergence2018, sebbouhAlmostSureConvergence2021}, as well as convergence of function values and gradients in expectation \autocite{bottouOptimizationMethodsLargeScale2018}.  
Bottou et al.\ \autocite{bottouOptimizationMethodsLargeScale2018} present convergence results for $L$-smooth objectives that satisfy \autoref{as:sgd_convergence}. They show that for strongly convex objectives convergence of function values in expectation is achieved and for non-convex objectives convergence of a subsequence of gradients to a stationary point is achieved.  
Gower et al.\ \autocite{gowerSGDGeneralAnalysis2019} introduce the concept of expected smoothness which combines $L$-smoothness with a bound on the variance in a novel way. Using this notion, in combination with strong quasi-convexity, they show convergence in expectation for the iterates with decreasing step sizes.

In the following, we examine an analysis by Sebbouh et al.\ \autocite{sebbouhAlmostSureConvergence2021} that provides insight into the assumptions and techniques necessary proving convergence.
We note that we extend on some minor details of the proof to make it more accessible to the reader.
Sebbouh et al.\ \autocite{sebbouhAlmostSureConvergence2021} show that combining convexity and $L$-smoothness leads to the following lemma.

\begin{lemma}[\protect{\cite{gowerSGDGeneralAnalysis2019}}]
  \label{lemma:gradient_inequality}
  Let $\gamma : \Omega \rightarrow \{1,\dots,n\}$ be a random variable defined on a probability space $(\Omega, \CF, \BP)$ and let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function with $f \defeq \ev{f_{\gamma}}$. If $f_{\gamma}$ is both $L_{\gamma}$-smooth, convex and $w^\star$ is a minimizer of $f$, then we have
  \begin{equation}
    \ev{\norm{\nabla f_{\gamma}(w)}^2} \leq 4 \CL (f(w) - f(w^\star)) + 2 \sigma^2,
  \end{equation}
  where $\sigma^2 \defeq \sup_{w \in \R^d} \ev{\norm{\nabla f_\gamma(w)}^2}$ and $\CL \defeq \sup_{\gamma} L_{\gamma}$.
\end{lemma}
\begin{proof}
  Let $w, w^\star \in \R^d$ where $w^\star$ is a minimizer of $f$. We use an inequality from the textbook \cite[pp.~67, (2.1.10)]{nesterovLecturesConvexOptimization2018} which states that for the $L$-smooth function $f_\gamma$ we have
  \begin{equation*}
    f_\gamma(w^\star) + \scp{\nabla f_\gamma(w^\star)}{w - w^\star} + \frac{1}{2 L_\gamma}\norm{\nabla f_\gamma(w^\star) - \nabla f(w)} \leq f_\gamma(w).
  \end{equation*}
  
  Rearranging yields
  \begin{align*}
    \norm{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)}^2 &\leq 2L_{\gamma}(f_{\gamma}(w) - f_{\gamma}(w^\star) - \langle \nabla f_{\gamma}(w^\star), w - w^\star \rangle) \\
    &\leq 2\CL(f_{\gamma}(w) - f_{\gamma}(w^\star) - \langle \nabla f_{\gamma}(w^\star), w - w^\star \rangle).
  \end{align*}
  Thus, by taking the expectation on both sides and using $f \equiv \ev{f_\gamma}$ we have
  \begin{equation*}
    \ev{\norm{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)}^2 } \leq 2\CL(f(w) - f(w^\star) - \langle \nabla f(w^\star), w - w^\star \rangle).
  \end{equation*}
  Since $w^\star$ is a minimizer from \autoref{thm:necessary_condition} it follows $\nabla f(w^\star) = 0$ and we have
  \begin{equation*}
    \ev{\norm{\nabla f_{\gamma}(w) - f_{\gamma}(w^\star)}^2 } \leq 2\CL(f(w) - f(w^\star)).
  \end{equation*}
  Now, by using \autoref{lemma:inequality} and the previous inequality we have
  \begin{align*}
    \ev{\norm{\nabla f_{\gamma}(w)}^2} &= \ev{\norm{(\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)) + \nabla f_{\gamma}(w^\star)}^2} \\
    &\leq 2( \ev{\norm{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(w^\star)}^2 } + \ev{\norm{\nabla f_{\gamma}(w^\star)}^2}) \\
    &\leq 4\CL(f(w) - f(w^\star)) + 2\sigma^2. \qedhere
  \end{align*}
\end{proof}
We now present the following result from \autocite{sebbouhAlmostSureConvergence2021} which relates the distances to the minimizer $w^\star$ for two consecutive iterates.
\begin{lemma}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  \label{lemma:sgd_iterates}
  Let $\gamma : \Omega \rightarrow \{1,\dots,n\}$ be a random variable defined on a probability space $(\Omega, \CF, \BP)$ and let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function that is $L_{\gamma}$-smooth and convex. Further, let $\{w_{k}\}_{k=0}^{\infty}$ be the iterates of SGD defined in \eqref{eq:stochastic_gradient_descent} and let $\eta_k \leq \frac{1}{4 \CL}$ for $k=1,2,\dots$. Then, we have
  \begin{equation*}
    \E[\norm{w_{k+1} - w^\star}|w_k] + \eta_k (f(w_{k}) - f(w^\star))) \leq \norm{w_{k} - w^\star}^2 + 2{\eta_k}^2\sigma^2,
  \end{equation*}
  for $k=1,2,\dots$.
\end{lemma}
\begin{proof}
  First, recall that the iterates of SGD are given by
  \begin{equation*}
    w_{k+1} = w_{k} - \eta_k \nabla f_{\gamma_k}(w_{k}).
  \end{equation*}
  We use this recurrence relation to obtain
  \begin{equation}
    \label{eq:iterate_norm}
    \begin{split}
      \norm{w_{k+1} - w^\star}^2 &= \norm{w_{k} - w^\star - \eta_k \nabla f_{\gamma_k}(w_{k})}^2 \\
    &= \norm{w_{k} - w^\star}^2 + 2 \eta_k \scp{\nabla f_{\gamma_k}(w_{k})}{ w^\star - w_{k}} + {\eta_k}^2\norm{\nabla f_{\gamma_k}(w_{k})}^2.
    \end{split}
  \end{equation}
  % Next, we note that the first property of \autoref{thm:conditional_expectation} implies $\ev{\nabla f_{\gamma_k}(w_k)|w_k} = \nabla f(w_k)$ and $\ev{\norm{w_{k} - w^\star}^2|w_k} = \norm{w_{k} - w^\star}^2$.
  At this point, we note that Sebbouh et al.\ glimpsed over one detail in this proof. We expand on the calculation of the conditional expectation $\ev{\nabla f_{\gamma_k}(w_k)| w_k}$.
  Let $\{S_k\}_{k=0}^\infty$ be a sequence of random vectors with $\ev{S_k} = \left(\frac{1}{n}, \dots, \frac{1}{n}\right)^\T$ and $\nabla f_{\gamma_k}(w_k) = \nabla \mathbf{f}(w_k)S_k$ for $k=0,1,\dots$,
  where $\nabla \mathbf{f}(w) = \left(\nabla f_1(w), \dots, \nabla f_n(w)\right)$ following a approach by Wu et al.\ \cite{wuNoisyGradientDescent2020a}. Using the properties of the conditional expectation found in \autoref{thm:conditional_expectation} we have
  \begin{multline*}
    \ev{\nabla f_{\gamma_k}(w_k)|w_k} = \ev{\nabla \mathbf{f}(w_k)S_k|w_k} \stackrel{\text{(i)}}{=} \nabla \mathbf{f}(w_k) \ev{S_k|w_k} \\
    \stackrel{\text{(iii)}}{=} \nabla \mathbf{f}(w_k) \ev{S_k} = \nabla \mathbf{f}(w_k) \left(\frac{1}{n}, \dots, \frac{1}{n}\right)^\T = \frac{1}{n}\sum_{i=1}^n \nabla f_i(w_k) = \nabla f(w_k).
  \end{multline*}
  Also, by \autoref{thm:conditional_expectation} (i) we have $\ev{\norm{w_{k} - w^\star}^2|w_k} = \norm{w_{k} - w^\star}^2$.
  We take the conditional expectation on both sides of \eqref{eq:iterate_norm} and obtain
  \begin{equation*}
    \E[\norm{w_{k+1} - w^\star}^2|w_k] = \norm{w_{k} - w^\star}^2 + 2 \eta_k \scp{\nabla f(w_{k})}{w^\star - w_{k}}+ {\eta_k}^2\E[\norm{\nabla f_{\gamma_k}(w_{k})}^2|w_k].
  \end{equation*}
  Now, using \autoref{lemma:gradient_inequality} and \autoref{lemma:convexity} we obtain
  \begin{equation*}
    \E[\norm{w_{k+1} - w^\star}^2|w_k] \leq \norm{w_{k} - w^\star}^2 + 2 \eta_k(2\CL \eta_k - 1)(f(w_{k}) - f(w^\star))+ {\eta_k}^22\sigma^2.
  \end{equation*}
  Now, using $\eta_k \leq \frac{1}{4 \CL}$ and $f(w) - f(w^\star) \leq 0$ for all $w \in \R^d$ we have
  \begin{equation*}
    \E[\norm{w_{k+1} - w^\star}^2|w_k] \leq \norm{w_{k} - w^\star}^2 - \eta_k(f(w_{k}) - f(w^\star))+ {\eta_k}^22\sigma^2. \qedhere
  \end{equation*}
\end{proof}
We now present a non-asymptotic bound for the convex and $L$-smooth case from Sebbouh et al.\ \cite{sebbouhAlmostSureConvergence2021} and expand on some details of the proof.
\begin{theorem}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  \label{thm:SGD_bound}
  Let $\gamma : \Omega \rightarrow \{1,\dots,n\}$ be a random variable defined on a probability space $(\Omega, \CF, \BP)$ and let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function that is both $L_{\gamma}$-smooth and convex. Then, for the iterates of SGD defined in \eqref{eq:stochastic_gradient_descent} we have
  \begin{equation*}
    \ev{f(\widetilde{w}_k) - f(w^\star)} \leq \frac{\norm{w_{0} - w^\star}^2}{\sum_{i=0}^{k-1}\eta_i} + 2 \sigma^2 \frac{\sum_{i=0}^{k-1}\eta_i^2}{\sum_{i=0}^{k-1}\eta_i},
  \end{equation*}
  for $k = 1, 2, \dots$, where $f \defeq \ev{f_\gamma}$ and
  \begin{equation*}
    \widetilde{w}_k = \sum_{i=0}^{k-1}\frac{\eta_i}{\sum_{j=0}^{k-1}\eta_j}w_i.
  \end{equation*}
\end{theorem}
\begin{proof}
  Using \autoref{lemma:sgd_iterates} we have
  \begin{equation*}
    \E[\norm{w_{k+1} - w^\star}|w_k] + \eta_k (f(w_{k}) - f(w^\star)) \leq \norm{w_{k} - w^\star}^2 + 2{\eta_k}^2\sigma^2.
  \end{equation*}
  At this point, we add that we have $\ev{\E[\norm{w_{k+1} - w^\star}|w_k]} = \ev{\norm{w_{k+1} - w^\star}}$ by \autoref{thm:conditional_expectation} (ii).
  Summing over $k$ and taking the expected value we have
  \begin{equation*}
    \sum_{t=0}^{k-1}\ev{\norm{w_{t+1} - w^\star}} + \sum_{t=0}^{k-1} \eta_t \ev{f(w_{t}) - f(w^\star)} \leq \sum_{t=0}^{k-1} \ev{\norm{w_{t} - w^\star}^2} + \sum_{t=0}^{k-1} 2{\eta_t}^2\sigma^2.
  \end{equation*}
  Rearranging and using the properties of telescoping sums we have
  \begin{equation*}
    \sum_{t=0}^{k-1} \eta_t \ev{f(w_{t}) - f(w^\star)} \leq \ev{\norm{w_{0} - w^\star}^2} - \ev{\norm{w_{k+1} - w^\star}^2} + \sigma^2\sum_{t=0}^{k-1} 2{\eta_t}^2.
  \end{equation*}
  Now, normalizing with the sum of the learning rates, we use Jensen's inequality to obtain
  \begin{equation*}
    \ev{f(\widetilde{w_{t}}) - f(w^\star)} \leq \sum_{t=0}^{k-1} \frac{\eta_t}{\sum_{i=0}^{k-1}\eta_i} \ev{f(w_{t}) - f(w^\star)} \leq \frac{\norm{w_{0} - w^\star}^2}{\sum_{i=0}^{k-1}\eta_i} + \frac{2\sigma^2\sum_{t=0}^{k-1} {\eta_t}^2}{\sum_{i=0}^{k-1}\eta_i}. \qedhere
  \end{equation*}
\end{proof}

\autoref{thm:SGD_bound} shows the relation between learning rate and gradient noise for the optimality gap. Note that for a fixed learning rate $\eta > 0$ we have
\begin{equation*}
  \E [f(\widetilde{w_{k}}) - f(w^\star)] \leq \frac{\norm{w_{0} - w^\star}^2}{\eta k} + 2 \sigma^2 \eta.
\end{equation*}
While the first term converges to zero as $k \rightarrow \infty$, the second term remains constant. This gap is induced by gradient noise scaled with the learning rate. Intuitively, this is verified by the fact that each gradient step is scaled by the learning rate.

% \begin{lemma}[\protect{\cite{robbinsCONVERGENCETHEOREMNON1971}}]
%   Let $\{\CF_n\}_{n=0}^\infty$ be a filtration and $\{V_n\}_{n=0}^\infty, \{U_n\}_{n=0}^\infty, \{Z_n\}_{n=0}^\infty$ be $\{\CF_n\}_{n=0}^\infty$-adapted nonnegative processes such that $\sum_{n=0}^\infty Z_n < \infty$ and for all $n \geq 0$
%   \begin{equation*}
%     \ev{V_{n+1}|\CF_n} + U_{n+1} \leq V_n + Z_n.
%   \end{equation*}
%   Then, $\{V_n\}_{n=0}^\infty$ converges and $\sum_{n=0}^\infty U_n < \infty$ almost surely.
% \end{lemma}
We introduce the well-known Landau notation to state a convergence theorem for the average SGD iterates.
\begin{definition}[Order notation, see, e.g., \protect{\cite[pp.~631]{nocedalNumericalOptimization2006}}]
  For two sequences $\{w_k\}_{k=1}^\infty$ and $\{v_k\}_{k=1}^\infty$ in $\R^d$ we write $w_k = o(v_k)$ if
  \begin{equation*}
    \lim_{k \rightarrow \infty} \frac{\norm{w_k}}{\norm{v_k}} = 0.
  \end{equation*}
  We write $w_k = \CO(v_k)$, if there exists a constant $C > 0$ and a $k_0 \in \N$ such that
  \begin{equation*}
    \norm{w_k} \leq C \norm{v_k}
  \end{equation*}
  for all $k \geq k_0$.
\end{definition}
% \begin{definition}[Big $\CO$ notation]
%   Let $f,g,h: \R^d \rightarrow \R^d$ be given. If there exists a constant $C > 0$ such that
%   \begin{equation*}
%     \norm{f(w) - h(w)} \leq C \norm{g(w)}
%   \end{equation*}
%   we write $f(w) = h(w) + \CO(g(w))$ for $w \in \R^d$.
% \end{definition}
The following theorem by Sebbouh et al.\ shows the dependence of the convergence on the learning rates.
\begin{theorem}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  \label{thm:almost_sure_convergence}
  Let $\gamma : \Omega \rightarrow \{1,\dots,n\}$ be a random variable defined on a probability space $(\Omega, \CF, \BP)$ and let $f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable function that is both $L_{\gamma}$-smooth and convex. Then, for the iterates of SGD defined in \eqref{eq:stochastic_gradient_descent} we have 
  \begin{equation*}
    f(\widehat{w_{k}}) - f(w^\star) = o\left(\frac{1}{\sum_{t=0}^{k-1}\eta_t}\right)
  \end{equation*}
  almost surely, where $\widehat{w}_{k+1} = v_k w_k + (1-v_k)\widehat{w}_k, \; \widehat{w}_0 = w_0$ and $v_k = \frac{2\eta_k}{\sum_{j=0}^k\eta_j}$ for $k=0,1,\dots$.
\end{theorem}
\begin{proof}
  The proof can be found in \autocite{sebbouhAlmostSureConvergence2021}.
\end{proof}
\autoref{thm:almost_sure_convergence} implies the following corollary.
\begin{corollary}[\protect{\cite{sebbouhAlmostSureConvergence2021}}]
  \label{cor:sgd_convergence}
  Let $\gamma : \Omega \rightarrow \{1,\dots,n\}$ be a random variable defined on a probability space $(\Omega, \CF, \BP)$ and let $f_{\gamma}: \R^d \rightarrow \R$ be a $L_\gamma$-smooth and convex function. Further, $0 < \eta \leq \frac{1}{4\CL}$ and $\epsilon > 0 $. Then, for the iterates of SGD defined in \eqref{eq:stochastic_gradient_descent} we have 
  \begin{enumerate}[label=(\roman*)]
    \item if $\sigma^2 \neq 0$ and $\eta_k = \frac{\eta}{k^{1/2+\epsilon}}$,
    \begin{equation*}
      f(\widehat{w_{k}}) - f(w^\star) = o\left(\frac{1}{k^{1/2-\epsilon}}\right)
    \end{equation*}
    \item if $\sigma^2 = 0$ and $\eta_k = \eta$
    \begin{equation*}
      f(\widehat{w_{k}}) - f(w^\star) = o\left(\frac{1}{k}\right),
    \end{equation*}
  \end{enumerate}
  almost surely, where $\widehat{w}_{k+1} = v_k w_k + (1-v_k)\widehat{w}_k, \; \widehat{w}_0 = w_0$ and $v_k = \frac{2\eta_k}{\sum_{j=0}^k\eta_j}$ for $k=0,1,\dots$.
\end{corollary}
\begin{proof}
  The proof can be found in \cite{sebbouhAlmostSureConvergence2021}.
\end{proof}
  This corollary reveals an interesting property of overparametrized models for which we have $\sigma^2 = 0$. For a fixed learning rate we have linear convergence. In \autoref{sec:smdedl}, we look at the behavior of such models.
% \begin{assumption}
%   \begin{enumerate}
%     \item The sequence of iterates $w_{k}$ is contained in an open set over which $f : \R^d \rightarrow \R$ is bounded from below by a scalar $f_{inf}$.
%     \item There exist scalars $\mu_G \geq \mu > 0$ such that, for all $k \in \mathbb{N}$, \begin{align}
%       \nabla {f(w_{k})}^T\mathbb{E}(g(w_{k}, \gamma^{(k)})) &\leq || \nabla f (w_{k}) ||^2 \\
%       || \mathbb{E}(g(w_{k}, \gamma^{(k)}))|| &\leq \mu_G ||\nabla f (w_{k})||.
%     \end{align}
%     \item There exist scalars $M \leq 0$ and $M_V \leq 0$ such that, for all $k \in \mathbb{N}$, \begin{equation}
%       \mathbb{V}(g(w_{k}, \gamma^{(k)})) \leq M + M_V ||\nabla f(w_{k}) ||^2
%     \end{equation} 
%   \end{enumerate}
% \end{assumption}
% (citation)

\subsection{Summary and outlook}
In this section, we introduced gradient descent and stochastic gradient descent for solving the empirical risk minimization problem. Further, we looked at theoretical analyses of SGD and discussed common assumptions. In particular, we highlighted an almost sure analysis that gives insight into the behavior of overparameterized models. 

At this point, we note that most of the analyses in literature make strong smoothness and convexity assumptions on the target function. As we see in \autoref{sec:smdedl}, modern deep learning model are neither convex nor smooth. Further, the analyses presented show the asymptotic behavior of SGD iterates and do not give insights in to the dynamics during optimization. 
In \autoref{sec:sde_model}, we see that SGD can be approximated by a continuous-time stochastic process. We can gain insight into the behavior of SGD by analyzing this continuous-time process. However, to define this continuous-time process we must first introduce the concept of stochastic differential equations. This is the topic of the next section.


\section{Stochastic differential equations}
\label{sec:BackgroundSDETheory}
In this section, we introduce the concept of stochastic differential equations (SDEs). First, we define a new notion of integration: Itô integration. Then, by reformulating an ordinary differential equation (ODE) as an integral equation, we generalize ODEs to stochastic processes by building on Itô integrals. Subsequently, we discuss the existence and uniqueness of solutions to SDEs. Finally, we introduce a framework for approximating solutions to SDEs numerically.
\subsection{Ordinary differential equations}
A system of ordinary differential equations are equations of the form
\begin{equation}
  \label{eq:initial_value_problem}
  w'(t) = \mathbf{a}(w(t),t), \quad w(0) = w_0
\end{equation}
where $\mathbf{a} : \R^d \times [0,T] \rightarrow \R^d$ is a continuous function. Equivalently, we can formulate the integral equation
\begin{equation}
  w(t) = w_0 + \int_0^t \mathbf{a}(w(s),s)ds.
\end{equation}
The existence and uniqueness of ODE solutions is given by the following theorem:
\begin{theorem}[\protect{\cite[pp.~73]{ahmadTextbookOrdinaryDifferential2015}}]
  \label{thm:ode_existence}
  Let $a : \R^d \times [0,T] \rightarrow \R$ be a continuous function that fulfills a Lipschitz condition in $w$, i.e.,
  \begin{equation*}
    \lvert a(w,t) - a(v,t) \rvert \leq L \norm{w - v}
  \end{equation*}
  for all $w,v \in \R^d$ and $t \in [0,T]$. Then, for every $w_0 \in \R^d$ there exists a unique solution $w:[0,T] \rightarrow \R^d$ to \eqref{eq:initial_value_problem}.
\end{theorem}
\begin{proof}
  The proof can be found in \protect{\cite[pp.~73]{ahmadTextbookOrdinaryDifferential2015}}.
\end{proof}
One of the simplest methods to approximate the solution of an ODE numerically is Euler's method. Li et al.\ \cite{liStochasticModifiedEquations2019} use its stochastic generalization to motivate the connection between SGD and stochastic modified equations (see \autoref{sec:sde_model}).
\begin{definition}[Euler's method, see, e.g., \protect{\cite[pp.~3]{hairerGeometricNumericalIntegration2013}}]
  Let $0 \leq t_0 < t_1 < \dots < t_N = T, \Delta t_n = t_{n+1} - t_n$. Then, the time-discrete scheme $\{w_n\}_{n=1}^N$ given by
  \begin{equation}
    \label{eq:ode_euler}
    w_{n+1} = w_n + \mathbf{a}(w_n, t_n) \Delta t_n
  \end{equation}
  for $n=0,1,\dots,N$ is called \emph{Euler's method}.
\end{definition}
Under a regularity assumption on $\mathbf{a}$ Euler's method converges to the true solution linearly.
\begin{theorem}
  \label{thm:euler_convergence}
  Let $\mathbf{a} : \R^d \times [0,T] \rightarrow \R^d$ be a continuous function satisfying a Lipschitz condition and let $w:[0,T] \rightarrow \R^d$ be the unique solution to \eqref{eq:initial_value_problem}. Further, let $0 \leq t_0 < t_1 < \dots < t_N = T$ be a partition of $[0,T]$ and $\{w_k\}_{k=1}^N$ be the iterates of Euler's method defined in \eqref{eq:ode_euler}. Then, for $\eta = \max_{k=0,\dots,N-1} t_{k+1} - t_k$ there exists a constant $C > 0$ such that
  \begin{equation*}
    \norm{w(t_k) - w_k} \leq C \eta.
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof can be obtained as a special case of Theorem 8.3.2 in \cite[pp.~294]{kloedenNumericalSolutionStochastic2013}.
\end{proof}
\subsection{Itô Integral}
\label{subsec:ItoIntegral}
We briefly introduce the well-known notion of Itô integrals. Note that further details on the derivation can be found in \autocite{eAppliedStochasticAnalysis2021}. 
First, we introduce a continuous time generalization of a random walk: Brownian motion.
\begin{definition}[\autocite{durrettProbabilityTheoryExamples2019}]
  A stochastic process $\{B_t\}_{t \geq 0}$ is called \emph{Brownian motion} if it satisfies the following properties:
  \begin{enumerate}[label=(\roman*)]
    \item For any $t \geq s > u \geq v \geq 0$, $B_{t+s} - B_t$ and $B_{v+u} - B_v$ are independent.
    \item For any $s,t \geq 0$ $B_{t+s} - B_s \sim \CN(0, tI_d)$.
    \item The paths $t \rightarrow B_t$ are continuous almost surely.
  \end{enumerate}
\end{definition}
\begin{theorem}
  There exists a stochastic process $\{B_t\}_{t \geq 0}$ that satisfies the definition of Brownian motion.
\end{theorem}
\begin{proof}
  For the detailed construction of Brownian motion we refer to \autocite{durrettProbabilityTheoryExamples2019}.
\end{proof}
% Now, we can define a notion of stochastic integration. Let $f : \R^d \rightarrow \R$ be continuous function. Then, we define a stochastic pathwise integral as a Riemann-Stieltjes integral:
% \begin{equation*}
%   \int_0^t f(X_s) dB_s = \lim\limits_{|\delta| \rightarrow 0} \sum_j f(X_j)(B_{t_{j+1}} - B_{t_j})
% \end{equation*}
For a rigorous construction of the Itô integral we refer to the textbook by Li et al.\ \cite{eAppliedStochasticAnalysis2021}.
We just note that the it is defined as the limit of 
\begin{equation*}
  \int_0^T f(X_s) dB_s \defeq \lim\limits_{h \rightarrow 0} \sum_j f(X_j)(B_{t_{j+1}} - B_{t_j}).
\end{equation*}
for a partition $0 \leq t_0 < t_1 < \dots < t_N = T$, $h = \max\limits_{k=0,\dots,N-1} t_{k+1} - t_k$ and $N \in \N$. This well-defined for the following class of functions:
% Then, it can be shown that a general class of functions can be approximated by these functions. This establishes the existence of the Itô integral as the limit of the integral defined for step-functions.
\begin{definition}[$\CV(0,T)$, \protect{\cite[pp.~5]{eAppliedStochasticAnalysis2021}}]
  Let $(\Omega, \CF, \BP)$ be a pobability space and $\CF_t^B = \sigma(B_s, 0 \leq s \leq t)$ the $\sigma$-algebra generated by Brownian motion. We define the class of functions $\CV(0,T)$ to be function $f :\Omega \times [0,T] \rightarrow \R$ that satisfy the following properties:
  \begin{enumerate}[label=(\roman*)]
    \item The function $f$ is $(\CF \times \sigma([0,T]))$-measurable.
    \item The function $f$ is $\CF_t^B$-adapted for all $t \in [0,T]$.
    \item The time integral of the second moment over $[0,T]$ is finite:
    \begin{equation*}
      \int_0^T \ev{f(\omega,t)^2}dt< \infty.
    \end{equation*}
  \end{enumerate}
\end{definition}
The Itô integral has the properties:
\begin{theorem}[\autocite{eAppliedStochasticAnalysis2021}]
  \label{thm:ito_isometry}
  Let $f,g \in \CV(0,T)$. Then, we have
  \begin{enumerate}[label=(\roman*)]
    \item \label{item:zero_integral} the expected value of the Itô integral of $f$ is given by
    \begin{equation}
    \label{eq:ito_integral_ev}
    \ev{\int_0^T f(\omega,t)dB_t} = 0,
    \end{equation}
    \item the expected value of of the Itô integral of the product $f,g$ is given by
    \begin{equation}
      \label{eq:ito_product}
      \ev{\int_0^T f(\omega,t)dB_t\int_0^T g(\omega,t)dB_t } = \ev{\int_0^T f(\omega,t)g(\omega, t)dt },
    \end{equation}
    \item and the Itô isometry states
    \begin{equation}
      \label{eq:ito_isometry}
      \ev{\left(\int_0^T f(\omega,t)dB_t\right)^2 } = \ev{\int_0^T f(\omega,t)^2dt }.
    \end{equation}
  \end{enumerate}
\end{theorem}
\begin{proof}
  The proof can be found in \cite[pp.~208]{capassoIntroductionContinuousTimeStochastic2012}
\end{proof}
\begin{lemma}[Proposition 7.3 \protect{\cite{eAppliedStochasticAnalysis2021}}]
  Assume that $f,g \in \CV(0,T)$ and $u \in [0,T]$. Then, we have
  \begin{enumerate}[label=(\roman*)]
    \item $\displaystyle\int_0^TfdB_t = \int_0^ufdB_t + \int_u^TfdB_t$,
    \item linearity $\displaystyle\int_0^T(f+cg)dB_t =  \int_0^TfdB_t + c \int_0^TgdB_t$,
    \item $\displaystyle\int_0^TfdB_t$ is $\CF_T^B$-measurable.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof can be found in \cite{eAppliedStochasticAnalysis2021}.
\end{proof}
The definition of stochastic modified equations requires the definition of systems of stochastic differential equations. We use the notation: Let $\{\mathbf{B}_t\}_{t\geq 0} = \{(B^1_t, \dots, B^m_t)\}_{t\geq 0}$ be an $m$-dimensional Brownian motion and $\mathbf{f}: \Omega \times [0,T] \rightarrow \R^{d \times m}$ be a matrix valued function. We write 
\begin{equation*}
  \int_0^T \mathbf{f}(\omega, t)\cdot d\mathbf{B}_t \defeq \left(\sum_{k=1}^m\int_0^T f_{1,k}(\omega, t)dB_t^k, \dots, \sum_{k=1}^m\int_0^T f_{d,k}(\omega, t)dB_t^k \right)
\end{equation*}
to denote the Itô integral applied to each component of $\mathbf{f}$.
We derive the following identity for \autoref{ex:quadratic_smde}.
\begin{lemma}
  \label{lem:ito}
  Let $\{\pmb{B}_t\}_{t\geq0}$ is an $m$-dimensional Brownian motion and $\mathbf{f}, \mathbf{g}: \Omega \times [0,T] \rightarrow \R^{d \times m}$ with $f_{i,j}, g_{i,j}  \in \CV(0,T)$ for $i=1,\dots,d, j=1,\dots,m$. Then, we have
  \begin{equation}
  \label{eq:multivariate_ito_isometry}
    \ev{\left(\int_0^T \pmb{f}(\omega, t)\cdot d\pmb{B}_t\right)^\mathsf{T}\int_0^T \pmb{g}(\omega, t)\cdot d\pmb{B}_t} = \ev{\int_0^T \Tr\left[\pmb{f}(\omega, t)\pmb{g}(\omega, t)^\T\right]dt}.
  \end{equation}

  % $\pmb{f},\pmb{g}  \in \R^{d \times m}$
\end{lemma}
\begin{proof}
  Let $\{\pmb{B}_t\}_{t\geq0}$ be an $m$-dimensional Brownian motion and $\pmb{f},\pmb{g}  \in \R^{d \times m}$, we have
\begin{multline*}
  \ev{\left(\int_0^T \pmb{f}(\omega, t)\cdot d\pmb{B}_t\right)^\mathsf{T}\int_0^T \pmb{g}(\omega, t)\cdot d\pmb{B}_t}
     = \sum_{i=1}^d \ev{\sum_{j,k=1}^m \int_0^T f_{i,j}(\omega, t)dB_t^j\int_0^T g_{i,k}(\omega, t)dB_t^k}\\
    \overset{\eqref{eq:ito_integral_ev}}{=} \sum_{i=1}^d \sum_{j=1}^m \ev{\int_0^T f_{i,j}(\omega, t)dB_t^j\int_0^T g_{i,j}(\omega, t)dB_t^j} 
    \overset{\eqref{eq:ito_product}}{=} \sum_{i=1}^d \sum_{j=1}^m \ev{\int_0^T f_{i,j}(\omega, t)g_{i,j}(\omega, t)dt} \\
    = \ev{\int_0^T \sum_{i=1}^d \sum_{j=1}^m f_{i,j}(\omega, t)g_{i,j}(\omega, t)dt}.
\end{multline*}
By the defintion of matrix multiplication and the trace (see \autoref{sec:notation}) we have 
\begin{equation*}
    \ev{\left(\int_0^T \pmb{f}(\omega, t)\cdot d\pmb{B}_t\right)^\mathsf{T}\int_0^T \pmb{g}(\omega, t)\cdot d\pmb{B}_t}
    = \ev{\int_0^T \Tr\left[\pmb{f}(\omega, t)\pmb{g}(\omega, t)^\T\right]dt}. \qedhere
\end{equation*}
\end{proof}
\begin{example}[\protect{\cite[pp.~143]{eAppliedStochasticAnalysis2021}}]
  The Itô integral does not behave like the Riemann integral:
  \begin{equation*}
    \int_0^t B_s dB_s = \frac{B_t^2}{2} - \frac{t}{2}.
  \end{equation*}
\end{example}
\begin{definition}[Itô process, \protect{\cite[pp.~230]{capassoIntroductionContinuousTimeStochastic2012}}]
  Let $\{\mathbf{B}_t\}_{t\geq 0}$ be an $m$-dimensional Brownian motion and let $\CF_t^\mathbf{B} = \sigma(\mathbf{B}_s, 0 \leq s \leq t)$ denote the $\sigma$-algebra generated it. A stochastic process is called \emph{Itô process} if it is given by
  \begin{equation}
    \label{eq:ito_process}
    W_t = W_0 + \int_0^t\mathbf{a}(\omega,s)ds + \int_0^t \mathbf{b}( \omega,s)\cdot d\mathbf{B}_s,
  \end{equation}
  where $b_{i,j} \in \CV(0,T)$, $i=1,\dots,d$,$j=1,\dots,m$, $\mathbf{b}$ is $\CF_t^\mathbf{B}$-adapted, and $\int_0^T \norm{\mathbf{a}(t, \omega)}dt < \infty$ almost surely.
\end{definition}
For Itô processes there exists an analog to the fundamental theorem of calculus: Itô's formula.
\begin{theorem}[Itô's formula, \protect{\cite[pp.~232]{capassoIntroductionContinuousTimeStochastic2012}}]
  \label{thm:ito_formula}
  Let $f : \R^d \rightarrow \R$ be a twice differentiable function, and let $V_t = f(W_t, t)$ where $W_t$ is an Itô process defined in \eqref{eq:ito_process}. Then $V_t$ is also an Itô process and
  \begin{equation}
    \begin{split}
    V_t = f(W_0, 0) &+ \int_0^t \frac{\partial}{\partial t} f(W_s, s) + \nabla f(W_s, s)^\T \mathbf{a}(\omega,s)+ \frac{1}{2}\Tr\left[\mathbf{b}(\omega,s) {H_w}_f(W_s, s) \mathbf{b}(\omega,s)^\T\right]ds \\
    &+ \int_0^t \nabla f(W_s, s)^\T \mathbf{b}(\omega,s) \cdot d\mathbf{B}_s.
    \end{split}
  \end{equation}
\end{theorem}
\begin{proof}
  The proof can be found in e.g., \cite{baldiEquazioniDifferenzialiStocastiche2000}.
\end{proof}
We present the result from \cite[pp.~149]{eAppliedStochasticAnalysis2021} as defintion because in the next subsection we introduce a more general existence result from \cite{liStochasticModifiedEquations2019}.
\begin{definition}[Drift, diffusion, SDE, \protect{\cite[pp.~149]{eAppliedStochasticAnalysis2021}}]
  We call the function $\mathbf{a} : \R^d \times [0,T] \rightarrow \R^d$ \emph{drift} and the function $\mathbf{b} : \R^d \rightarrow \R^{d \times m}$ \emph{diffusion}. Then a \emph{stochastic differential equation} is defined as the relation
  \begin{equation}
    \label{eq:sde}
    W_t = Z + \int_0^t \mathbf{a}(W_s, s) dt + \int_0^t \mathbf{b}(W_s,s)\cdot d\mathbf{B}_s, \quad t \in [0,T],
  \end{equation}
  for $Z$ a random variable that is independent of $\CF_t^\mathbf{B}$ with $\ev{\norm{Z}^2} < \infty$.
  We call a stochastic process $\{W_t:t\in [0,T] \}$ a \emph{solution} to \eqref{eq:sde} if it is $\CF_t^\mathbf{B}$-measurable and the relation holds almost surely for all $t \in [0,T]$. Instead of \eqref{eq:sde} we use the short hand notation
  \begin{equation*}
    dW_t = \mathbf{a}(W_t,t)dt + \mathbf{b}(W_t,t)d\mathbf{B}_t, \quad W_0 = Z.
  \end{equation*}
\end{definition}


\subsection{Existence and uniqueness}
\label{subsec:sde_existence_uniqueness}
In this section, we cover the conditions for the existence and uniqueness of solutions to SDEs. We present a theorem by Li et al. \cite{liStochasticModifiedEquations2019} that generalizes results presented in \cite{oksendalStochasticDifferentialEquations2003}. 
% \begin{theorem}
%   \label{thm:sde_existence_uniqueness}
%   Let $T > 0$ and $a(\cdot,\cdot):[0,T] \times \R^n \rightarrow \R^n$, $b(\cdot,\cdot):[0,T] \times \R^n \rightarrow \R^{n \times m}$ be measurable functions satisfying 
%   \begin{equation}
%     |a(t,x)| + |b(t,x)| \leq C(1+|x|); x \in \R^n, t \in [0,T]
%   \end{equation}
%   for some constant $C$, and such that 
%   \begin{equation}
%     |a(t,x) - a(t,y)| + |b(t,x) - b(t,y)| \leq D(|x-y|); x,y \in \R^n, t \in [0,T]
%   \end{equation}
%   for some constant $D$. Let $Z$ be a random variable which is independent of the $b$-algebra $\mathcal{F}_{\infty}^{m}$ generated by $B_s(\cdot)$, $s\geq 0$ and such that 
%   \begin{equation}
%     E[|Z|^2] < \infty
%   \end{equation}
% Then the stochastic differential equation 
% \begin{equation}
%   dX_t = a(t,X_t)dt + b(t, X_t)dB_t, 0 \leq t \leq T, X_0 = Z
% \end{equation}
% has a unique t-continuous solution $X_t(\omega)$ with the property that $X_t(\omega)$ is adapted to the filtration $\mathcal{F}_t^Z$ generated by $Z$ and $B_s(\cdot)$; $s \leq t$
% and 
% \begin{equation}
%   \ev{\int_0^T|X_t|^2dt} < \infty.
% \end{equation}

% \end{theorem}
Let $T > 0$ and $Q$ be a subset of Euclidean space. For $(x,t,q) \in \R^d \times [0,T] \times Q$, let $\mathbf{a}(x,t,q)$ be a $d$-dimensional random vector and $\mathbf{b}(x,t,q)$ be a $d \times d$-dimensional random matrix. Then we assume: 
\begin{assumption}[\protect{\cite[pp. 30]{liStochasticModifiedEquations2019}}]
  \label{as:sde_existence}
  The random functions $\mathbf{a},\mathbf{b}$ satisfy the following:
  \begin{enumerate}[label=(\roman*)]
    \item $\mathbf{a},\mathbf{b}$ are $\CF^\mathbf{B}_t$-adapted and continuous in $(x,t) \in \R \times [0,T]$ almost surely.
    \item $\mathbf{a},\mathbf{b}$ satisfy a uniform linear growth condition, i.e., there exists a non-random constant $L > 0$ such that
    \begin{equation*}
      \norm{\mathbf{a}(x,t,q)}^2 + \normf{\mathbf{b}(x,t,q)}^2 \leq L^2 (1 + \norm{x}^2) \text{ almost surely}
    \end{equation*}
    for all $x \in \R^d, \; t \in [s,T], \; q \in Q$.
    \item $\mathbf{a},\mathbf{b}$ satisfy a uniform Lipschitz condition in $x$, i.e.,
    \begin{equation*}
      \norm{\mathbf{a}(x,t,q) - \mathbf{a}(y,t,q)} + \normf{\mathbf{b}(x,t,q) - \mathbf{b}(y,t,q)} \leq L \norm{x - y}  \text{ almost surely}
    \end{equation*}
    for all $x,y \in \R^d, \; t \in [s, T], \; q \in Q$.
  \end{enumerate}
\end{assumption}

\begin{theorem}
  \label{thm:sde_existence}
  Let $s \in [0,T)$ and for each $q \in Q$, let $\{\phi_t^q : t \in [s,T]$ be a $\R^d$-valued, $\CF^\mathbf{B}_t$-adapted random process that is continuous in $t \in [s,T]$ almost surely, with
  \begin{equation*}
    \sup_{q \in Q} \ev{\sup_{t \in [s,T]} \norm{\phi^q_t}^2} < \infty.
  \end{equation*}
  Then, for each $q \in Q$ the stochastic differential equation
  \begin{equation*}
    \xi^q_t = \phi_t^q + \int_s^t \mathbf{a}(\xi^q_v,v,q)dv + \int_s^t\mathbf{b}(\xi_v^q, v, q)dB_v
  \end{equation*}
  admits a unique solution $\{\xi_t^q : t \in [s,T]\}$ which is continuous for $t\in [s,T]$ almost surely and satisfies
  \begin{equation*}
    \sup_{q \in Q} \ev{\sup_{t \in [s,T]} \norm{\xi^q_t}^2} \leq C \left( 1 + \sup_{q \in Q} \ev{\sup_{t \in [s,T]} \norm{\phi^q_t}^2}\right)
  \end{equation*}
  for some constant $C > 0$ that depends only on $L,T$.
\end{theorem}
% \begin{theorem}
%   Let us assume the same conditions as in \autoref{thm:sde_existence} and let $q^\star \in Q$ be fixed. Suppose futher that the following holds for any $t \in [s,T],\; R > 0$ and $\epsilon > 0$:
%   \begin{enumerate}
%     \item 
%   \end{enumerate}
% \end{theorem}
\begin{proof}
  The proof for this theorem can be found in \cite{liStochasticModifiedEquations2019}.
\end{proof}
\begin{example}[Ornstein-Uhlenbeck process, see, e.g., \protect{\cite{uhlenbeckTheoryBrownianMotion1930}}]
  \label{ex:ornstein_uhlenbeck}
  In this example we generalize the \emph{Ornstein-Uhlenbeck process} from example 7.15 \cite[pp.~152]{eAppliedStochasticAnalysis2021} to the $d$-dimensional case.
  Let $F, G \in \R^{d \times d}$ and $F$ be a symmetric, positive definite matrix (see \autoref{def:pos_neg_def}). Then, the Ornstein-Uhlenbeck process is given by
  \begin{equation*}
    dW_t = -F W_t dt + G d\mathbf{B}_t, \quad W_0 = w,
  \end{equation*}
  for $w \in \R^d$.
  Its solution is given by 
  \begin{equation}
    \label{eq:ornstein_solution_1d}
    W_t = e^{-F t}w + \int_0^t e^{-F (t - s)}G \cdot d\mathbf{B}_s.
  \end{equation}
  We see this by applying defining the function $f(w,t) = e^{F t}w$. Its time derivative, gradient and Hessian are given by
  \begin{equation*}
    \frac{\partial}{\partial t} f(w,t) = F e^{F t}w, \quad \nabla_w f(w,t) = e^{F t}, \quad {H_w}_f(w,t) = 0.
  \end{equation*}
  Using \hyperref[thm:ito_formula]{Itô's formula}, have 
  \begin{equation*}
      f(W_t, t) = w + \int_0^t F e^{F t}W_s - e^{F^\T t} FW_sds + \int_0^t e^{F^\T t} G \cdot d\mathbf{B}_s.
  \end{equation*}
  We note that $Fe^{Ft} = e^{Ft}F = e^{F^\T t}F$ because $F$ is symmetric.
  Thus, we have 
  \begin{equation*}
    e^{F t}W_t = w + \int_0^t e^{F s} G \cdot d\mathbf{B}_s
  \end{equation*}
  and finally by rearranging we obtain \eqref{eq:ornstein_solution_1d}.
  % 
\end{example}
\subsection{Numerical approximations}
\label{subsec:sde_numerical_methods}
% There are two classes of numerical approximations for SDEs: Strong and weak approximations. 
Numerical methods for stochastic differential equations extend the methods for ordinary differential equations. In this subsection, we briefly introduce two notations of convergence and an extension of Euler's method to SDEs following the textbook by Kloeden et al. \cite{kloedenNumericalSolutionStochastic2013}. We note that there are numerical schemes generally considered more efficient than Euler's method for simulating SDEs, but for the SDEs considered in \autoref{sec:smdedl} each evaluation of the drift and diffusion term is computionally expensive. Therefore, we omit the discussion of the Milstein and other Runge-Kutta methods.
% In addition to the deterministic part given by the drift term
\begin{definition}[Time discretization, Time discrete approximation, \protect{\cite[pp.~321]{kloedenNumericalSolutionStochastic2013}}]
  We write $\mathcal{T}_\eta^N([0,T])$ to denote $0 \leq t_0 < t_1 < \dots < t_N = T$, $\eta \defeq \max_{k=1,\dots,N-1} t_{k+1} - t_k$ a \emph{time discretization} of the interval $[0,T]$.
  A \emph{time discrete approximation} $\{w_k\}_{k=1}^N$ with maximum step size $\eta > 0$ is a right continuous process with left-hand limits (see, e.g. \cite[pp.~65]{kloedenNumericalSolutionStochastic2013}). The approximation $w_k$ is $\mathcal{F}_{t_k}$-measurable for $k=1,\dots,N$ and recursively defined by a function $\Phi$ such that for $k=0,1,\dots,N-1$ holds
  \begin{equation}
    w_{k+1} = \Phi(w_0, \dots, w_k, t_0, \dots, t_k, Z^1_k,\dots, Z_k^n)
  \end{equation}
  for some finite number $n \in \N$ of $\mathcal{F}_{t_{k+1}}$ measurable random variables $Z^j_k, j = 1,\dots,n$. 
\end{definition}
\begin{definition}[Strong convergence, \protect{\cite[pp.~323]{kloedenNumericalSolutionStochastic2013}}]
  \label{def:strong_convergence}
  Let $\mathcal{T}_\eta^N([0,T])$ be a time discretization. A time discrete approximation $\{w_k\}_{k=1}^N$ defined on $\mathcal{T}_\eta^N([0,T])$ \emph{converges strongly} to $\{W_t:t\in [0,T] \}$ at time $T$ as $\eta \rightarrow 0$ if 
  \begin{equation*}
    \lim_{\eta \rightarrow 0} \ev{\norm{W_{T} - w_N}} = 0.
  \end{equation*}
  The time discrete approximation $\{w_k\}_{k=1}^N$ \emph{converges strongly with order} $p>0$ to the time-continuous process $\{W_t:t\in [0,T] \}$ at time $T$ as $\eta \rightarrow 0$ if there exists a constant $C > 0$, which does not depend on $\eta$, and a $\delta_0 > 0$ such that 
  \begin{equation*}
    \ev{\norm{W_T - w_N}} \leq C \eta^p
  \end{equation*}
  holds for each $\eta \in ]0, \delta_0[$.
\end{definition}

\begin{definition}[Weak convergence, \protect{\cite[pp.~327]{kloedenNumericalSolutionStochastic2013}}]
  \label{def:weak_convergence}
  Let $\mathcal{T}_\eta^N([0,T])$ be a time discretization. A time discrete approximation $\{w_k\}_{k=1}^N$ defined on $\mathcal{T}_\eta^N([0,T])$ \emph{converges weakly} to $\{W_t:t\in [0,T] \}$ at time $T$ as $\eta \rightarrow 0$ with respect to a class $\mathcal{K}$ of test functions $f: \R^n \rightarrow \R$ if 
  \begin{equation}
    \lim_{\eta \rightarrow 0} |\ev{f(W_T)} - \ev{f(w_N)}| = 0
  \end{equation}
  holds for all $f \in \mathcal{K}$.
  A time discrete approximation $\{w_k\}_{k=1}^N$ \emph{converges weakly with order} $p > 0$ to $\{W_t:t\in [0,T] \}$ at time $T$ as $\eta \rightarrow 0$ if for each $f \in \mathcal{K}$ there exists a constant $C_f$ and a finite $\delta_0$ such that 
  \begin{equation}
     |\ev{f(W_T)} - \ev{f(w_N)}| = C_f \eta^p
  \end{equation}
  for each $\eta \in ]0, \delta_0[$.
\end{definition}
\begin{definition}[Euler-Maruyama scheme, \protect{\cite[pp.~305]{kloedenNumericalSolutionStochastic2013}}]
  \label{def:euler_maruyama}
  Let $0 \leq t_0 < t_1 < \dots < t_N = T$, $\Delta t_k = t_{k+1} - t_k$ and let $\Delta \mathbf{B}_k \sim \CN(0,\Delta t_k I_d)$. Then, for an initial value $w_0 \in \R^d$ the time-discrete scheme $\{w_k\}_{k=0}^N$ given by
\begin{equation}
  \label{eq:euler_maruyama}
  w_{k+1} = w_k + \mathbf{a}(w_k, t_k)\Delta t_k + \mathbf{b}(w_k, t_k) \Delta \mathbf{B}_k
\end{equation}
for $0 \leq k \leq N$ is called \emph{Euler-Maruyama scheme}.
\end{definition}

\begin{theorem}[\protect{\cite[Proposition 7.22.]{eAppliedStochasticAnalysis2021}}]
  The Euler-Maruyama scheme is of strong order~$1/2$.
\end{theorem}
\section{Stochastic modified differential equations}
\label{sec:sde_model}
In this section, we introduce a continuous-time model for SGD developed by Li et al.\ in \cite{liStochasticModifiedEquations2017}. First, we consider the non-deterministic case of GD and derive an ODE system to model the limiting behavior of GD following a framework found in \cite{hairerGeometricNumericalIntegration2013}. Then, we introduce a stochastic generalization of the model building on the results of \autoref{sec:BackgroundSDETheory}. Lastly, we discuss the rigorous framework by Li et al.\ to show the convergence of SGD and the continuous-time model in a weak sense and expand on some details of theirs proofs.
% Note that in section \autoref{sec:applications} we detail analyses of the continuous-time process and make a case for the usefulness of the SDE model.

\subsection{Gradient descent}
\label{sec:gradient_flow}
% To motivate the continuous-time model for GD we consider the following example.
Recall the iterates $\{ w_k\}_{k=1}^N$ of GD given in \eqref{eq:gradient_descent} for a continuously differentiable function $f : \R^d \rightarrow \R$ and a fixed learning rate $\eta > 0$ are given by
\begin{equation*}
  w_{k+1} = w_{k} - \eta \nabla f(w_{k}),
\end{equation*}
for $k=1,\dots,N$. Now, we want to highlight the following observation: GD has the same structure as Euler's method for a system of ordinary differential equations. If we interpret the learning rate $\eta$ as a time step, GD is a first order approximation to a system of ODEs. In fact, this follows from \autoref{thm:euler_convergence}, if $\nabla f$ satisfies a Lipschitz condition. This leads to the well-known gradient flow equation.
% In fact, if $\nabla f$ satisfies a Lipschitz condition we know by \autoref{thm:euler_convergence} that the iterates form a first order approximation to a system of ODEs. 
\begin{definition}[Gradient flow, see, e.g., \protect{\cite{santambrogioEuclideanMetricWasserstein2017}}]
  Let $f : \R^n \rightarrow \R$ be an $L$-smooth function. Then for $w_0 \in \R^d$ the system of ODEs 
  \begin{equation}
  \label{eq:gradient_flow}
  w'(t) = - \nabla f(w(t)), \quad w(0) = w_0
  \end{equation}
  with is called \emph{gradient flow}.
\end{definition}
Next, we consider the gradient flow of a linear regression problem for which the continuous-time model reduces to a system of linear ordinary differential equations.
\begin{example}[Linear regression]
  \label{ex:linear_regression}
  Let $\{x_i\}_{i=1}^n$ be a sequence of values with $x_i = \frac{i}{n}$ for $i = 1,2,\dots,n$. We define the sequence $\{y_i\}_{i=1}^n$ by
  \begin{equation*}
    y_i = wx_i + b + \epsilon_i,
  \end{equation*}
  where $w,b \in \R$ are fixed values and $\epsilon_i \sim \CN(0,1)$ for $i = 1, 2, \dots, n$. Therefore, we have $y_i \sim \CN(wx_i + b,1)$.
  Now, given a sample of $\{(x_i, y_i)\}_{i=1}^n$ for unknown parameters $w, b \in \R$ we want to find $\hat{w}, \hat{b} \in \R$ that optimally represent the data. In order to characterize optimality we define the $\ell^2$-loss $\CL : \R^2 \rightarrow \R$ with 
  \begin{equation}
    \CL (w, b) = \frac{1}{2n} \sum_{i=1}^n(wx_i + b  - y_i)^2.
  \end{equation}
  In \autoref{fig:quadratic_loss_function}, we plot the loss function $\CL$ for a random sample of $\{(x_i, y_i)\}_{i=1}^n$ with $n = 1000$, $w=4$ and $b=1$. The gradient of $\CL$ is given by
  \[
    \nabla \CL (w,b) = 
    \begin{pmatrix}
      \displaystyle\partial_w\CL(w,b) \\
      \addlinespace
      \displaystyle\partial_b\CL(w,b) 
    \end{pmatrix}
    = \frac{1}{n}
    \begin{pmatrix}
      \left(\displaystyle\sum_{i=1}^n x_i^2\right)w +  \left(\displaystyle\sum_{i=1}^n x_i\right)b -  \displaystyle\sum_{i=1}^n x_i y_i \\
       \left(\displaystyle\sum_{i=1}^n x_i\right) w + nb -  \displaystyle\sum_{i=1}^n y_i
    \end{pmatrix}.
  \]
  For a learning rate $\eta > 0$ and an initial values $w_{0},b_0 \in \R$ the iterates of gradient descent are given by
  \begin{equation*}
    \begin{pmatrix}
      w_{k+1} \\
      b_{k+1}
    \end{pmatrix}
    =
    \begin{pmatrix}
      w_{k} \\
      b_k
    \end{pmatrix}
    - \frac{\eta}{n}
    \begin{pmatrix}
      \left(\displaystyle\sum_{i=1}^n x_i^2\right)w_k +  \left(\displaystyle\sum_{i=1}^n x_i\right)b_k -  \displaystyle\sum_{i=1}^n x_i y_i \\
       \left(\displaystyle\sum_{i=1}^n x_i\right) w_k + nb_k -  \displaystyle\sum_{i=1}^n y_i
    \end{pmatrix}.
  \end{equation*}
  In \autoref{fig:learning_rates}, we show the behavior of the iterates for different values of $\eta >0$. We observe that a large learning rate leads to a faster convergence to the optimal value. This observation is in agreement with the results from \autoref{sec:stochastic_optimization}.
  %  Fig size reduzieren
  % Bounding boxes tight
  % Eta in den figures, konsistente schreibweise der kommazahlen

  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/quadratic_loss.pdf}
    \caption{Loss function for linear regression problem with $n=1000$, $w=4$ and $b=1$}
    \label{fig:quadratic_loss_function}
  \end{figure}

  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/learning_rates.pdf}
    \caption{Evolution of $w$ and $b$ for GD}
    \label{fig:learning_rates}
  \end{figure}

 
\end{example}

We reiterate some standard results linear algebra (see, e.g., \cite[pp.~250]{romanAdvancedLinearAlgebra2010}) that we use to solve this special case of gradient flow.
\begin{definition}[Positive definite, negative definite, indefinite]
  \label{def:pos_neg_def}
  Let $A \in \R^{d \times d}$ be a symmetric matrix. Then, we call the matrix $A$ \emph{positive definite} if
  \begin{equation*}
    x^\T A x > 0
  \end{equation*}
  for all $x \in \R^d \setminus \{0\}$. Further, we call $A$ \emph{positive semi-definite} if
  \begin{equation*}
    x^\T A x \geq 0
  \end{equation*}
  for all $x \in \R^d$. We call the matrix $A$ \emph{negative (semi)-definite} if $-A$ is positive (semi)-definite.
  Otherwise, we call $A$ \emph{indefinite}.
\end{definition}
\begin{lemma}[\protect{\cite[pp.~250]{romanAdvancedLinearAlgebra2010}}]
  \label{lem:positive_eigen_values}
  Let $A \in \R^{d \times d}$ be a symmetric, positive (negative) definite matrix. Then, the matrix $A$ has strictly positive (negative) eigenvalues, i.e.,
  \begin{equation*}
    \lambda_i(A) > (<) 0
  \end{equation*}
  for $i = 1, \dots, d$.
\end{lemma}
\begin{proof}
  Because of its simplicity, we briefly show the proof. Let $A \in \R^{d \times d}$ be a symmetric, positive definite matrix. Further, let $v \in \R^d \setminus \{0\}$ be an eigenvector to the eigenvalue $\lambda \in \{\lambda \in \R | \exists v \in \R^d \setminus \{0\}: Av = \lambda v \}$, then we have
  \begin{equation*}
    0 < v^\T A v = v^\T (\lambda v) = \lambda \norm{v}^2.
  \end{equation*}
  Clearly, this implies $\lambda > 0$.
\end{proof}

\begin{theorem}
  \label{thm:ode_system}
  Let  $A \in \R^{d \times d}$ be a nonsingular matrix and $b \in \R^d$ a vector. Then, the linear system of ordinary differential equations
  \begin{equation*}
    w'(t) = Aw(t) + b, \quad w(0) = w_0 \in R^d
  \end{equation*}
  has the solution
  \begin{equation*}
    w(t) = e^{At}(w_0 + A^{-1}b) - A^{-1}b,
  \end{equation*}
  where $e^A \defeq \sum_{k=0}^\infty \frac{A^k}{k!}$ is the matrix exponential. For a symmetric, negative definite matrix $A \in \R^{d \times d}$ we have
  \begin{equation*}
    \lim_{t \rightarrow \infty} w(t) = A^{-1}b.
  \end{equation*}
\end{theorem}
\begin{proof}
  First, we observe that
  \begin{equation*}
    w(0) = (w_0 + A^{-1}b) - A^{-1}b = w_0.
  \end{equation*}
  Second, by differentiation we obtain
  \begin{equation*}
    w'(t) = A e^{At}(w_0+A^{-1}b) = A (e^{At}(w_0+A^{-1}b) - A^{-1}b) + b = Aw(t) + b.
  \end{equation*}
  For the limiting behavior we use \autoref{lem:positive_eigen_values} and use the eigenvalues decomposition of $A = V^T \Lambda V$, where $\Lambda=\diag(\lambda_1, \dots, \lambda_d)$ and $V^TV = I_d$. Then, we have
  \begin{equation*}
    \lim_{t \rightarrow \infty} w(t) + A^{-1}b = V^Te^{\Lambda t}V(w_0 + A^{-1}b) = 0,
  \end{equation*}
  where we see the last step because $\lambda_1,\dots,\lambda_d < 0$ implies $\lim\limits_{t \rightarrow \infty} e^{\Lambda t} = 0$.
\end{proof}



\begin{example}[\autoref{ex:linear_regression} continued]
  Now, we apply \autoref{thm:ode_system} to the linear regression example. In \autoref{fig:scaled_weights_biases}, we observe that all curves scaled according to the learning rate $\eta$ follow the same trajectory. This follows directly from the fact that they all approximate the solution of the gradient flow equation. The gradient flow system is given by
  \begin{equation*}
    \begin{pmatrix}
      w'(t)      \\
      b'(t)     
  \end{pmatrix}
  = \frac{1}{n}
  \begin{pmatrix}
    -  \displaystyle\sum_{i=1}^n x_i^2  &  -  \displaystyle\sum_{i=1}^n x_i      \\
      -  \displaystyle\sum_{i=1}^n x_i  &  -n      
  \end{pmatrix}
  \begin{pmatrix}
    w(t)     \\
    b(t)    
  \end{pmatrix}
  + \frac{1}{n}
  \begin{pmatrix}
      \displaystyle\sum_{i=1}^n x_i y_i     \\
      \displaystyle\sum_{i=1}^n y_i    
  \end{pmatrix},
  \end{equation*}
  where $w(0) = w_{0}$ and $b(0) = b_0$. We define 
  \begin{equation*}
    A \defeq \frac{1}{n}
    \begin{pmatrix}
      -  \displaystyle\sum_{i=1}^n x_i^2  &  -  \displaystyle\sum_{i=1}^n x_i      \\
        -  \displaystyle\sum_{i=1}^n x_i  &  -n      
    \end{pmatrix}
  \end{equation*}
  and
  \begin{equation*}
    b \defeq
    \frac{1}{n}
  \begin{pmatrix}
      \displaystyle\sum_{i=1}^n x_i y_i     \\
      \displaystyle\sum_{i=1}^n y_i    
  \end{pmatrix}.
  \end{equation*}
  Since the matrix $A$ is negative definite, by \autoref{thm:ode_system} the limiting behavior of this gradient flow is given by 
  \begin{equation*}
    \lim_{t \rightarrow \infty} 
    \begin{pmatrix}
      w(t) \\
      b(t)
    \end{pmatrix}
    = 
    \begin{pmatrix}
      -  \displaystyle\sum_{i=1}^n x_i^2  &  -  \displaystyle\sum_{i=1}^n x_i      \\
        -  \displaystyle\sum_{i=1}^n x_i  &  -n      
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        \displaystyle\sum_{i=1}^n x_i y_i     \\
        \displaystyle\sum_{i=1}^n y_i    
    \end{pmatrix},
  \end{equation*}
  
  which is the unique stationary point of $\CL$.
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/scaled_weights_biases.pdf}
    \caption{Evolution of $w$ and $b$ scaled for SGD}
    \label{fig:scaled_weights_biases}
  \end{figure}
\end{example}
We saw that gradient descent is a first order approximation to the gradient flow equation. We can analyze the asymptotic behavior of gradient descent by studying the gradient flow equation. However, gradient flow does not capture the phenomena induced by discretization. 
A technique to study the asymptotic behavior of numerical schemes is backward analysis. The idea is to construct a modified equation that more closely matches the behavior of the iterative scheme by adding terms in powers of the step size $\eta$.
Next, we introduce a framework taken from \cite{hairerGeometricNumericalIntegration2013} and slightly modify it such that it matches the notation used for stochastic modified equations in \autoref{sec:smde_sgd} and impose some additional conditions.

\begin{definition}[Modified ordinary differential equation, \protect{\cite[pp.~337]{hairerGeometricNumericalIntegration2013}}]
  Let $\mathbf{a}: \R^d \rightarrow \R^d$ be a function with continuous derivatives of up to order $r \in \N$. Let the $w:[0,T] \rightarrow \R^d$ be the solution of the differential equation
  \begin{equation*}
    w'(t) = \mathbf{a}(w(t)), \quad w(0) = w_0
  \end{equation*}
  and for the step size $\eta > 0$ let $\{w_k\}_{k=0}^N$ be the iterates of the iterative scheme $\mathbf{\Phi}^\eta : \R^d \rightarrow \R^d$ given by
  \begin{equation*}
    w_{k+1} = \mathbf{\Phi}^\eta(w_k) = w_k + \eta \phi_1(w_k) + \eta^2 \phi_2(w_k) + \cdots.
  \end{equation*}
  Then, the \emph{$r$-order modified ordinary differential equation} (MODE) is given by
  \begin{equation}
    \label{eq:modified_ode}
    \widetilde{w}'(t) = \mathbf{a}^\eta(\widetilde{w}(t)), \quad \widetilde{w}(0) = w_0
  \end{equation}
  where $\mathbf{a}^\eta(\widetilde{w}) \defeq \mathbf{a}_0(\widetilde{w}) + \eta\mathbf{a}_1(\widetilde{w}) + \eta^2\mathbf{a}_2(\widetilde{w})+\cdots + \eta^r\mathbf{a}_r(\widetilde{w})$
  for some $\mathbf{a}_0, \mathbf{a}_1, \dots, \mathbf{a}_r : \R^d \rightarrow \R^d$ such that 
  \begin{equation*}
    \widetilde{w}(t+\eta) = \mathbf{\Phi}^\eta(\widetilde{w}(t)) + \CO(\eta^{r+1}),
  \end{equation*}
  for all $t \geq 0$.
\end{definition}
We note that to obtain existence and uniqueness of the solution to the modified equation, by \autoref{thm:ode_existence} the right-hand side $\mathbf{a}^\eta$ needs to be Lipschitz continuous. If we require that $\mathbf{a}_0, \dots, \mathbf{a}_r$ are Lipschitz continuous, we meet the conditions of \autoref{thm:ode_existence}.
\begin{example}
Applying Euler's method to the gradient flow equation \eqref{eq:gradient_flow}, we have 
\begin{equation}
  \label{eq:gradient_descent_function}
  \mathbf{\Phi}^\eta(w) = w + \eta \mathbf{a}(w) = w - \eta \nabla f(w).
\end{equation}
 Let $\widetilde{w}$ denote the solution of the modified differential equation. Then, using Taylor's theorem \autoref{thm:taylor} we have
  \begin{equation*}
    \widetilde{w}(t+\eta) = \widetilde{w}(t) + \widetilde{w}'(t)\eta + \widetilde{w}''(t)\eta^2 + \CO(\eta^3).
  \end{equation*}
  Next, we use the gradient flow equation \eqref{eq:gradient_flow} to obtain
\begin{align*}
  \widetilde{w}(t+\eta) &= \widetilde{w}(t) + \mathbf{a}^\eta(\widetilde{w}(t))\eta + \frac{1}{2}\nabla \mathbf{a}^\eta(\widetilde{w}(t))^\T\mathbf{a}^\eta(\widetilde{w}(t)) \eta^2 + \CO(\eta^3) \\
  &= \widetilde{w}(t) + (\mathbf{a}_0(\widetilde{w}(t)) + \mathbf{a}_1(\widetilde{w}(t))\eta + \CO(\eta^2))\eta \\
  &+ \frac{1}{2}(\nabla \mathbf{a}_0(\widetilde{w}(t)) + \CO(\eta))^\T(\mathbf{a}_0(\widetilde{w}(t)) + \CO(\eta))\eta^2 + \CO(\eta^3).
\end{align*}
Rearranging by the powers of $\eta$ yields
\begin{equation}
  \label{eq:modified_equation_powers}
  \widetilde{w}(t+\eta) = \widetilde{w}(t) + \mathbf{a}_0(\widetilde{w}(t))\eta + \left( \frac{1}{2}\nabla \mathbf{a}_0(\widetilde{w}(t))^\T \mathbf{a}_0(\widetilde{w}(t)) + \mathbf{a}_1(\widetilde{w}(t))\right)\eta^2 + \CO(\eta^3). 
\end{equation}
By comparing the coefficients of the first and second powers of the iterative scheme $\mathbf{\Phi}^\eta$ \eqref{eq:gradient_descent_function} and the modified solution $\widetilde{w}$ \eqref{eq:modified_equation_powers} we obtain the system
\begin{equation*}
    \mathbf{a}_0(\widetilde{w}(t)) = - \nabla f(w(t)), \quad
    \frac{1}{2}\nabla \mathbf{a}_0(\widetilde{w}(t))^\T \mathbf{a}_0(\widetilde{w}(t)) + \mathbf{a}_1(\widetilde{w}(t)) = 0.
\end{equation*}
Solving for $\mathbf{a}_0$ and $\mathbf{a}_1$ yields $\mathbf{a}_0(w) = -\nabla f(w), \; \mathbf{a}_1(w) = - \frac{1}{2} H_f(w)\nabla f(w) = -\frac{1}{4}\nabla\norm{\nabla f(w)}^2$, where $H_f(w)$ denotes the Hessian matrix. Therefore, the second order modified differential equation for gradient flow is given by
\begin{equation}
  \label{eq:second_order_ode}
  w'(t) = -\nabla\left(f(w(t)) + \frac{\eta}{4}\norm{\nabla f(w(t))}^2\right),\quad w(0) = w_0.
\end{equation}
This ODE has a unique solution if both $\nabla f$ and $\nabla\norm{\nabla f}$ are Lipschitz continuous.
Barrett et al.\ \cite{barrettImplicitGradientRegularization2021} make the observation that GD with a fixed finite learning rate leads to a gradient flow 
\begin{equation*}
  w'(t) = -\nabla\widetilde{f}(w(t)),\quad w(0) = w_0,
\end{equation*}
for the loss function $\widetilde{f}(w) = f(w) + \frac{\eta}{4}\norm{\nabla f(w)}^2$. They characterize the norm term as an implicit regularization to the gradients of the loss function.
In \autoref{sec:smde_sgd}, we introduction a generalization of this second order modified equation to SGD.
\end{example}
\begin{example}[\protect{\autoref{ex:linear_regression}} continued]
  Let us extend the linear regression example to the $d$-dimensional case, i.e., $f(w)=\frac{1}{2n} \norm{Xw - y}^2$, where $X \defeq [x_1\;x_2\;\dots\;x_n]^\T \in \R^{d \times n}$ and $y \defeq [y_1\;y_2\;\dots\;y_n]^\T$. 
  The gradient is given by $\nabla f(w) = \frac{1}{n}\left(X^\T Xw - X^\T y\right)$. 
  Notice that the stationary points of $f$ are given by the normal equation
  \begin{equation*}
    X^\T Xw^\star = X^\T y.
  \end{equation*}
  If we assume $d < n$ and $x_i \neq x_j$ for $i \neq j$, then the matrix $X^\T X$ has full rank and is positive definite. Thus, we have the unique minimizer $w^\star = (X^\T X)^{-1}X^\T y$.
  Now, we analyze the dynamics of gradient descent using the first and second order modified equations. First, we consider the gradient flow equation.
  For an initial value $w_0 \in \R^n$ we have 
  \begin{equation}
    \label{eq:linear_regression_gradient_flow}
    w'(t) = -\frac{1}{n}\left(X^\T Xw(t) - X^\T y\right), \quad w(0) = w_0.
  \end{equation}
  Equation \eqref{eq:linear_regression_gradient_flow} is a system of inhomogeneous linear equations. Under our assumptions on $X$ it has the solution
  \begin{equation*}
    w(t) = e^{-\frac{1}{n}X^\T Xt}(w_0 - (X^\T X)^{-1}X^\T y) + (X^\T X)^{-1}X^\T y = e^{-X^\T Xt}(w_0 - w^\star) + w^\star.
  \end{equation*}
  We see that since $X^\T X$ is positive definite, we have $\lambda_i(-\frac{1}{n}X^\T X) < 0$, $i = 1, \dots, d$ for all eigenvalues of $-X^\T X$. Thus, the solution to the gradient flow equation converges to the minimizer:
  \begin{equation*}
    \lim_{t \rightarrow \infty} w(t) = \lim_{t \rightarrow \infty} e^{-\frac{1}{n}X^\T Xt}(w_0 - w^\star) + w^\star = w^\star.
  \end{equation*}
  The rate of convergence is determined by $\sigma_d^2/n$, where $\sigma_d$ is the smallest singular value of $X$. Next, we derive the second order modified equation and compare the solution with the first order solution. In the case of linear regression the second order modified equation defined in \eqref{eq:second_order_ode} is given by
  \begin{equation*}
    \widetilde{w}'(t) = -\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)\widetilde{w}(t) + \frac{1}{n}\left(I_d + \frac{\eta}{2n} X^\T X\right)X^\T y, \quad \widetilde{w}(0) = w_0.
  \end{equation*}
  Once again this is a linear system of inhomogeneous equations. We note that the matrix $-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)$ has full rank since the sum of two negative definite matrices is negative definite by \autoref{lem:sum_positive_definite}. 
  Thus, by \autoref{thm:ode_system} we have the solution
  \begin{equation}
    \label{eq:linear_regression_second_order}
    \widetilde{w}(t) = e^{-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)t}(w_0 - w^\star) + w^\star.
  \end{equation}
  We can see this by using the singular value decomposition of $X = U D V^\T$, where $U \in \R^{n \times n},V \in \R^{d \times d}$ are orthonormal matrices and $D \in R^{n \times d}$ is a rectangular diagonal matrix. We have $X^\T X = V D^2 V^\T$ and for $A^{-1}b$ in \autoref{thm:ode_system} we follow
  \begin{align*}
    \left(-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)\right)^{-1}\left(\frac{1}{n}\left(I_d + \frac{\eta}{2n} X^\T X\right)X^\T y\right) \\
    =- V\left(I_d + \frac{\eta}{2n}D^\T D\right)^{-1}(D^\T D)^{-1}\left(I_d + \frac{\eta}{2n}D^\T D\right)V^\T X^\T y \\
    = -V (D^\T D)^{-1} V^\T X^\T y = (X^\T X)^{-1} X^\T y = w^\star.
  \end{align*}

  % Thus, we have the solution
  % \begin{align*}
  %   \widetilde{w}(t) &= e^{-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)}\Bigl(w_0 - \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}(X^\T X)^{-1}X^\T y\Bigr) \\
  %   &+ \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}(X^\T X)^{-1}X^\T y \\
  %   &=e^{-\frac{1}{n}X^\T X \left(I_d + \frac{\eta}{2n}X^\T X\right)}\Bigl(w_0 - \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}w^\star\Bigr) + \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}w^\star.
  % \end{align*}
  % Now, consider the eigenvalue decomposition $X^\T X = Q^\T \Lambda Q$, where $Q^\T Q = I_d$ and $\Lambda = \diag(\lambda_1, \dots, \lambda_d)$. If we denote the limiting value of $\widetilde{w}$ as $\widetilde{w}^\star \defeq \Bigl(I_d +\frac{\eta}{2n}X^\T X \Bigr)^{-1}w^\star$, we have
  % \begin{align*}
  %   \norm{\widetilde{w}^\star}^2 &= {w^\star}^\T (I_d + \frac{\eta}{2 n}X^\T X)^{-2} w^\star = {Qw^\star}^\T (I_d + \frac{\eta}{2 n}\Lambda)^{-2} Qw^\star \\
  %   &= \sum_{j=1}^d \frac{1}{(1 + \frac{\eta}{2 n}\lambda_i)^2} (Qw^\star)_i^2 \leq \sum_{j=1}^d (Qw^\star)_i^2 = \norm{Qw^\star}^2 = \norm{w^\star}.
  % \end{align*}
  % This result agrees with the intuition that gradient descent for finite learning rates regularizes the loss function. Indeed, the gap between the stationary first and second order solution is driven by $\frac{\eta}{2 n} \min_{i=1,\dots,d}\lambda_i(X^\T X)$. This means that the strength of the regularization is proportional to the learning rate and smallest eigenvalue of $X^\T X$ and inversely proportional to the sample size $n$. 
  While the limiting behavior of the second order modified equation is the same as the first order equation, we can see different dynamics in \eqref{eq:linear_regression_second_order}. The dynamics of the second order equation are driven by $\sigma_d^2/n + \eta\sigma_d^4/(2n^2)$, where again $\sigma_d$ is the smallest singular value of $X$. In the first order equation we do not observe the improved convergence speed for larger learning rates.
  We conclude that for sufficiently small learning rates gradient flow captures the dynamics of GD. However, for larger learning rates gradient flow fails to model dynamics induced by the discretization. In \autoref{fig:linear_regression_error}, we present experimental verification for the linear regression case. We set $n = 1000$, $d = 5$ and start with a learning rate of $\eta = 0.2$ and decrease by powers of two. We can see that indeed the maximal error for the first order model decreases linearly, while the second order model decreases quadratically. Further, we observe numerical instabilities in the calculation second order model for learning rates $\eta < 0.001$.

  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/linear_regression_error.pdf}
    \caption{Approximation error for first and second order modified equations}
    \label{fig:linear_regression_error}
  \end{figure}
\end{example}
\subsection{Stochastic gradient descent}
\label{sec:smde_sgd}
Now, similar to the modified equations for GD we present an argument to approximate SGD by a continuous-time process.
% We start with an observation from Li et al.\ \cite{liStochasticModifiedEquations2019} that the SGD iterates can be decomposed into a deterministic and a stochastic part.
Let $(\Omega, \CF, \BP)$ be a probability space. Recall from \autoref{sec:stochastic_optimization} that for a fixed learning rate $\eta > 0$ and i.i.d random variables $\gamma_k:\Omega \rightarrow \{1,\dots,n\}$ the iterates of SGD are given by
\begin{equation*}
  w_{k+1} = w_{k} - \eta \nabla f_{\gamma_k}(w_{k})
\end{equation*}
for $k = 0,1,\dots$. Further, we have $f : \R^d \rightarrow \R$ with $f(w) \defeq \ev{f_\gamma(w)}$ for all $w \in \R^d$.

Li et al.\ \cite{liStochasticModifiedEquations2019} observe that the iterates can be decomposed as
\begin{equation}
  \label{eq:sgd_decomposition}
  w_{k+1} = w_{k} - \eta \nabla f(w_{k}) + \eta Z_k,
\end{equation}
where $Z_k \defeq \nabla f(w_{k}) - \nabla f_{\gamma_k}(w_{k})$.

\begin{definition}[Gradient noise process]
  Let $\{w_k\}_{k=0}^\infty$ be the iterates of SGD. Then we call the stochastic process $\{Z_k\}_{k=0}^\infty$ given by
  \begin{equation*}
    Z_k \defeq \nabla f(w_{k}) - \nabla f_{\gamma_k}(w_{k}),
  \end{equation*}
  for $k=0,\dots$ \emph{gradient noise process}. For $k = 1, \dots$ we write 
  \begin{equation*}
    \Sigma(w_k) \defeq \Cov\left[Z_k|w_{k}\right] = \ev{\left(\nabla f(w_{k}) - \nabla f_{\gamma_k}(w_{k})\right)\left(\nabla f(w_{k}) - \nabla f_{\gamma_k}(w_{k})\right)^\T |w_k}
  \end{equation*}
  to denote the conditional covariance of the gradient noise process.
\end{definition}
Using decomposition \eqref{eq:sgd_decomposition} we see that SGD is a stochastic process driven by the negative gradient and the gradient noise process $\{Z_k\}_{k=0}^\infty$. Note that each iterate of the noise process has zero conditional expectation, i.e., $\ev{Z_k|w_{k}} = 0$.

To continue the discussion of modelling SGD by a continuous-time process we investigate the distribution of the gradient noise process empirically.
\begin{example}[\autoref{ex:linear_regression} continued]
  \label{ex:sgd}
  In this example, we apply SGD to the linear regression problem from \autoref{ex:linear_regression}. In \autoref{fig:sgd_linear_fit},for $N = 1000$ iterations we observe the mean weight $\{\bar{w}_k\}_{k=0}^{N}$ and mean bias $\{\bar{b}_k\}_{k=0}^{N}$ of SGD for $m=10000$ independent runs, i.e.,
  \begin{equation*}
    \bar{w}_k = \frac{1}{m}\sum_{i=1}^m w_{k,i}, \quad \bar{b}_k = \frac{1}{m}\sum_{i=1}^mb_{k,i},
  \end{equation*}
  for $k = 0, \dots, N$ and a fixed learning rate $\eta = 0.1$. We can see that the mean trajectories correspond to the trajectories observed for GD in \autoref{fig:learning_rates}.
  Further, we use a bar plot to visualize the distribution of the weight parameter $w$ for $12.5\%,\; 25\%,\; 50\%$ and $100\%$ of the total iterations in \autoref{fig:sgd_weight_histogram}. The shape of each bar plot approximates a bell curve which is characteristic for a normally distributed random variable. In order to confirm this hypothesis, we make use of a quantile-quantile plot in \autoref{fig:sgd_weight_qq}.
  In this plot, we compare the empirical distribution function against the distribution function of a normally distributed random variable. We observe that empirical measurements nearly perfectly match the quantiles of a normal distribution. Therefore, we conclude that the weights are approximately normally distributed.
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/sgd_weight_bias.pdf}
    \caption{Weight and bias for SGD iterates}
    \label{fig:sgd_linear_fit}
  \end{figure}
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/sgd_weight_histogram.pdf}
    \caption{Histogram of parameter $w$ for SGD sample trajectories}
    \label{fig:sgd_weight_histogram}
  \end{figure}
  \begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{../seminar_talk/plots/sgd_weight_qq.pdf}
    \caption{Quantile-quantile plot for normal distribution and sampled values of the weight $w$ at the last iteration}
    \label{fig:sgd_weight_qq}
  \end{figure}
\end{example}
Following the discussion in \autoref{ex:sgd}, for the rest of this section we assume that each iterate of the gradient noise process follows a normal distribution. Li et al.\ \cite{liStochasticModifiedEquations2019} make this assumption to motivate the connection between SGD iterates and the Euler-Maruyama scheme. However, we note that for deep learning objectives experimental evidence suggest that this assumption is not valid in general. We discuss this in \autoref{sec:smdedl} in more detail.
% However, ̧Simsekli et al.\ \cite{simsekliTailIndexAnalysisStochastic2019} argue gradient noise follows a heavy-tailed distribution.
\begin{assumption}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{as:normality}
  Let $\{w_k\}_{k=0}^\infty$ the SGD iterates defined in \eqref{eq:stochastic_gradient_descent} and $\{Z_k\}_{k=0}^\infty$ be the gradient noise process. Then, we assume that the iterates are normally distributed with expected value $\ev{Z_k|w_k} = 0$ and covariance matrix $\Cov \left[Z_k | w_k\right] = \Sigma(w_k)$, i.e.,
  \begin{equation*}
    Z_k \sim \CN(0, \Sigma(w_k)),
  \end{equation*}
for $k=0,1,\dots$.  
\end{assumption}
Now, in the case that the covariance matrix $\Sigma(w)$ has full rank, we write $\Sigma(w)^{1/2}$ for its symmetric and positive definite square root matrix that fulfills $\Sigma(w)^{1/2}\Sigma(w)^{1/2} = \Sigma(w)$ for $w\in \R^d$. Applying \autoref{as:normality} to the decomposition in \eqref{eq:sgd_decomposition} we have
\begin{equation}
  \label{eq:sgd_normal_decomposition}
  \begin{split}
    w_{k+1} &= w_{k} - \eta \nabla f(w_{k}) + \eta Z_k \\
  &= w_k - \nabla f(w_k) \eta + \sqrt{\eta}\Sigma(w_k)^{1/2}\Delta \mathbf{B}_k,
  \end{split}
\end{equation}
where $\Delta \mathbf{B}_k \sim \CN(0,\eta I_d)$ and $k=0,1,\dots$. 
% Comparing \eqref{eq:sgd_normal_decomposition} with the Euler-Maruyama scheme in \eqref{eq:euler_maruyama}, we see that SGD approximates a SDE. 
% This motivates the following definition.
By defining $\mathbf{a}(w,t) = -\nabla f(w)$ and $\mathbf{b}(w,t) = \sqrt{\eta}\Sigma(w)^{1/2}$ in \autoref{def:euler_maruyama}, we see that the SGD iterates approximate a SDE. 
\begin{definition}[Stochastic modified differential equation]
  For an initial value $w_0 \in \R^d$ the stochastic differential equation
  \begin{equation}
    \label{eq:first_oder_smde}
    dW_t = -\nabla f(W_t)dt + \sqrt{\eta} \Sigma(W_t)^{1/2}d\mathbf{B}_t, \quad W_0 = w_0
  \end{equation}
  modelling the continuous-time behavior of SGD is called \emph{stochastic modified differential equation} (SMDE).
\end{definition}
Up to this point we have motivated the stochastic model of SGD using heuristic arguments. In the following, we go through the approximation results by Li et al.\ \cite{liStochasticModifiedEquations2019} and extend the proof of the main convergence theorem.
In \autoref{cor:first_order}, we see that equation \eqref{eq:first_order_sde} is a generalization of the gradient flow equation for GD. 

First, Li et al.\ \cite{liStochasticModifiedEquations2019} make three assumptions that guarantee the existence of the first and second moment. We note that in the case of a risk minimization objective, where the random variable $\gamma:\Omega \rightarrow \{1,\dots,n\}$ simply selects an index, all three assumption immediately follow.
\begin{assumption}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{as:sde_model}
  Let $(\Omega, \CF, \BP)$ be a probability space and $(\Gamma, \CF_\Gamma)$ be a measure space. For $w \in \R^d$ the random variable $f_\gamma(w)$ satisfies 
  \begin{enumerate}[label=(\roman*)]
    \item $f_{\gamma}(w) \in \mathcal{L}^1(\Omega)$ for all $w \in \R^d$,
    \item \label{as:bounded_gradient} $f_{\gamma}(w)$ is continuously differentiable in $w$ almost surely and for each $R > 0$, there exists a random variable $M_{R,\gamma}$ such that $\max_{\norm{x} \leq R} \norm{ \nabla f_{\gamma}(w) } \leq M_{R,\gamma}$ almost surely, with $\mathbb{E} |M_{R,\gamma}| < \infty$,
    \item $\nabla f_{\gamma}(w) \in \mathcal{L}^2(\Omega)$ for all $w \in \R^d$.
  \end{enumerate}
\end{assumption}

Next, they consider the family of equations of the form
\begin{equation}
  \label{eq:general_sde}
  dW^{\eta, \epsilon}_t = \mathbf{a}^\eta(W^{\eta, \epsilon}_t,\epsilon)dt + \sqrt{\eta}\mathbf{b}^\eta(W^{\eta, \epsilon}_t, \epsilon)d\mathbf{B}_t, \quad W_0 = w_0, \quad t \in [0,T],
\end{equation}
for $\epsilon \in (0,1)$, $T > 0$. 
This equation is a stochastic generalization of the modified ordinary differential equation in \eqref{eq:modified_ode}. Li et al.\ consider a first order approximation for the drift term and zeroth order approximation for the diffusion term, i.e.,
\begin{equation}
  \mathbf{a}^\eta(w, \epsilon) = \mathbf{a}_0(w, \epsilon) + \eta \mathbf{a}_1(w, \epsilon), \quad \mathbf{b}^\eta(w, \epsilon) = \mathbf{b}_0(w, \epsilon).
\end{equation}


The parameter $\epsilon \in (0,1)$ is a mollification parameter. This is a technique introduced by Li et al.\ \cite{liStochasticModifiedEquations2019} to relax the smoothness assumption on the target function $f$. In particular, both the first (see \autoref{cor:first_order}) and second order (see \autoref{thm:second_order})  convergence theorems do not require any additional smoothness on $f$ other than the smoothness required to guarantee the existence and uniqueness of solutions to the modified equations.
To understand this technique they introduce two concepts: weak derivatives and mollifiers. Weak derivatives generalize differentiation to locally integrable functions. Mollifiers are useful for creating a sequence of smooth functions that approximate a particular nonsmooth function. We briefly introduce weak derivatives and refer to the appendix for more details on mollifiers. The textbook by Evans \cite[pp.~629]{evansPartialDifferentialEquations2010} provides an in-depth introduction.
\begin{definition}[Weak derivatives, \protect{\cite[pp.~242]{evansPartialDifferentialEquations2010}}]
  \label{def:weak_derivatives}
  Let $f,g \in \CL^1_{\text{loc}}(\R^d, \R)$ (see \autoref{sec:notation}) and let $\alpha = (\alpha_1, \dots, \alpha_n)$ be a multiindex. Then, $g = D^{\alpha} f$ is called \emph{$\alpha$ order weak derivative of $f$}, if
  \begin{equation}
    \label{eq:weak_derivative}
    \int_U f D^{\alpha} \phi dx = (-1)^{|\alpha|} \int_U g \phi dx
  \end{equation}
  for all test functions $\phi \in \CC_c^{\infty}(U)$, where $U \subset \R^d$ is an open set.
\end{definition}
\begin{example}
  Let us consider the function $f : \R \rightarrow \R^+$, $f(x) = |x|$ which is continuous in every $x \in \R$. For $x \neq 0$ the absolute value is differentiable. However, for $x = 0$ we have
  \begin{align*}
    \lim_{h\rightarrow 0^+} \frac{\lvert x + h \rvert - \lvert x \rvert}{h} &= \lim_{h\rightarrow 0^+} \frac{h}{h} = 1 \\
    \lim_{h\rightarrow 0^-} \frac{\lvert x + h \rvert - \lvert x \rvert}{h} &= \lim_{h\rightarrow 0^+} \frac{-h}{h} = -1.
  \end{align*}
  Thus, $f$ is not differentiable in $x = 0$.
  Now, let $\phi \in C_c^{\infty}(\R^d)$ be a test function. Then, we have
  \begin{equation*}
    I \defeq \int_{-\infty}^{\infty}|x|\phi'(x)dx = \int_{-\infty}^{0}-x\phi'(x)dx + \int_{0}^{\infty}x\phi'(x)dx
  \end{equation*}
  Using partial integration we obtain
  \begin{equation*}
    \begin{split}
      I &= \lim_{x \rightarrow -\infty} -x \phi(x) - \int_{-\infty}^{0}-\phi(x)dx + \lim_{x \rightarrow \infty} x \phi(x) - \int_{0}^{\infty}\phi(x)dx \\
      &= -\int_{-\infty}^{\infty}\sign(x)\phi(x)dx.
    \end{split}
  \end{equation*}
  We see that the first order weak derivative of $f$ is the $\sign$ function. Notice, that the weak derivative is not unique in a pointwise sense. In this example, we could change the value of the weak derivative at $x=0$ and the integral would not change. In general, modifications of on sets of zero measure do not change the integral in \eqref{eq:weak_derivative}. Therefore, we understand uniqueness of the weak derivative in an almost sure sense.
\end{example}



% The existence and uniqueness result in \autoref{subsec:sde_existence_uniqueness} require a uniform linear growth and a Lipschitz condition for the drift $\mathbf{a}^\eta$ and the diffusion $\mathbf{b}^\eta$ function. 
% To proof the approximation result we need to make an assumption on the growth of the drift and diffusion term of \eqref{eq:general_sde}.
Li et al.\ make an important observation regarding the SMDE and SGD: the filtration $\CF^{\mathbf{B}}_{k\eta} = \sigma(B_s, 0 \leq s \leq k \eta)$ generated by Brownian motion and the filtration $\CF^\gamma_k = \sigma(\gamma_l, l = 0,1,\dots, k)$ generated by the sampling variable are independent. This implies that we cannot establish a strong convergence (see \autoref{def:strong_convergence}) of the SGD iterates and the SMDE solution. For this reason, Li et al.\ consider weak convergence that implies convergence in a distributional sense for a class of functions $\mathcal{K}$ (see \autoref{def:weak_convergence}). In particular, they define the class of functions with at most polynomial growth.
\begin{definition}[Polynomial growth, $G$, $G_w$ \protect{\cite{liStochasticModifiedEquations2019}}]
  Let $G$ denote the set of continuous functions $g : \R^d \rightarrow \R$ of at most \emph{polynomial growth}, i.e., $g \in G$ if there exist positive integers $\kappa_1, \kappa_2 > 0$ such that
  \begin{equation*}
    |g(w)| \leq \kappa_1(1 + \norm{w}^{2\kappa_2})
  \end{equation*} 
  for all $w \in \R^d$. Moreover, for each integer $\alpha \geq 1$ we denote by $G^{\alpha}$ the set of $\alpha$-times continuously differentiable functions $g : \R^d \rightarrow \R$ which, together with its partial derivatives up to and including order $\alpha$, belong to $G$. By $G^{\alpha}_w$ we denote the class of functions with polynomial growth and partial weak derivatives of up to order $\alpha$. 
\end{definition}
Next, we adopt some notation for the one-step error of both SGD and the SDE:
\begin{equation}
  \label{eq:one_step_error}
  \Delta(w) \defeq w_{1}^{w,0} - w, \quad \widetilde{\Delta}(w) \defeq \widetilde{W}^{w,0}_1 - w,
\end{equation}
where $\{w_k^{w,0}: k = 0,1,\dots, N\}$ denotes the iterates of SGD defined in \eqref{eq:stochastic_gradient_descent} with $w_0 = w$ and $\{\widetilde{W}_k^{w,0} = W_{k\eta}: k = 0,1,\dots, N\}$ the solution of the SDE defined in \eqref{eq:general_sde} with initial value $W_0 = w$ at the time steps corresponding to the SGD iterates.
Li et al.\ present two results that characterize the first three moments of the one-step error for both SGD iterates and the SDE solution. In \autoref{thm:second_order}, they use these estimates to meet the assumptions of \autoref{thm:approximation} and obtain a global bound.
% In \autoref{thm:approximation},  these which is the main ingredient to prove the approximation results \autoref{thm:second_order} and \autoref{cor:first_order}.
\begin{lemma}[\protect{\cite[Lemma 5]{liStochasticModifiedEquations2019}}]
  \label{lem:sgd_one_step}
  Let $\Delta(w)$ be defined as in \eqref{eq:one_step_error}. Suppose that for each $w \in \R^d$, $f \in G^1$. Then, for $i,j,k=1,\dots,d$ we have
  \begin{enumerate}[label=(\roman*)]
    \item $\ev{\Delta_{i}(w)} = -\partial_{w_i} f(w) \eta$,
    \item $\ev{\Delta_{i}(w)\Delta_{j}(w)} = \partial_{w_i} f(w)\partial_{w_j} f(w) \eta^2 + \Sigma(w)_{i,j}\eta^2$,
    \item $\ev{\Delta_{i}(w)\Delta_{j}(w)\Delta_{k}(w)} = \CO(\eta^3)$,
  \end{enumerate}
  where $\Sigma(w)$ is the covariance matrix of the gradient noise process.
\end{lemma}
\begin{proof}
  By the definition of the SGD iterates we have
  \begin{equation*}
    \Delta(w) = w_{1} - w = w - \eta \nabla f_{\gamma}(w) - w = - \eta \nabla f_{\gamma}(w).
  \end{equation*}
  By \autoref{as:sde_model} \ref{as:bounded_gradient} and the \hyperref[thm:dominated_convergence]{dominated convergence theorem} for $i \in \{1,\dots,d\}$ we follow
  \begin{equation*}
    \ev{\Delta_{i}(w)} = - \eta \partial_{w_i} f(w).
  \end{equation*}
  Further, for  $i,j \in \{1,\dots,d\}$
  \begin{equation*}
    \ev{\Delta_{i}(w)\Delta_{j}(w)} = \eta^2 \ev{\partial_{w_i}f_{\gamma}(w)\partial_{w_j}f_{\gamma}(w)}.
  \end{equation*}
  Now, we add and subtract some terms to obtain 
  \begin{align*}
    \ev{\Delta_{i}(w)\Delta_{j}(w)}
    &= \eta^2 \mathbb{E}\left[(\partial_{w_i}f_{\gamma}(w)- \partial_{w_i}f(w))(\partial_{w_j}f_{\gamma}(w) - \partial_{w_j}f(w)) \right. \\
    &+ \left. \partial_{w_i}f_{\gamma}(w)\partial_{w_j}f(w) + \partial_{w_i}f(w)\partial_{w_j}f_{\gamma}(w) - \partial_{w_i}f(w)\partial_{w_j}f(w) \right].
  \end{align*}
  Then, again using \autoref{as:sde_model} \ref{as:bounded_gradient} we have $\ev{\partial_{w_i}f_\gamma(w)} = \partial_{w_i}f(w)$ and thus
  \begin{align*}
    \ev{\Delta_{i}(w)\Delta_{j}(w)} &= \partial_{w_i} f(w)\partial_{w_j} f(w) \eta^2 + \ev{(\partial_{w_i}f_{\gamma}(w)- \partial_{w_i}f(w))(\partial_{w_j}f_{\gamma}(w) - \partial_{w_j}f(w))}\eta^2 \\
    &= \partial_{w_i} f(w)\partial_{w_j} f(w) \eta^2 + \Sigma(w)_{i,j}\eta^2.
  \end{align*}
  Lastly, for $i,j,k \in \{1,\dots,d\}$ we have
  \begin{equation*}
    \ev{\Delta_{i}(w)\Delta_{j}(w)\Delta_{k}(w)} = \eta^3 \ev{-\partial_{w_i}f_{\gamma}(w)\partial_{w_j}f_{\gamma}(w)\partial_{w_k}f_{\gamma}(w)} = \CO(\eta^3). \qedhere
  \end{equation*}
\end{proof}
\begin{lemma}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{lem:sde_one_step}
  Let $ \widetilde{\Delta}(w)$ be defined as in \eqref{eq:one_step_error}. Suppose that $\mathbf{a}_0, \mathbf{a}_1, \mathbf{b}_0 \in G^3$. Then, for $i,j,k = 1,\dots, d$ we have
  \begin{enumerate}[label=(\roman*)]
    \item  $\ev{\widetilde{\Delta}_{i}(w)} = a_{0}(w,\epsilon)_{i}\eta + \left[\frac{1}{2}\sum_{l=1}^d a_{0}(w,\epsilon)_{l}\partial_{w_l}a_0(w,\epsilon)_{i} + a_1(w,\epsilon)_{i}\right] \eta^2 + \mathcal{O}(\eta^3)$,
    \item $\ev{\widetilde{\Delta}_{i}(w)\widetilde{\Delta}_{j}(w)} = \left[a_0(w, \epsilon)_{i}a_0(w,\epsilon)_{j} + \sum_{k=1}^db_0(w, \epsilon)_{i,k}b_0(w,\epsilon)_{j,k}\right]\eta^2 + \mathcal{O}(\eta^3)$,
    \item $\ev{\widetilde{\Delta}_{i}(w)\widetilde{\Delta}_{j}(w)\widetilde{\Delta}_{k}(w)} = \mathcal{O}(\eta^3)$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof can be found in \cite[Lemma 4]{liStochasticModifiedEquations2019}.
\end{proof}
\begin{theorem}[\protect{\cite[pp.~9]{liStochasticModifiedEquations2019}}]
  \label{thm:approximation}
  Let $T > 0, \; \eta \in (0,\min\{1,T\}), \: \epsilon \in (0,1)$ and $N = \lfloor T / \eta \rfloor$. Let $p \geq 1$ be an integer. Suppose further that the following conditions hold:
  \begin{enumerate}[label=(\roman*)]
    \item There exists a function $\rho : (0,1) \rightarrow \R_+$ and $K_1 \in G$ independent of $\eta, \epsilon$ such that
    \begin{equation*}
      \left\lvert \ev{\prod_{j=1}^s \Delta_{(i_j)}(w)} - \ev{\prod_{j=1}^s \tilde{\Delta}_{(i_j)}(w)}\right\rvert \leq K_1(w)(\eta \rho(\epsilon) + \eta^{p+1}),
    \end{equation*}
    for $s=1,2,\dots,p$ and
    \begin{equation*}
      \ev{\prod_{j=1}^{p+1}\left\lvert \Delta_{(i_j)}(w)\right\rvert} \leq K_1(w)\eta^{p+1},
    \end{equation*}
    for all $i_j \in \{1,\dots,d\}$.
    \item For each $m \geq 1$, the $2m$-moment of $w_k^{w,0}$ is uniformly bounded with respect to $k$ and $\eta$, i.e., there exists a $K_2 \in G$, independent of $\eta, k$, such that
    \begin{equation*}
      \ev{\left\lvert w_k^{w,0} \right\rvert^{2m}} \leq K_2(w),
    \end{equation*}
    for all $k = 0, \dots, N = \lfloor T\ \eta \rfloor$.
  \end{enumerate}
  Then, for each $g \in G^{p + 1}$, there exists a constant $C > 0$, independent of $\eta, \epsilon$, such that
  \begin{equation*}
    \max_{k=0,\dots,N}|\ev{g(w_k)} - \ev{g(W_{k\eta})}|\leq C(\eta^{p} + \rho(\epsilon))
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof of this theorem can be found in \cite{liStochasticModifiedEquations2019}.
\end{proof}
\begin{lemma}[\protect{\cite[Lemma 29]{liStochasticModifiedEquations2019}}]
  \label{lemma:moment_bound}
  Let $\{w_k: k \geq 0 \}$ be the SGD iterations defined in \eqref{eq:stochastic_gradient_descent}. Suppose for some random variable $L_{\gamma} > 0$ almost surely and 
  \begin{equation*}
    \ev{L_{\gamma}^m} < \infty
  \end{equation*}
  for all $m \geq 1$. Then, for fixed $T > 0$ and any $m \geq 1$, $\ev{\norm{w_k}^m}$ exists and is uniformly bounded in $\eta$ and $k=0,\dots,N = \lfloor T / \eta \rfloor$.
\end{lemma}
\begin{proof}
  The proof can be found in \cite[Lemma 29]{liStochasticModifiedEquations2019}.
\end{proof}
\begin{theorem}[\autocite{liStochasticModifiedEquations2019}]
  \label{thm:second_order}
  Let $(\Omega, \CF, \BP)$ be a probability space, $(\Gamma, \CF_{\Gamma})$ be a measure space, $\gamma : \Omega \rightarrow \Gamma$ be an indexing random variable and $f_\gamma : \R^d \rightarrow \R$ a function fulfilling \autoref{as:sde_model}.
  Further, let $T > 0$, $\eta \in (0, \min\{1,T\})$ and set $N = \lfloor T/\eta \rfloor$. Let $\{w_k:k\geq 0\}$ be the iterations defined in \eqref{eq:stochastic_gradient_descent}. Suppose and the following conditions are met:
  \begin{enumerate}[label=(\roman*)]
    \item $f \defeq \ev{f_{\gamma}}$ is twice continuously differentiable, $\nabla \norm{\nabla f} ^2$ is Lipschitz continuous, and $f \in G^4_w$.
    \item $\nabla f_{\gamma}$ satisfies the Lipschitz condition
    \begin{equation*}
      |\nabla f_{\gamma}(w) - \nabla f_{\gamma}(v)| \leq L_{\gamma} |w - v|
    \end{equation*}
    almost surely for all $w,v \in \R^d$, where $L_{\gamma}$ is a random variable which is positive almost surely and $\ev{L_{\gamma}^m} < \infty$ for each $m \geq 1$.
  \end{enumerate}
  Define $\{W_t:t\in [0,T] \}$ as the stochastic process satisfying the SDE
  \begin{equation}
    \label{eq:second_order_sde}
    d W_t = -\nabla\left(f(W_t) + \frac{1}{4}\eta \norm{\nabla f(W_t)}^2\right)dt + \sqrt{\eta}\Sigma(W_t)^{1/2}d\mathbf{B}_t, \quad W_0 = w_0.
  \end{equation}
  Then, $\{W_t:t\in [0,T] \}$ is a second order weak approximation of SGD, i.e., for each $g \in G^3$, there exists a constant $C > 0$ independent of $\eta$ such that
  \begin{equation}
    \label{eq:second_order_convergence}
    \max_{k=0,\dots,N} |\ev{g(w_k)} - \ev{g(W_{k\eta})}| \leq C \eta^2.
  \end{equation}
\end{theorem}

\begin{proof}
  The proof of the approximation result from Li et al.\ consists of the following four steps:
  \begin{enumerate}
    \item Using the smoothness and Lipschitz assumption they show existence and uniqueness of the solution to \eqref{eq:second_order_sde}.
    \item They mollify the drift and diffusion terms in equation \eqref{eq:second_order_sde} to obtain \autoref{lem:sde_one_step} the moment estimates of the one-step errors in \autoref{lem:sgd_one_step} and \autoref{lem:sde_one_step}.
    \item Using \autoref{thm:approximation} they show the bound for the modified stochastic differential equation \eqref{eq:general_sde} and the iterates of SGD.
    \item Using the convergence of the mollified drift and diffusion terms they show the weak approximation inequality.
  \end{enumerate}
  To show that there exists a unique stochastic process $\{W_t: t \in [0,T]\}$ satisfying equation \eqref{eq:second_order_sde} we need to show that the drift and diffusion in \eqref{eq:second_order_sde} satisfy the conditions of \autoref{thm:sde_existence}.

  First, we show that $\nabla f$ satisfies a Lipschitz condition. By assumption \autoref{as:sde_model}~\ref{as:bounded_gradient} and by \autoref{thm:dominated_convergence} we have $\nabla f = \nabla \ev{f_\gamma} = \ev{\nabla f_\gamma}$. Now, let $w,v \in \R^d$. Using Jensen's inequality we have
  \begin{align*}
    \norm{\nabla f(w) - \nabla f(v)} &= \norm{\ev{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(v)}} \\
    &\leq \ev{\norm{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(v)}} 
    \leq \ev{L_{\gamma}}\norm{w-v}.
  \end{align*}
  Next, we show that $\Sigma(w)^{1/2}$ is Lipschitz continuous. For this we define $h(w) \defeq \nabla f_{\gamma}(w) - \nabla f(w)$. We see that for $w,v \in \R^d$, using the triangle inequality we have
  \begin{equation}
    \label{eq:u_lipschitz}
    \begin{split}
      \norm{h(w) - h(v)} &\leq \norm{\nabla f(w) - \nabla f(v)} + \norm{\nabla f_{\gamma}(w) - \nabla f_{\gamma}(v)} \\
      &\leq \left(\ev{L_{\gamma}} + L_{\gamma}\right)\norm{w-v}.  
    \end{split}
  \end{equation}
  Next, we observe that
  \begin{equation}
    \label{eq:sigma_inequality}
    \begin{split}
      \normf{\Sigma(w)^{1/2} - \Sigma(v)^{1/2}} &= \left| \norml{[h(w)h(w)^\T]^{\frac{1}{2}}} - \norml{[h(v)h(v)^\T]^{\frac{1}{2}}} \right| \\
      &\leq \norml{[h(w)h(w)^\T]^{\frac{1}{2}} - [h(v)h(v)^\T]^{\frac{1}{2}}}.
    \end{split}
  \end{equation}
  We expand on the claim by Li et al.\ that the mapping $h \rightarrow (hh^\T)^{\frac{1}{2}} = hh^\T/\norm{h}$ is Lipschitz continuous. Let $h,q \in \R^d$. Then, we have
  \begin{equation}
    \label{eq:mapping_lipschitz}
    \begin{split}
    \normf{(hh^\T)^{\frac{1}{2}} - (qq^\T)^{\frac{1}{2}}}^2 &= \normf{hh^\T/\norm{h} - qq^\T/\norm{q}}^2 = \sum_{i,j = 1}^d (h_ih_j/\norm{h} - q_iq_j/\norm{q})^2 \\
    &= \norm{h}^2 - 2 \frac{\scp{h}{q}^2}{\norm{h}\norm{q}} + \norm{q}^2.
    % = \norm{h}^2 - 2 \lvert \scp{h}{q} \rvert+ \norm{q}^2 \\
    % &\leq \norm{h}^2 - 2 \scp{h}{q} + \norm{q}^2 = \norm{h-q}^2.
    \end{split}
  \end{equation}
  Now, we note that the scalar product the two vector $h,q$ can be written as $\scp{h}{q} = \cos(\theta_{h,q}) \norm{h} \norm{q}$, where $\theta_{h,q}$ is the angle between $h$ and $q$. Thus, it follows
  \begin{equation*}
    \frac{\scp{h}{q}^2}{\norm{h}\norm{q}} = \cos^2(\theta_{h,q})\norm{h}\norm{q}.
  \end{equation*}
  We claim that for $L = \sqrt{2}$ we have Lipschitz continuity. We prove that by showing that the difference $2 \norm{h-q}^2 - \normf{(hh^\T)^{\frac{1}{2}} - (qq^\T)^{\frac{1}{2}}}^2$ is nonnegative. We have
  \begin{align*}
    2 (\norm{h}^2 - 2 &\cos(\theta_{h,q}) \norm{h} \norm{q} + \norm{q}^2) - (\norm{h}^2 - 2 \cos^2(\theta_{h,q})) \norm{h} \norm{q} + \norm{q}^2) \\
    &= \norm{h}^2 + 2 \cos(\theta_{h,q})(\cos(\theta_{h,q}) - 2) \norm{h} \norm{q} + \norm{q}^2 \\
    &\geq \norm{h}^2 - 2\norm{h} \norm{q} + \norm{q}^2 = (\norm{h} - \norm{q})^2 \geq 0.
  \end{align*}
  We note that $\cos(\theta_{h,q})(\cos(\theta_{h,q}) -2 ) \geq -1$ for all $\theta_{h,q} \in \R$.
  Combining equations \eqref{eq:sigma_inequality} and \eqref{eq:u_lipschitz} it follows
  \begin{align*}
    \normf{\Sigma(w)^{1/2} - \Sigma(v)^{1/2}} \leq \sqrt{2}\norm{h(w) - h(v)} \leq \sqrt{2}\left(\ev{L_{\gamma}} + L_{\gamma}\right)\norm{w-v}.
  \end{align*}
  From the arguments above and \autoref{lemma:linear_growth} we see that $\mathbf{a}(w) = -\nabla \left( f(w) + \frac{1}{4}\eta \norm{\nabla f(w)}^2\right)$ and $\mathbf{b}(w) = \eta \Sigma(w)^{1/2}$ fulfill both a Lipschitz and linear growth condition. Thus, $\mathbf{a}$ and $\mathbf{b}$ fulfill \autoref{as:sde_existence} and by \autoref{thm:sde_existence} equation \eqref{eq:second_order_sde} has a unique solution.

  Further, let $\epsilon \in (0,1)$. We define the mollified functions (see \autoref{def:mollifier})
  \begin{equation*}
    \mathbf{a}_0(w, \epsilon) = - \moll * \nabla f(w), \quad \mathbf{a}_1(w, \epsilon) = -\frac{1}{4}\moll * (\nabla\norm{\nabla f(w)}^2), \quad\mathbf{b}_0(w, \epsilon) = \moll * \Sigma(w)^{1/2}.
  \end{equation*}
  Next, we show that $\mathbf{a}_0 + \eta \mathbf{a}_1$, $\mathbf{b}_0$ satisfy a Lipschitz condition in $w \in \R^d$ uniformly in $\eta, \epsilon$.
  For that we note that a Lipschitz continuous function $\psi:\R^d \rightarrow \R$ preserves Lipschitz continuity through mollification. Let $w, v \in \R^d$. Then, by Jensen's inequality we have
  \begin{align*}
    \lvert\moll*\psi(w) - \moll*\psi(v) \rvert &\leq \int_{\CB(0,\epsilon)}\moll(z)\lvert\psi(w-z) - \psi(v-z)\rvert dz \\
    &\leq \int_{\CB(0,\epsilon)}\moll(z)dz \norm{w-v} = \norm{w-v}.
  \end{align*}
  Thus, we have that $\mathbf{a}_0 + \eta \mathbf{a}_1$, $\mathbf{b}_0$ are uniformly Lipschitz. 
  % Proof linear growth condition here
  By \autoref{thm:sde_existence} the family of stochastic processes $\{W_t^{\epsilon}: \epsilon \in (0,1)\}$ satisfying 
  \begin{equation*}
    dW_t^{\epsilon} = \mathbf{a}_0(W_t^{\epsilon}, \epsilon) + \eta \mathbf{a}_1(W_t^{\epsilon}, \epsilon)dt + \sqrt{\eta}\mathbf{b}_0(W_t^{\epsilon}, \epsilon)dB_t, \quad W_0^{\epsilon} = w,
  \end{equation*}
  admits a unique solution for each $\epsilon \in (0,1)$.
  Next, we show that $\mathbf{a}_0(\cdot, \epsilon),\mathbf{a}_1(\cdot, \epsilon),\mathbf{b}_0(\cdot, \epsilon) \in G^3$ uniformly in $\epsilon$.
  We note that we have $f \in G^4_w \subset \CL^1_{\text{loc}}(\R^d)$. Thus, we have $\nabla f$, $\nabla \norm{\nabla f}^2$, $\Sigma^{1/2} \in \CL^1_{\text{loc}}(\R^d)$. By \autoref{lemma:mollifiers} we obtain smoothness for $\mathbf{a}_0, \mathbf{a}_1$ and $\mathbf{b}_0$. Further, polynomial growth is satisfied since for $\psi \in G$, a multiindex $J$ and $w \in \R^d$ we have
  \begin{align*}
    \moll * D^J \psi (w) &= \int_{\R^d}\moll(v-w) D^J \psi (v)dv = \int_{\R^d}\nabla^J\moll(w-v)\psi (v) dv \\
    &= \nabla^J(\moll * \psi)(w).
  \end{align*}
  Moreover, by linear growth and \autoref{lemma:inequality} for $\kappa_1, \kappa_2 \in \N$ we have
  \begin{align*}
    \lvert \psi^{\epsilon}(w) \rvert &\leq \int_{\CB(0, \epsilon)} \moll(v) \lvert \psi(w-v) \rvert dv \leq \int_{\CB(0, \epsilon)} \moll(v) \kappa_1 (1 + \norm{x-y}^{2\kappa_2})dv\\
    &\leq \kappa_1\left(1 + 2^{2\kappa_2 - 1} \norm{w}^{2\kappa_2} + 2^{2\kappa_2 - 1}\frac{1}{\epsilon^d}\int_{\CB(0,\epsilon)}\norm{v}^{2\kappa_2}dv\right).
  \end{align*}
  Notice that $\int_{\CB(0,\epsilon)}\norm{v}^{2\kappa_2} \leq \int_{\CB(0,\epsilon)}dv = \vol(\CB(0,\epsilon)) = \frac{\pi^{d/2}}{\Gamma(\frac{d}{2}+1)}\epsilon^d$, where $\Gamma$ denotes the Gamma function. Thus, we have
  \begin{equation*}
    \lvert \psi^{\epsilon}(w) \rvert \leq \widetilde{\kappa}_1(1+ \norm{w}^{2\kappa_2}),
  \end{equation*}
  for some $\widetilde{\kappa_1} \in \N$ independent of $\epsilon$.
  Therefore, $\psi^{\epsilon} \in G$ uniformly in $\epsilon$.
  This implies that $\mathbf{a}_0(\cdot,\epsilon), \mathbf{a}_1(\cdot,\epsilon),\mathbf{b}_0(\cdot, \epsilon) \in G^3$.
  Then, by \autoref{lemma:mollifier_bound}, \autoref{lemma:moment_bound}, \autoref{lem:sde_one_step} and \autoref{lem:sgd_one_step} we have all conditions of \autoref{thm:approximation} and conclude for each $g \in G^3$
  \begin{equation*}
    \max_{k=0,\dots,N}\norm{\ev{g(w_k)} - \ev{g(W^{\epsilon}_{k\eta})}} \leq C(\eta^2 + \rho(\epsilon)),
  \end{equation*}
  where $C$ is independent of $\eta$ and $\epsilon$ and $\rho(\epsilon) \rightarrow 0$ as $\epsilon \rightarrow 0$. Moreover, since $\mathbf{a}_0(w, \epsilon) \rightarrow \mathbf{a}_0(w, 0)$ uniformly on compact sets, we apply Theorem 20 from \cite{liStochasticModifiedEquations2019} to conclude that
  \begin{equation*}
    \sup_{t \in [0,T]}\ev{\norm{W^{\epsilon}_t - W_t}^2} \rightarrow 0 \text{ as } \epsilon \rightarrow 0.
  \end{equation*}
  Thus, we have
  \begin{multline*}
    \norm{\ev{g(W_{k\eta})} - \ev{g(w_k)}} \leq  \norm{\ev{g(W_{k\eta}^\epsilon)} - \ev{g(w_k)}} + \norm{\ev{g(W_{k\eta}^{\epsilon})} - \ev{g(W_{k\eta})}} \\
    \leq C(\eta^2 + \rho(\epsilon))
    + \ev{\norm{W^{\epsilon}_{k\eta} - W_{k\eta}}^2}^{\frac{1}{2}} \left( \int_0^1 \ev{\normf{Hg(\lambda W^{\epsilon}_{k\eta} + (1-\lambda)W_{k\eta})}^2}d\lambda\right)^{\frac{1}{2}}.
  \end{multline*}
  Using Theorem 19 from \cite{liStochasticModifiedEquations2019} and the assumption that we have $Hg \in G$ for the Hessian of $g$ , the last expectation is finite and hence taking the limit $\epsilon \rightarrow 0$ yields the result.
\end{proof}
\begin{corollary}[\protect{\cite{liStochasticModifiedEquations2019}}]
  \label{cor:first_order}
  Assume the same conditions as in \autoref{thm:second_order}, except that we replace (i) with $f \defeq \E f_{\gamma}$ is continuously differentiable, and $f \in G^3_w$.
  Define $\{W_t:t\in [0,T] \}$ as the stochastic process satisfying the SDE
  \begin{equation}
    \label{eq:first_order_sde}
    d W_t = -\nabla f(W_t) dt + \sqrt{\eta}\Sigma(W_t)^{1/2}d\mathbf{B}_t
  \end{equation}
  with $W_0 = w_0$. Then, $\{W_t:t\in [0,T] \}$ is a first order weak approximation of SGD, i.e., for each $g \in G^2$, there exists a constant $C > 0$ independent of $\eta$ such that
  \begin{equation}
    \label{eq:first_order_convergence}
    \max_{k=0,\dots,N} |\ev{g(w_k)} - \ev{g(W_{k\eta})}| \leq C \eta.
  \end{equation}
\end{corollary}
Li et al.\ \cite{liStochasticModifiedEquations2019} present an example where the solution to the first and second order approximation can be calculated analytically. We extend the calculations and add the derivation of the second order model that is omitted by the authors.
% \begin{lemma}
%   Let $H \in \R^{d \times d}$ be a symmetric, positive definite matrix. Then, for any scalar $t \in \R$ we have
%   \begin{equation*}
%     e^{Ht}H = He^{Ht}.
%   \end{equation*}
% \end{lemma}
% \begin{proof}
%   Let $H \in \R^{d \times d}$ be a symmetric, positive definite matrix. Then, we can decompose $H = V^\T \Lambda V$, where $V$ is orthonormal, i.e., $V^\T V = I_d$ and $\Lambda = \diag(\lambda_1, \dots, \lambda_d)$ is a diagonal matrix of eigenvalues of $H$.
%   We use this decomposition to obtain
%   \begin{equation*}
%     e^{Ht}H = \sum_{k=0}^\infty \frac{(Ht)^k}{k!} H = V^\T \sum_{k=0}^\infty \frac{(Ht)^k}{k!} 
%   \end{equation*}
  
% \end{proof}
\begin{example}[\autocite{liStochasticModifiedEquations2019}]
  \label{ex:quadratic_smde}
  Let $H \in \R^{d\times d}$ be a symmetric, positive matrix. Define the sample objective 
\begin{equation*}
  f_{\gamma}(w) = \frac{1}{2} (w - \gamma)^T H (w - \gamma) - \frac{1}{2} \text{Tr}(H)
\end{equation*}
for $\gamma \sim N(0,I)$.  
The total objective is
\begin{equation}
  \label{eq:objective_function}
  f(w) = \ev{f_{\gamma}(w)} = \frac{1}{2} w^T H w.
\end{equation}
The gradient of the sample objective is given by $\nabla f_\gamma(w) = H(w-\gamma)$. Subsequently, its covariance matrix is given by $\Sigma(w) = \ev{H\gamma \gamma^\T H}$. The entries of this matrix can be simplified as follows
\begin{equation*}
  \Sigma(w)_{i,j} = \ev{\sum_{k=1}^d H_{i,k} \gamma_k \sum_{l=1}^d\gamma_l H_{l,j}} = \sum_{k=1}^d \sum_{l=1}^d H_{i,k} H_{l,j}\ev{ \gamma_k \gamma_l}=\ev{\sum_{k=1}^d H_{i,k}H_{k,j}},
\end{equation*}
for $i,j=1,\dots,d$.  
Therefore, we have $\Sigma(w) = H^2$.
% For this objective function the SGD iterates are defined as
% \begin{equation*}
%   w_{k+1} = w_k - \eta H(w - \gamma)
% \end{equation*}
% for $w_0 \in \R^d$ and learning rate $\eta > 0$.
For an initial value $w_0 \in \R^d$ the first order stochastic modified differential equation approximating the SGD is given by
\begin{equation*}
  dW_t = -H W_t dt + \sqrt{\eta}H d\pmb{B}_t, \quad W_0 = w_0.
\end{equation*}
This process is a special case of the Ornstein-Uhlenbeck process from \autoref{ex:ornstein_uhlenbeck}. If we set $F = H$ and $G = \sqrt{\eta}H$ its solution is given by
\begin{equation}
  \label{eq:ornstein_solution}
  W_t = e^{-t H}\left(w_0 + \sqrt{\eta}\int_0^te^{s H}H \cdot d\pmb{B}_s\right).
\end{equation}
Substituting the solution \eqref{eq:ornstein_solution} into the objective \eqref{eq:objective_function} we obtain
\begin{equation}
  \label{eq:expected_value_ornstein}
  \begin{split}
    f(W_t) &= \frac{1}{2} w_0^T e^{-tH}He^{-tH}w_0 +
     \frac{1}{2} \sqrt{\eta}w_0e^{-tH}H\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s \\ 
    &+ \frac{1}{2} \sqrt{\eta}\left(\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s\right)^T He^{-tH}w_0 \\
    &+ \frac{1}{2} \eta \left(\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s\right)^T H\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s.
  \end{split}
\end{equation}
Taking the expectation in \eqref{eq:expected_value_ornstein} we note that we have
\begin{equation*}
  \ev{\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s} = 0
\end{equation*}
for all $t \geq 0$ by property \hyperref[item:zero_integral]{(i)} of \autoref{thm:ito_isometry}.
Thus, we have 
\begin{equation*}
  \begin{split}
  \ev{f(W_t)} &= \frac{1}{2}w_0^\T He^{-2tH}w_0 \\
  &+ \frac{1}{2} \eta \ev{ \left(\int_0^t e^{(s-t)H}H\cdot d\pmb{B}_s\right)^\T \int_0^tH e^{(s-t)H}H\cdot d\pmb{B}_s}.
  \end{split}
\end{equation*}
Now, using \autoref{lem:ito}, we follow
\begin{equation*}
  \ev{f(W_t)} = \frac{1}{2}w_0^\T He^{-2tH}w_0 + \frac{1}{2}\eta \int_0^t\Tr\left[H^3e^{2(s-t)H}\right]ds.
\end{equation*}
Next, we use the well-known result that the trace of a matrix can be calculated as the sum of its eigenvalues and obtain 
\begin{equation}
  \begin{split}
    \label{eq:analytical_expected_value}
    \ev{f(W_t)} &= \frac{1}{2}w_0^\T He^{-2tH}w_0 + \frac{1}{2}\eta \sum_{i=1}^d \int_0^t\lambda_i(H)^3e^{2(s-t)\lambda_i(H)}ds \\
    & = \frac{1}{2}w_0^\T He^{-2tH}w_0 + \frac{1}{4}\eta \sum_{i=1}^d \lambda_i(H)^2\left(1 - e^{-2t\lambda_i(H)}\right).
  \end{split}
\end{equation}
The second order stochastic modified equation is given by 
\begin{equation*}
  dW_t = -(H + \frac{1}{2}\eta H^2)W_tdt + \sqrt{\eta}Hd\mathbf{B}_t, \quad W_0 = w_0.
\end{equation*}
Analogous to the calculations above, this equation is a special case of the Ornstein-Uhlenbeck process of \autoref{ex:ornstein_uhlenbeck}.This can be seen by setting $F = H + \frac{1}{2}H^2$ and $G = \sqrt{\eta}H$. We substitute the solution into the objective and take the expected value to obtain
\begin{equation*}
  \ev{f(W_t)} = \frac{1}{2} w_0^\T H e^{(-2H + \eta H^2)t}w_0 + \frac{1}{2}\eta \int_0^t \Tr(H^3 e^{-(2H + \eta H^2)(t-s)})ds.
\end{equation*}
Now, using the representation of the trace as the sum of eigenvalues we have
\begin{align*}
  \ev{f(W_t)} &= \frac{1}{2}w_0^\T H \exp(-(2H + \eta H^2)t)w_0 \\
  &+ \frac{1}{2}\eta \sum_{i=1}^d \frac{\lambda_i(H)}{2 + \eta \lambda_i(H)}\left(1 - \exp(-\lambda_i(H)(2 + \eta \lambda_i(H))t)\right).
\end{align*}
Using the analytical expression we have derived for the expected value of the objective function, we compare the average trajectories of 1000 SGD samples with the SDE solution. In \autoref{fig:sde_sgd} we can see that for decreasing learning rate $\eta$ the average SGD trajectory and the SDE solution approach each other.
To confirm \autoref{cor:first_order} we look at the maximum absolute difference between the expected value and the average SGD trajectories in \autoref{fig:convergence_rate}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../seminar_talk/plots/sde_sgd.pdf}
  \caption{Loss value of SDE and SGD}
  \label{fig:sde_sgd}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../seminar_talk/plots/convergence_rate.pdf}
  \caption{Maximum of the absolute difference of analytical expected value and average SGD trajectories}
  \label{fig:convergence_rate}
\end{figure}
\end{example}
% \subsection{Will be removed soon}
% A class of loss function $f : \R^d \rightarrow \R$ relevant to machine learning applications is usually given in the form $f(x,y) = \sum_{i=1}^n f_i(x,y)$.
% \begin{equation}
%   x^{(k+1)} = x^{(k)} - \psi^k \nabla f(x^{(k)}) h +  \psi(t)\sqrt{h/b^{(k)}} \sigma_{MB}(x^{(k)})Z^{(k)}
% \end{equation}
% \begin{equation}
%   dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t))dB(t)
% \end{equation}
% \begin{equation}
%   dX(t) = -\psi(t)\nabla f(X(t))dt + \psi(t)\sqrt{h/b(t)} \sigma_{MB}(X(t), X(t-\xi(t)))dB(t)
% \end{equation}
% A simplified model SDE model is given by 
% \begin{equation}
%   dX_t = -\nabla f(X_t)dt + (\eta \Sigma(X_t))^{\frac{1}{2}}dB_t
% \end{equation}
% where
% \begin{equation}
%   \Sigma(X_t) = \frac{1}{N} \sum_{i=1}^N (\nabla f(x) - \nabla f_i(x))(\nabla f(x) - \nabla f_i(x))^T.
% \end{equation}
% One key observation is that the model includes the full gradient as well as a covariance term that requires the evaluation of each individual gradient term. Many commonly used numerical schemes  to obtain samples of the solution of this SDE require the evaluation of the bias $b$ and the drift $\sigma$. For example, the Euler-Maruyama scheme introduced in section (\ref{subsec:sde_numerical_methods}) requires the evaluation of $b$ and $\sigma$ in each time step. 
% Additionally, the evaluation of $\sigma$ requires the storage of $n^2$ entries, where $n \in \mathbb{N}$ is the number of parameters in the model.
% In \autocite{liValidityModelingSGD2021}, the authors introduce the time discrete approximation called stochastic variance amplified gradient (SVAG). This method does need evaluations of the full gradient. It is given the following recurrence relation
% \begin{equation}
%   X_{k+1} = X_k - \frac{\eta}{l} \nabla f^l(X_k)
% \end{equation}
% where $f^l$ is defined as
% \begin{equation}
%   f^l_{i,j}(x) = \frac{1+\sqrt{2l - 1}}{2}f_i(x) + \frac{1-\sqrt{2l - 1}}{2}f_j(x)
% \end{equation}
% for independently sampled $i,j$.

\section{Stochastic modified equations in deep learning}
\label{sec:smdedl}
In this section, we apply stochastic modified equations in the context of deep learning model. First, we define dense neural networks and detail the components of more advanced models like ResNet. Secondly, we discuss simulating the solution of the SDE model and introduce an efficient algorithm. Lastly, we investigate whether the SDE model holds empirically.
\subsection{Dense neural networks}
\begin{definition}[Dense neural network, weight matrix, bias, activation function, \protect{\cite[pp.~19]{aggarwalNeuralNetworksDeep2018}}]
  A \emph{dense neural network} is a concatenation of affine function interlaced with nonlinear functions. We call the matrices $W_k \in \R^{m_{k+1} \times m_k}$ \emph{weights} and the vectors $b_k \in \R^{m_{k+1} \times m_k}$ \emph{biases}, and the nonlinear functions $\sigma_k : \R \rightarrow \R$ \emph{activation functions} for $k = 1, \dots, L-1$. The mapping $M : \R^{m_1} \rightarrow \R^{m_L}$ defined by
  \begin{equation*}
      M(x;W_1,\dots,W_L,b_1,\dots,b_L) = \sigma_L(z_L),
  \end{equation*}
  where 
  \begin{equation*}
    z_{k+1} = \sigma_k(W_k z_k + b_k), \quad z_1 = x
  \end{equation*}
  for $k = 1, \dots L-1$ is called \emph{dense neural network}.
\end{definition}
To simply notation we write $w \in \R^d$ to denote the vectorization and concatenation of all parameters $W_1, \dots, W_L, b_1, \dots, b_L$, where $d = \sum_{k=1}^{L-1} m_{k+1}m_k + m_k$.
Note that a neural network can be interpreted as both a function of the input $x \in X$ and the network parameters $w \in \R^d$. In the following discussion, we focus on the optimization of the parameters $w \in \R^d$ and view the input data as fixed. Therefore, we drop $x$ from $f(x;w)$ and simply write $f(w)$.
\subsection{Activation functions}
The activation functions are nonlinear functions that are applied after each linear layer. We highlight some commonly used variants:
\begin{enumerate}
  \item \emph{Hyperbolic activation}: $\tanh : \R \rightarrow (-1,1), \quad \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
  \item \emph{Sigmoid activation}: $\sigma : \R \rightarrow (0,1), \quad \sigma(x) = \frac{1}{1+e^{-x}}$
  \item \emph{Rectified linear unit}: $\ReLU : \R \rightarrow [0,\infty), \quad \ReLU(x) = \max\{x,0\}$
\end{enumerate}
% Dense neural network 

\subsection{Simulating stochastic modified equations}
In \autoref{sec:sde_model}, we showed theoretical results that provide approximation results for SGD by SMDE. 
In \autoref{ex:quadratic_smde}, we saw that for a quadratic loss function we can derive an analytical solution to the SMDE and validated the order of convergence predicted by the theory.
However, both \autoref{thm:second_order} and \autoref{cor:first_order} require $L$-smoothness. As we have seen in the discussion in the previous section, deep learning models do not fulfill the conditions imposed by the theory. Further, the constants in the approximation equations \eqref{eq:first_order_convergence} and \eqref{eq:second_order_convergence} are not known and might be large in practice. 
Still, we want to investigate whether SMDE are a good model for deep learning objectives. To verify this we need an efficient algorithm to simulate the solutions to 
\begin{equation}
  d W_t = -\nabla f(W_t) dt + \sqrt{\eta}\Sigma(W_t)^{1/2}d\mathbf{B}_t, \quad W_0 = w_0.
\end{equation}
For an initial value $w_0 \in \R^d$ the iterates of Euler-Maruyama scheme for this equation are given by 
\begin{equation}
  w_{k+1} = w_k -\eta\nabla f(w_k) + \sqrt{\eta}\Sigma(w_k)^{1/2} \Delta \mathbf{B}_k,
\end{equation}
where $\Delta \mathbf{B}_k \sim \CN(0, \eta I_d)$ for $k=0,1,\dots$.

At this point, we make two observations. Deep learning objectives are of the form
\begin{equation*}
  f(w) = \frac{1}{n}\sum_{i=1}^n f_i(w), \quad w \in \R^d
\end{equation*}
where $n \in \N$ is large. Therefore, calculating $\nabla f$ is computationally expensive. Further, the matrix
\begin{equation*}
  \Sigma(w)= \ev{(\nabla f_{\gamma}(w) - \nabla f(w))(\nabla f_{\gamma}(w) - \nabla f(w))^\T}
\end{equation*}
has $d^2$ entries. For large models computing the square root matrix $\Sigma(w)^{\frac{1}{2}}$ via a singular value decomposition at each iteration has a cost of $\CO(d^3)$ \cite{golubMatrixComputations1996}.
Wu et al. \cite{wuNoisyGradientDescent2020a} address this problem by introducing multiplicative SGD which decouples the loss calculation from the sampling algorithm. This allows sampling $Z \sim \CN(0, \Sigma(w))$ without explicitly calculating $\Sigma(w)^{\frac{1}{2}}$.

In the following we explain the procedure to generate these samples.
Let $\pmb{f}(w) = (f_1(w), \dots, f_n(w))$ be a vector of the individual losses and let $\mathbbm{1} = (1, \dots, 1)^\T \in R^n$ denote a vector of ones. Further, let $\widetilde{S} : \Omega \rightarrow \R^n$ be a random variable  with $\widetilde{S} \sim \CN(0, \frac{1}{bn}(I_d - \frac{1}{n}\mathbbm{1}\mathbbm{1}^\T )(I_d - \frac{1}{n}\mathbbm{1}\mathbbm{1}^\T))^\T$. Then for $S = \frac{1}{n} = \mathbbm{1} + \widetilde{S}$ we have $Z = \nabla \pmb{f}(w)S \sim \CN(0, \Sigma(w))$.
Note that the sampling of $S$ does not require knowledge of the covariance matrix $\Sigma(w)$ and its square root.

Multiplicative SGD does not address the fact that at each iteration the full gradient has to be computed. Li et al.\ \cite{liValidityModelingSGD2021} propose an algorithm that amplifies the gradient noise for each iteration and converges to the SMDE equation. The key point is that each iteration only requires the gradient for two independently sampled batches. 
\begin{definition}[Stochastic variance amplified gradient, \cite{liValidityModelingSGD2021}]
  Let $f, f_{\gamma} : \R^d \rightarrow \R$ be a continuously differentiable functions with $\ev{f_{\gamma}} \equiv f$. Further, let $\bar{\gamma}_k : \Omega \rightarrow \{1,\dots,n\}^2 $ with $\bar{\gamma}_k =(\gamma_{k,1}, \gamma_{k,2})$, $k=1,\dots,N$ be a sequence of i.i.d. random variables defined on a probability space $(\Omega, \CF, \BP)$. We call the function $f^l_{\bar{\gamma}} : \R^d \rightarrow \R$ with
  \begin{equation*}
    f^l_{\bar{\gamma}}(w) =  \frac{1+\sqrt{2l - 1}}{2}f_{\gamma_{1}}(w) + \frac{1-\sqrt{2l - 1}}{2}f_{\gamma_{2}}(w), \quad w \in \R^d
  \end{equation*}
  \emph{stochastic variance amplified gradient function}.
    For a given $w_{0} \in \R^d$ the iterates defined by the recursive relation
  \begin{equation*}
    w_{k+1} = w_{k} - \eta f_{\bar{\gamma_k}}(w_{k})
  \end{equation*}
  are called \emph{stochastic variance amplified gradient} (SVAG) iterates.
\end{definition}
We see that for $w \in \R^d$ we have
\begin{align*}
  \ev{\nabla f^l_{\bar{\gamma}}(w)} &= \frac{1+\sqrt{2l - 1}}{2}\ev{\nabla f_{\gamma_{k,1}}(w)} + \frac{1-\sqrt{2l - 1}}{2}\ev{\nabla f_{\gamma_{k,2}}(w)} \\
  &= \frac{1+\sqrt{2l - 1}}{2}\nabla f(w) + \frac{1-\sqrt{2l - 1}}{2} \nabla f(w) = \nabla f(w)
\end{align*}
and
\begin{align*}
  \Cov\left[\nabla f^l_{\bar{\gamma}}(w)\right] &= \ev{\left(\nabla f^l_{\bar{\gamma}}(w) - \nabla f(w)\right)\left(\nabla f^l_{\bar{\gamma}}(w) - \nabla f(w)\right)^\T} \\
  &= l\ev{\nabla f_{\gamma}(w)\nabla f_{\gamma}(w)^\T} - l \nabla f(w) \nabla f(w)^\T = l \Sigma(w).
\end{align*}
We see that the gradients of the amplified function has the same expectation as a SGD iterate, but has a covariance matrix scaled by $l$.
The following theorem builds on the theory introduced in \autoref{sec:sde_model} and shows that the SVAG iterates approximate the SMDE solution.
\begin{theorem}[\cite{liValidityModelingSGD2021}]
  \label{thm:svag}
  Suppose the following conditions are met:
  \begin{enumerate}[label=(\roman*)]
    \item $f \equiv \ev{f_{\gamma}}$ is $C^{\infty}$-smooth, and $f \in G^4$.
    \item $\norm{\nabla f_{\gamma}(x) - \nabla f_{\gamma}(y)} \leq L_{\gamma} \norm{x - y}$, for all $x, y \in \R^d$, where $L_{\gamma}$ is a random variable with finite moments, i.e., $\ev{L_{\gamma}^k}$ is bounded for $k \in \N$.
    \item $\Sigma^{\frac{1}{2}}(x)$ is $C^{\infty}$-smooth.
  \end{enumerate}
  Let $T > 0$ be a constant and $l$ be the SVAG hyperparameter. Define $\{W_t : t \in [0,T] \}$ as the stochastic process satisfying equation \eqref{eq:first_order_sde} and $\{w_k^{\eta/l}: 1 \leq k \leq \left\lfloor Tl/\eta \right\rfloor \}$ as the trajectory of SVAG where $w_0 = W_0$. Then SVAG $\{w_k^{\eta/l}\}$ is an order-1 approximation of the SDE solution $\{W_t:t\in [0,T] \}$, i.e., for each $g \in G^4$, there exists a constant $C > 0$ independent of $l$ such that
  \begin{equation*}
    \max_{k=0,\dots, \lfloor \frac{lT}{\eta} \rfloor} \left\lvert \ev{g(w^{\eta/l}_k)} - \ev{g(W_{\frac{k\eta}{l}})} \right\rvert \leq C l^{-1}.
  \end{equation*}
\end{theorem}
\begin{proof}
  The proof can be found in \cite{liValidityModelingSGD2021}.
\end{proof}
% Notice that the conditions of \autoref{thm:svag} are 
\subsection{Limitations}
\subsection{Experiments}
\section{Applications}
\label{sec:applications}
In this section, we cover some applications of SMDE. In particular, we look at the linear scaling rule.

The linear scaling rule \cite{goyalAccurateLargeMinibatch2018, krizhevskyOneWeirdTrick2014} asserts that when increasing the batch size $B$ by a factor of $m > 0$, then the dynamics of SGD are preserved if the learning rate $\eta > 0$ is scaled by the same factor.

The iterates of mini-batch SGD are given by
\begin{equation*}
  w_{k+1} = w_k - \eta \sum_{i=1}^B \nabla f_{\gamma_{k, i}}(w_k),
\end{equation*}
where $\gamma_{k,i} : \Omega \rightarrow \{1,\dots,n\}$, $k = 1,\dots,N$, $i =1, \dots, B$ are identically distributed independent random variables.
We see that
\begin{equation*}
  \ev{\sum_{i=1}^B \nabla f_{\gamma_i}(w)} = \nabla f(w)
\end{equation*}
and 
\begin{equation*}
  \ev{\left(\sum_{i=1}^B \nabla f_{\gamma_i}(w) - \nabla f(w)\right)\left(\sum_{i=1}^B \nabla f_{\gamma_i}(w) - \nabla f(w)\right)^\T} = \frac{1}{B} \Sigma(w).
\end{equation*}
Therefore, the first order SMDE is given by
\begin{equation*}
  dW_t = -\nabla f(W_t) + \sqrt{\eta/B}\Sigma(W_t)^{1/2}d\mathbf{B}_t.
\end{equation*}
\section{Conclusion}
We can see 
% \nomenclature{\(\nabla f\)}{Gradient of a function $f$}
% \nomenclature{\(\Tr A\)}{Trace of the matrix $A \in \R^{d \times d}$, $\Tr A = \sum_{i=1}^d a_{i,i}$}
% \nomenclature{\(||\cdot||_2\)}{$||\cdot||_2 : \R^d \rightarrow \R, \quad ||x||_2 = \sqrt{\sum_{i=1}^d |x_i|^2}$ for $x \in \R^d$}
% \nomenclature{\(||\cdot||_F\)}{$||\cdot||_F : \R^{d \times d} \rightarrow \R, \quad ||A||_F = \sqrt{\sum_{i,j=1}^d |a_{i,j}|^2}$ for $A \in \R^{d \times d}$}


\printbibliography

\appendix
\section{Appendix}
\begin{lemma}
  \label{lemma:inequality}
  Let $a,b \geq 0$ be non-negative real values. Then we have
  \begin{equation}
    \label{eq:inequality}
    \left(a+b\right)^n \leq 2^{n-1} \left( a^n + b^n \right),
  \end{equation}
  for $n \in \N$.
\end{lemma}
\begin{proof}
  Let $a \geq b \geq 0$ be given without loss of generality. Then for $n = 1$ the inequality holds trivially.
  Assuming \eqref{eq:inequality} holds for $n = k-1 \geq 1$, we have
  \begin{equation*}
    (a+b)^k = (a+b)^{k-1}(a+b) \leq 2^{k-2} (a^{k-1} + b^{k-1}) (a+b) = 2^{k-1}(a^k+b^k + ab^{k-1} + a^{k-1}b).
  \end{equation*}
  We see that
  \begin{align*}
    &a^k+b^k + ab^{k-1} + a^{k-1}b \leq 2 (a^k + b^k) \\
    &\iff a^k + b^k -ab^{k-1} - a^{k-1}b \geq 0 \\
    &\iff (a^{k-1} - b^{k-1}) (a-b) \geq 0,
  \end{align*}
  which holds by assumption. Thus, we have
  \begin{equation*}
    (a+b)^k \leq 2^{k-1}(a^k+b^k + ab^{k-1} + a^{k-1}b) \leq 2^{k-1} (a^k + b^k) \qedhere
  \end{equation*}
\end{proof}
\begin{definition}[Mollifier, \protect{\cite{evansPartialDifferentialEquations2010}}]
  \label{def:mollifier}
  Let us denote by $\nu : \R^d \rightarrow \R$, $\nu \in \CC^{\infty}_c(\R^d)$ the \emph{standard mollifier}
  \begin{equation*}
    \nu(w) \defeq \begin{cases}
      C \exp\left(-\frac{1}{1 - \norm{w}^2}\right), &\norm{w} < 1 \\
      0, &\norm{w} \geq 1,
    \end{cases}
  \end{equation*}
  where $C \defeq (\int_{\R^d}\nu(w)dw)^{-1}$ is chosen so that the integral of $\nu$ is 1. Further, define $\nu^{\epsilon}(w) = \epsilon^{-d}(w/\epsilon)$. Let $\psi \in \CL^1_{\text{loc}}(\R^d)$ be locally integrable, then we may define its mollification by
  \begin{equation*}
    \psi^{\epsilon}(w) \defeq (\nu^{\epsilon}*\psi)(w) = \int_{\R^d}\nu^{\epsilon}(w-v)\psi(v)dv = \int_{\CB(0,\epsilon)}v^{\epsilon}(v)\psi(w-v)dv,
  \end{equation*}
  where $\CB(0,\epsilon)$ is the $d$-dimensional ball of radius $\epsilon$ centered at 0. 
\end{definition}
\begin{lemma}[\protect{\cite[pp. 630]{evansPartialDifferentialEquations2010}}]
  \label{lemma:mollifiers}
  Let $\psi \in \CL^1_{\text{loc}}(\R^d)$. Then, we have
  \begin{enumerate}[label=(\roman*)]
    \item $\psi^{\epsilon} \in \CC^{\infty}(\R^d)$,
    \item $\psi^{\epsilon}(w) \rightarrow \psi(w)$ as $\epsilon \rightarrow 0$ for almost every $w \in \R^d$,
    \item If $\psi$ is continuous, then $\psi^{\epsilon}(w) \rightarrow \psi(w)$ as $\epsilon \rightarrow 0$ uniformly on compact subset of $\R^d$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof can be found in \cite[pp. 630]{evansPartialDifferentialEquations2010}.
\end{proof}
\begin{lemma}[\protect{\cite[Lemma 30]{liStochasticModifiedEquations2019}}]
  \label{lemma:mollifier_bound}
  Let $\epsilon \in (0,1)$ and $\psi : \R^d \rightarrow \R$ be continuous with its weak derivative $D\psi$ belonging to $G_w$. Denote by $\psi^{\epsilon}  = \moll * \psi$ the mollification of $\psi$. Then, there exists a $K \in G$ independent of $\epsilon$ such that
  \begin{equation*}
    \lvert\psi^{\epsilon}(w) - \psi(w)\rvert \leq \epsilon K(w)
  \end{equation*}
  for all $w \in \R^d$.
\end{lemma}
\begin{proof}
  The proof can be found in 
  \cite{liStochasticModifiedEquations2019}.
\end{proof}
\begin{theorem}[Dominated convergence theorem, \protect{\cite[pp.~648]{evansPartialDifferentialEquations2010}}]
  \label{thm:dominated_convergence}
  Assume the functions $\{f_k\}_{k=1}^{\infty} \subset \CL^1(\R^d)$ are integrable and
  \begin{equation*}
    f_k \rightarrow f \text{ almost surely.}
  \end{equation*}
  Suppose
  \begin{equation*}
    |f_k| \leq g \text{ almost surely,}
  \end{equation*}
  for some integrable function $g \in \CL^1(\R^d)$. Then, we have
  \begin{equation*}
    \int_{\R^d} f_k(x) dx \rightarrow \int_{\R^d} f(x) dx.
  \end{equation*}
\end{theorem}
\begin{lemma}
  \label{lemma:linear_growth}
  Let $\mathbf{f} : \R^m \rightarrow \R^n$ be a Lipschitz continuous function. Then, $\mathbf{f}$ fulfills a linear growth condition, i.e., there exists a $C > 0$ such that 
  \begin{equation*}
    \norm{\mathbf{f}(x)}^2 \leq C (1 + \norm{x}^2).
  \end{equation*}
\end{lemma}
\begin{proof}
  Let $x \in \R^m$. Then, we have
  \begin{align*}
    \norm{\mathbf{f}(x)}^2 &= \norm{\mathbf{f}(x) - \mathbf{f}(0) + \mathbf{f}(0)}^2 \leq 2 \left( \norm{\mathbf{f}(x) - \mathbf{f}(0)}^2 + \norm{\mathbf{f}(0)}^2\right) \\
    &\leq C(1+\norm{x}),
  \end{align*}
  where $C \defeq 2\max\{L^2, \norm{\mathbf{f}(0)}^2\}$.
\end{proof}
\begin{lemma}
  \label{lem:sum_positive_definite}
  Let $A,B \in \R^{d \times d}$ be symmetric, positive definite matrices. Then, the matrix $A+B$ is positive definite.
\end{lemma}
\begin{proof}
  Let $A,B \in \R^{d \times d}$ be symmetric, positive definite matrices. For $x \in \R^d \setminus \{0\}$ we have
  \begin{equation*}
    x^\T(A+B)x = x^\T A x + x^\T B x > 0.
  \end{equation*}
  Thus, $A+B$ is positive definite.
\end{proof}
\begin{theorem}[\protect{\cite[pp.~65]{koenigsbergerAnalysis2006}}]
  \label{thm:taylor}
  Let $f : \R \rightarrow \R$ be a function with continuous derivatives at a point $c \in \R$ of up to order $r \in \N$. Then, for each $x \in \R$ there exists a $\xi \in [c,x]$ such that 
  \begin{equation*}
    f(x) = \sum_{k=0}^r f^{(k)}(c) \frac{(x-c)^k}{k!} + R_{r+1}(c,x),
  \end{equation*}
  where the \emph{remainder} is given by
  \begin{equation*}
    R_{r+1}(c,x) \defeq \frac{1}{(r+1)!}f^{(r+1)}(\xi)(x-c)^{r+1}.
  \end{equation*}
\end{theorem}
\end{document}
