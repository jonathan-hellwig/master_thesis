% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{aggarwalNeuralNetworksDeep2018}{book}{}
      \name{author}{1}{}{%
        {{hash=86d11a07b1bfeac58dc68f7a419b3039}{%
           family={Aggarwal},
           familyi={A\bibinitperiod},
           given={Charu\bibnamedelima C.},
           giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{fullhash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{bibnamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authorbibnamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authornamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authorfullhash}{86d11a07b1bfeac58dc68f7a419b3039}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories: The basics of neural networks: Many traditional machine learning models can be understood as special cases of neural networks. An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10. The book is written for graduate students, researchers, and practitioners. Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.}
      \field{day}{25}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-3-319-94463-0}
      \field{langid}{english}
      \field{month}{8}
      \field{pagetotal}{512}
      \field{shorttitle}{Neural {{Networks}} and {{Deep Learning}}}
      \field{title}{Neural {{Networks}} and {{Deep Learning}}: {{A Textbook}}}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{eprint}
      \verb achqDwAAQBAJ
      \endverb
    \endentry
    \entry{ahmadTextbookOrdinaryDifferential2015}{book}{}
      \name{author}{2}{}{%
        {{hash=d22fd59c249c8217336450b9b9b7b209}{%
           family={Ahmad},
           familyi={A\bibinitperiod},
           given={Shair},
           giveni={S\bibinitperiod}}}%
        {{hash=e70ebef8e2f3e0716a1fd5dc52aa218d}{%
           family={Ambrosetti},
           familyi={A\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{cd26d7f63a2b7eb6b2a023b7329073ee}
      \strng{fullhash}{cd26d7f63a2b7eb6b2a023b7329073ee}
      \strng{bibnamehash}{cd26d7f63a2b7eb6b2a023b7329073ee}
      \strng{authorbibnamehash}{cd26d7f63a2b7eb6b2a023b7329073ee}
      \strng{authornamehash}{cd26d7f63a2b7eb6b2a023b7329073ee}
      \strng{authorfullhash}{cd26d7f63a2b7eb6b2a023b7329073ee}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book offers readers a primer on the theory and applications of Ordinary Differential Equations. The style used is simple, yet thorough and rigorous. Each chapter ends with a broad set of exercises that range from the routine to the more challenging and thought-provoking. Solutions to selected exercises can be found at the end of the book. The book contains many interesting examples on topics such as electric circuits, the pendulum equation, the logistic equation, the Lotka-Volterra system, the Laplace Transform, etc., which introduce students to a number of interesting aspects of the theory and applications. The work is mainly intended for students of Mathematics, Physics, Engineering, Computer Science and other areas of the natural and social sciences that use ordinary differential equations, and who have a firm grasp of Calculus and a minimal understanding of the basic concepts used in Linear Algebra. It also studies a few more advanced topics, such as Stability Theory and Boundary Value Problems, which may be suitable for more advanced undergraduate or first-year graduate students. The second edition has been revised to correct minor errata, and features a number of carefully selected new exercises, together with more detailed explanations of some of the topics.A complete Solutions Manual, containing solutions to all the exercises published in the book, is available. Instructors who wish to adopt the book may request the manual by writing directly to one of the authors.}
      \field{day}{5}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-3-319-16408-3}
      \field{langid}{english}
      \field{month}{6}
      \field{pagetotal}{337}
      \field{title}{A {{Textbook}} on {{Ordinary Differential Equations}}}
      \field{year}{2015}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 1vHLCQAAQBAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/FKUTC4WQ/Ahmad_Ambrosetti_2015_A Textbook on Ordinary Differential Equations.pdf
      \endverb
    \endentry
    \entry{allen-zhuConvergenceTheoryDeep2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=eaa5cc98b7f19b4ce07c832213a13b9d}{%
           family={Allen-Zhu},
           familyi={A\bibinithyphendelim Z\bibinitperiod},
           given={Zeyuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=0d2c91b24f885b22d373a892a768abc8}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yuanzhi},
           giveni={Y\bibinitperiod}}}%
        {{hash=04d07d54d268f89c0e192345e9b152ba}{%
           family={Song},
           familyi={S\bibinitperiod},
           given={Zhao},
           giveni={Z\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{eb6758eac7d47595942aa1d38316f28e}
      \strng{fullhash}{eb6758eac7d47595942aa1d38316f28e}
      \strng{bibnamehash}{eb6758eac7d47595942aa1d38316f28e}
      \strng{authorbibnamehash}{eb6758eac7d47595942aa1d38316f28e}
      \strng{authornamehash}{eb6758eac7d47595942aa1d38316f28e}
      \strng{authorfullhash}{eb6758eac7d47595942aa1d38316f28e}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100\% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps ~ e\^\{-T\}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).}
      \field{booktitle}{Proceedings of the 36th {{International Conference}} on {{Machine Learning}}}
      \field{day}{24}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{A {{Convergence Theory}} for {{Deep Learning}} via {{Over-Parameterization}}}
      \field{urlday}{21}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{242\bibrangedash 252}
      \range{pages}{11}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/D3Y8I392/Allen-Zhu et al_2019_A Convergence Theory for Deep Learning via Over-Parameterization.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v97/allen-zhu19a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v97/allen-zhu19a.html
      \endverb
    \endentry
    \entry{barrettImplicitGradientRegularization2021}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=a1b1f461686916d4b7095bdc0b046916}{%
           family={Barrett},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=1a44295013e733b94c180644a828598a}{%
           family={Dherin},
           familyi={D\bibinitperiod},
           given={Benoit},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{590819a5e4a1c315c2a94348927691f1}
      \strng{fullhash}{590819a5e4a1c315c2a94348927691f1}
      \strng{bibnamehash}{590819a5e4a1c315c2a94348927691f1}
      \strng{authorbibnamehash}{590819a5e4a1c315c2a94348927691f1}
      \strng{authornamehash}{590819a5e4a1c315c2a94348927691f1}
      \strng{authorfullhash}{590819a5e4a1c315c2a94348927691f1}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly regularize models by penalizing gradient descent trajectories that have large loss gradients. We call this Implicit Gradient Regularization (IGR) and we use backward error analysis to calculate the size of this regularization. We confirm empirically that implicit gradient regularization biases gradient descent toward flat minima, where test errors are small and solutions are robust to noisy parameter perturbations. Furthermore, we demonstrate that the implicit gradient regularization term can be used as an explicit regularizer, allowing us to control this gradient regularization directly. More broadly, our work indicates that backward error analysis is a useful theoretical approach to the perennial question of how learning rate, model size, and parameter regularization interact to determine the properties of overparameterized models optimized with gradient descent.}
      \field{day}{11}
      \field{eventtitle}{International {{Conference}} on {{Learning Representations}}}
      \field{langid}{english}
      \field{month}{3}
      \field{title}{Implicit {{Gradient Regularization}}}
      \field{urlday}{5}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/6KI7QFEE/Barrett_Dherin_2021_Implicit Gradient Regularization.pdf;/home/jonathan/Zotero/storage/FR4HFHZ5/forum.html
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=3q5IqUrkcF
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=3q5IqUrkcF
      \endverb
    \endentry
    \entry{bottouOptimizationMethodsLargeScale2018}{article}{}
      \name{author}{3}{}{%
        {{hash=ac762f2592005edf6bad58f566967c6c}{%
           family={Bottou},
           familyi={B\bibinitperiod},
           given={Léon},
           giveni={L\bibinitperiod}}}%
        {{hash=70bd503fe79d5024977023f0d0f2e6dd}{%
           family={Curtis},
           familyi={C\bibinitperiod},
           given={Frank\bibnamedelima E.},
           giveni={F\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=88a342d927bf795b0d92af8a5613da31}{%
           family={Nocedal},
           familyi={N\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Society for Industrial and Applied Mathematics}%
      }
      \strng{namehash}{e1edfb395f54bf5da0ff4732b4f64379}
      \strng{fullhash}{e1edfb395f54bf5da0ff4732b4f64379}
      \strng{bibnamehash}{e1edfb395f54bf5da0ff4732b4f64379}
      \strng{authorbibnamehash}{e1edfb395f54bf5da0ff4732b4f64379}
      \strng{authornamehash}{e1edfb395f54bf5da0ff4732b4f64379}
      \strng{authorfullhash}{e1edfb395f54bf5da0ff4732b4f64379}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.}
      \field{issn}{0036-1445}
      \field{journaltitle}{SIAM Review}
      \field{month}{1}
      \field{number}{2}
      \field{shortjournal}{SIAM Rev.}
      \field{title}{Optimization {{Methods}} for {{Large-Scale Machine Learning}}}
      \field{urlday}{5}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{volume}{60}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{223\bibrangedash 311}
      \range{pages}{89}
      \verb{doi}
      \verb 10.1137/16M1080173
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/HVTDRSTJ/Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://epubs.siam.org/doi/10.1137/16M1080173
      \endverb
      \verb{url}
      \verb https://epubs.siam.org/doi/10.1137/16M1080173
      \endverb
      \keyw{convergence,SGD,theory,thesis,toread}
    \endentry
    \entry{boydConvexOptimization2004}{book}{}
      \name{author}{2}{}{%
        {{hash=ec94d94cc487dd71939a90cbeaaf47d0}{%
           family={Boyd},
           familyi={B\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=2608d599a0bc2a784c301329a8da5f7b}{%
           family={Vandenberghe},
           familyi={V\bibinitperiod},
           given={Lieven},
           giveni={L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{fullhash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{bibnamehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{authorbibnamehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{authornamehash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \strng{authorfullhash}{436abed3bce8d1b1d30640d4e5ff3d58}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.}
      \field{day}{8}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-1-107-39400-1}
      \field{langid}{english}
      \field{month}{3}
      \field{pagetotal}{744}
      \field{title}{Convex {{Optimization}}}
      \field{year}{2004}
      \field{dateera}{ce}
      \verb{eprint}
      \verb IUZdAAAAQBAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/I8Z26ZZ5/Boyd_Vandenberghe_2004_Convex Optimization.pdf
      \endverb
      \keyw{background,optimization,theory,thesis}
    \endentry
    \entry{brownLanguageModelsAre2020}{misc}{}
      \name{author}{31}{}{%
        {{hash=7c16e8d36475e58d4f2d161e6ecb7704}{%
           family={Brown},
           familyi={B\bibinitperiod},
           given={Tom\bibnamedelima B.},
           giveni={T\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=d4543c3bcd6aaf414da4296b349603e5}{%
           family={Mann},
           familyi={M\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=9ac58bd43db2434e8e1ffd6182c3fcda}{%
           family={Ryder},
           familyi={R\bibinitperiod},
           given={Nick},
           giveni={N\bibinitperiod}}}%
        {{hash=9ed243177743da3e650f1cff6376bb3c}{%
           family={Subbiah},
           familyi={S\bibinitperiod},
           given={Melanie},
           giveni={M\bibinitperiod}}}%
        {{hash=9e85370e215d552f72c95a2b63a37802}{%
           family={Kaplan},
           familyi={K\bibinitperiod},
           given={Jared},
           giveni={J\bibinitperiod}}}%
        {{hash=4164e43d8cf919f5e3f8d80f5ea23f36}{%
           family={Dhariwal},
           familyi={D\bibinitperiod},
           given={Prafulla},
           giveni={P\bibinitperiod}}}%
        {{hash=4ca421ceeb5b516dd8fc64bea5a23f2a}{%
           family={Neelakantan},
           familyi={N\bibinitperiod},
           given={Arvind},
           giveni={A\bibinitperiod}}}%
        {{hash=ab5d7d7b9cfeaad635c4a60e8950d7dd}{%
           family={Shyam},
           familyi={S\bibinitperiod},
           given={Pranav},
           giveni={P\bibinitperiod}}}%
        {{hash=3c1d9a663596faaf544c1a65aac581be}{%
           family={Sastry},
           familyi={S\bibinitperiod},
           given={Girish},
           giveni={G\bibinitperiod}}}%
        {{hash=1e84eff933be9f4887bf369cf181bf12}{%
           family={Askell},
           familyi={A\bibinitperiod},
           given={Amanda},
           giveni={A\bibinitperiod}}}%
        {{hash=abe4801e322e893b23785fd6d0800b5c}{%
           family={Agarwal},
           familyi={A\bibinitperiod},
           given={Sandhini},
           giveni={S\bibinitperiod}}}%
        {{hash=787b9715a98ea66a8d5e6bae042ae0b9}{%
           family={Herbert-Voss},
           familyi={H\bibinithyphendelim V\bibinitperiod},
           given={Ariel},
           giveni={A\bibinitperiod}}}%
        {{hash=c3a5cc5e520e0d1a9f8bbf377c74cd27}{%
           family={Krueger},
           familyi={K\bibinitperiod},
           given={Gretchen},
           giveni={G\bibinitperiod}}}%
        {{hash=eb1d3044b466619459c76843f0e98bb9}{%
           family={Henighan},
           familyi={H\bibinitperiod},
           given={Tom},
           giveni={T\bibinitperiod}}}%
        {{hash=5c2f5c2e6d4a9ec8681377f8a8e5e6af}{%
           family={Child},
           familyi={C\bibinitperiod},
           given={Rewon},
           giveni={R\bibinitperiod}}}%
        {{hash=82063a12702e7b2c026ae0ff03b8f102}{%
           family={Ramesh},
           familyi={R\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod}}}%
        {{hash=18becbb5ea500211a8219e94dd0a975c}{%
           family={Ziegler},
           familyi={Z\bibinitperiod},
           given={Daniel\bibnamedelima M.},
           giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=495187f3a2c93ddb8083bd18a5702527}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
        {{hash=fdaff50f84c08440dcb5fad75b559780}{%
           family={Winter},
           familyi={W\bibinitperiod},
           given={Clemens},
           giveni={C\bibinitperiod}}}%
        {{hash=68a04c5006dbbf98f7719709540c6b56}{%
           family={Hesse},
           familyi={H\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=fb15a691583ec94aafe0be6e7da4878f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
        {{hash=2a5f009f12a0567430729b5ace41435d}{%
           family={Sigler},
           familyi={S\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=e5aa3a709cbe706efba113bec9789364}{%
           family={Litwin},
           familyi={L\bibinitperiod},
           given={Mateusz},
           giveni={M\bibinitperiod}}}%
        {{hash=7006ca8c1ce969019b89de50fece60dd}{%
           family={Gray},
           familyi={G\bibinitperiod},
           given={Scott},
           giveni={S\bibinitperiod}}}%
        {{hash=01f70651539bbd7dccc01e86ed9c78c3}{%
           family={Chess},
           familyi={C\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
        {{hash=1480c861b1a73e1d1de1b227e985b179}{%
           family={Clark},
           familyi={C\bibinitperiod},
           given={Jack},
           giveni={J\bibinitperiod}}}%
        {{hash=ca86811e7a0582a9e7cb8d33e7ab445d}{%
           family={Berner},
           familyi={B\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=b51e7c5fe92844f39ce52b8a5fa5675f}{%
           family={McCandlish},
           familyi={M\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
        {{hash=a812c46caad94fc8701be37871f303ba}{%
           family={Radford},
           familyi={R\bibinitperiod},
           given={Alec},
           giveni={A\bibinitperiod}}}%
        {{hash=8d569d1d5b8b5a7836017a98b430f959}{%
           family={Sutskever},
           familyi={S\bibinitperiod},
           given={Ilya},
           giveni={I\bibinitperiod}}}%
        {{hash=1e6adbf36ab730cd5fdadb838b4d2667}{%
           family={Amodei},
           familyi={A\bibinitperiod},
           given={Dario},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{0884233a667161739ddd01569813457f}
      \strng{fullhash}{b108601e4cbc36ef53b96ac01974e069}
      \strng{bibnamehash}{0884233a667161739ddd01569813457f}
      \strng{authorbibnamehash}{0884233a667161739ddd01569813457f}
      \strng{authornamehash}{0884233a667161739ddd01569813457f}
      \strng{authorfullhash}{b108601e4cbc36ef53b96ac01974e069}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.}
      \field{day}{22}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{7}
      \field{number}{arXiv:2005.14165}
      \field{title}{Language {{Models}} Are {{Few-Shot Learners}}}
      \field{urlday}{12}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{version}{4}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2005.14165
      \endverb
      \verb{eprint}
      \verb 2005.14165
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/ZFJVLUBD/Brown et al_2020_Language Models are Few-Shot Learners.pdf;/home/jonathan/Zotero/storage/Q6TQ82US/2005.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2005.14165
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2005.14165
      \endverb
      \keyw{motivation,thesis}
    \endentry
    \entry{dengImageNetLargescaleHierarchical2009}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=0ae7fdc13773f928525f673b05f37149}{%
           family={Deng},
           familyi={D\bibinitperiod},
           given={Jia},
           giveni={J\bibinitperiod}}}%
        {{hash=7d87c5957b07153c7f18918b92830bf8}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=2afdae52015b97674d81efea449edce2}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Li-Jia},
           giveni={L\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=4838f7fdd28d5cefb28f3b3c734976d4}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=cd00ce5bc45f687c432e52e0fa1a7aa6}{%
           family={Fei-Fei},
           familyi={F\bibinithyphendelim F\bibinitperiod},
           given={Li},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{990420f755e01028377fcad1464c9706}
      \strng{fullhash}{a16fdd05c52c264b99fe98f4a5e24c60}
      \strng{bibnamehash}{990420f755e01028377fcad1464c9706}
      \strng{authorbibnamehash}{990420f755e01028377fcad1464c9706}
      \strng{authornamehash}{990420f755e01028377fcad1464c9706}
      \strng{authorfullhash}{a16fdd05c52c264b99fe98f4a5e24c60}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.}
      \field{booktitle}{2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
      \field{eventtitle}{2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}
      \field{issn}{1063-6919}
      \field{month}{6}
      \field{shorttitle}{{{ImageNet}}}
      \field{title}{{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database}
      \field{year}{2009}
      \field{dateera}{ce}
      \field{pages}{248\bibrangedash 255}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/CVPR.2009.5206848
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/6JXMR36I/Deng et al_2009_ImageNet.pdf;/home/jonathan/Zotero/storage/C34WFZS9/5206848.html
      \endverb
    \endentry
    \entry{duchiAdaptiveSubgradientMethods2011}{article}{}
      \name{author}{3}{}{%
        {{hash=b8fbef1897da5bf46822ced31bc865c6}{%
           family={Duchi},
           familyi={D\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=0b3d32703edd2b7248e8e8f33c5893db}{%
           family={Hazan},
           familyi={H\bibinitperiod},
           given={Elad},
           giveni={E\bibinitperiod}}}%
        {{hash=300d4990e626d975e0c28630444f63c3}{%
           family={Singer},
           familyi={S\bibinitperiod},
           given={Yoram},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{fullhash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{bibnamehash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{authorbibnamehash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{authornamehash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{authorfullhash}{21039f1d6e97f13dbd40c187a8131cf0}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{61}
      \field{title}{Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}}
      \field{urlday}{4}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{volume}{12}
      \field{year}{2011}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2121\bibrangedash 2159}
      \range{pages}{39}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/E9ZRV7U6/Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf
      \endverb
      \verb{urlraw}
      \verb http://jmlr.org/papers/v12/duchi11a.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v12/duchi11a.html
      \endverb
    \endentry
    \entry{durrettProbabilityTheoryExamples2019}{book}{}
      \name{author}{1}{}{%
        {{hash=175a15e34b656dab74bea645e2ed84d0}{%
           family={Durrett},
           familyi={D\bibinitperiod},
           given={Rick},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{175a15e34b656dab74bea645e2ed84d0}
      \strng{fullhash}{175a15e34b656dab74bea645e2ed84d0}
      \strng{bibnamehash}{175a15e34b656dab74bea645e2ed84d0}
      \strng{authorbibnamehash}{175a15e34b656dab74bea645e2ed84d0}
      \strng{authornamehash}{175a15e34b656dab74bea645e2ed84d0}
      \strng{authorfullhash}{175a15e34b656dab74bea645e2ed84d0}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This lively introduction to measure-theoretic probability theory covers laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. Concentrating on results that are the most useful for applications, this comprehensive treatment is a rigorous graduate text and reference. Operating under the philosophy that the best way to learn probability is to see it in action, the book contains extended examples that apply the theory to concrete applications. This fifth edition contains a new chapter on multidimensional Brownian motion and its relationship to partial differential equations (PDEs), an advanced topic that is finding new applications. Setting the foundation for this expansion, Chapter 7 now features a proof of Itô's formula. Key exercises that previously were simply proofs left to the reader have been directly inserted into the text as lemmas. The new edition re-instates discussion about the central limit theorem for martingales and stationary sequences.}
      \field{day}{18}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-1-108-47368-2}
      \field{langid}{english}
      \field{month}{4}
      \field{pagetotal}{433}
      \field{shorttitle}{Probability}
      \field{title}{Probability: {{Theory}} and {{Examples}}}
      \field{year}{2019}
      \field{dateera}{ce}
      \verb{eprint}
      \verb b22MDwAAQBAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/UKLZ9CJD/Durrett_2019_Probability.pdf
      \endverb
      \keyw{background}
    \endentry
    \entry{eAppliedStochasticAnalysis2021}{book}{}
      \name{author}{3}{}{%
        {{hash=268e9715905f2ab3cb20df636d3750c1}{%
           family={E},
           familyi={E\bibinitperiod},
           given={Weinan},
           giveni={W\bibinitperiod}}}%
        {{hash=c0cb1a20ae7697536af40c5d0af48095}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Tiejun},
           giveni={T\bibinitperiod}}}%
        {{hash=3e3d82a14a02cda960c2d71e77f5453c}{%
           family={Vanden-Eijnden},
           familyi={V\bibinithyphendelim E\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {American Mathematical Soc.}%
      }
      \strng{namehash}{495826976eada715a888e7719521b72a}
      \strng{fullhash}{495826976eada715a888e7719521b72a}
      \strng{bibnamehash}{495826976eada715a888e7719521b72a}
      \strng{authorbibnamehash}{495826976eada715a888e7719521b72a}
      \strng{authornamehash}{495826976eada715a888e7719521b72a}
      \strng{authorfullhash}{495826976eada715a888e7719521b72a}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This is a textbook for advanced undergraduate students and beginning graduate students in applied mathematics. It presents the basic mathematical foundations of stochastic analysis (probability theory and stochastic processes) as well as some important practical tools and applications (e.g., the connection with differential equations, numerical methods, path integrals, random fields, statistical physics, chemical kinetics, and rare events). The book strikes a nice balance between mathematical formalism and intuitive arguments, a style that is most suited for applied mathematicians. Readers can learn both the rigorous treatment of stochastic analysis as well as practical applications in modeling and simulation. Numerous exercises nicely supplement the main exposition.}
      \field{day}{22}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-1-4704-6569-8}
      \field{langid}{english}
      \field{month}{9}
      \field{pagetotal}{329}
      \field{title}{Applied {{Stochastic Analysis}}}
      \field{year}{2021}
      \field{dateera}{ce}
      \verb{eprint}
      \verb YVpQEAAAQBAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/G3DGXFTY/E et al. - 2021 - Applied Stochastic Analysis.pdf
      \endverb
      \keyw{background,thesis}
    \endentry
    \entry{evansPartialDifferentialEquations2010}{book}{}
      \name{author}{1}{}{%
        {{hash=0d72932a7775f85a51622d3af75ae26e}{%
           family={Evans},
           familyi={E\bibinitperiod},
           given={Lawrence\bibnamedelima C.},
           giveni={L\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {American Mathematical Soc.}%
      }
      \strng{namehash}{0d72932a7775f85a51622d3af75ae26e}
      \strng{fullhash}{0d72932a7775f85a51622d3af75ae26e}
      \strng{bibnamehash}{0d72932a7775f85a51622d3af75ae26e}
      \strng{authorbibnamehash}{0d72932a7775f85a51622d3af75ae26e}
      \strng{authornamehash}{0d72932a7775f85a51622d3af75ae26e}
      \strng{authorfullhash}{0d72932a7775f85a51622d3af75ae26e}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, a significantly expanded bibliography. About the First Edition: I have used this book for both regular PDE and topics courses. It has a wonderful combination of insight and technical detail...Evans' book is evidence of his mastering of the field and the clarity of presentation (Luis Caffarelli, University of Texas) It is fun to teach from Evans' book. It explains many of the essential ideas and techniques of partial differential equations ...Every graduate student in analysis should read it. (David Jerison, MIT) I use Partial Differential Equations to prepare my students for their Topic exam, which is a requirement before starting working on their dissertation. The book provides an excellent account of PDE's ...I am very happy with the preparation it provides my students. (Carlos Kenig, University of Chicago) Evans' book has already attained the status of a classic. It is a clear choice for students just learning the subject, as well as for experts who wish to broaden their knowledge ...An outstanding reference for many aspects of the field. (Rafe Mazzeo, Stanford University.}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-0-8218-4974-3}
      \field{langid}{english}
      \field{pagetotal}{778}
      \field{title}{Partial {{Differential Equations}}}
      \field{year}{2010}
      \field{dateera}{ce}
      \verb{eprint}
      \verb Xnu0o_EJrCQC
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/LLUR3PVZ/Evans_2010_Partial Differential Equations.pdf
      \endverb
      \keyw{book,pde}
    \endentry
    \entry{freidlinRandomPerturbationsDynamical1998}{book}{}
      \name{author}{4}{}{%
        {{hash=83fd6366b4bc2f77445fff5a26abccc3}{%
           family={Freidlin},
           familyi={F\bibinitperiod},
           given={Mark\bibnamedelima I.},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
        {{hash=bec7e5d9fc4e1326faa89033ad297808}{%
           family={Freĭdlin},
           familyi={F\bibinitperiod},
           given={Mark\bibnamedelima Iosifovich},
           giveni={M\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
        {{hash=3ade2f3d731eec89410504a5e861f74e}{%
           family={Ventcelʹ},
           familyi={V\bibinitperiod},
           given={Aleksandr\bibnamedelima D.},
           giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=9eb16a0050d91fcc10dfbb33924c3af2}{%
           family={Wentzell},
           familyi={W\bibinitperiod},
           given={Alexander\bibnamedelima D.},
           giveni={A\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Science \& Business Media}%
      }
      \strng{namehash}{56f92c027795b51f358d7828d250ede7}
      \strng{fullhash}{e30be5b64574e41122666bb75ebb9b5e}
      \strng{bibnamehash}{56f92c027795b51f358d7828d250ede7}
      \strng{authorbibnamehash}{56f92c027795b51f358d7828d250ede7}
      \strng{authornamehash}{56f92c027795b51f358d7828d250ede7}
      \strng{authorfullhash}{e30be5b64574e41122666bb75ebb9b5e}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This text is a treatment of various kinds of limit theorems for stochastic processes defined as a result of random perturbations of dynamical systems. Apart from the long-time behaviour of the perturbed system, exit problems, metastable states, optimal stabilization, and asymptotics of stationary distributions are considered in detail. The author's main tools are the large deviation theory, the central limit theorem for stochastic processes, and the averaging principle. The results allow for explicit calculations of the asymptotics of many interesting characteristics of the perturbed system, and most of these results are closely conncected with PDE. This second edition contains expansions on the averaging principle, a new chapter on random perturbations of Hamiltonian systems, along with results on fast oscillating perturbations of systems with conservations laws. New sections on wave front propagation in semilinear PDE and on random perturbations of certain infinite-dimensional dynamical systems have been incorporated into the chapter on sharpenings and generalizations.}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-0-387-98362-2}
      \field{langid}{english}
      \field{pagetotal}{448}
      \field{title}{Random {{Perturbations}} of {{Dynamical Systems}}}
      \field{year}{1998}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 0yE74YEXpWEC
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/4N5CI6SI/Freidlin et al_1998_Random Perturbations of Dynamical Systems.pdf
      \endverb
    \endentry
    \entry{geoffreyhintonnitishsrivastavaandkevinswer-NeuralNetworksMachine2012}{unpublished}{}
      \name{author}{2}{}{%
        {{hash=a395cf3624916c366143a927e03a72ee}{%
           family={{Geoffrey Hinton, Nitish Srivastava, and Kevin Swer-}},
           familyi={G\bibinitperiod}}}%
        {{hash=900bc885d7553375aec470198a9514f3}{%
           family={{sky}},
           familyi={s\bibinitperiod}}}%
      }
      \strng{namehash}{0f207c68000b1a16c2173e07947ab6c7}
      \strng{fullhash}{0f207c68000b1a16c2173e07947ab6c7}
      \strng{bibnamehash}{0f207c68000b1a16c2173e07947ab6c7}
      \strng{authorbibnamehash}{0f207c68000b1a16c2173e07947ab6c7}
      \strng{authornamehash}{0f207c68000b1a16c2173e07947ab6c7}
      \strng{authorfullhash}{0f207c68000b1a16c2173e07947ab6c7}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Neural Networks for Machine Learning Lecture 6a Overview of Mini-Batch Gradient Descent}
      \field{year}{2012}
      \field{dateera}{ce}
    \endentry
    \entry{glorotUnderstandingDifficultyTraining2010}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=0b3943c3bfdbb5867b3760f7c7d488c2}{%
           family={Glorot},
           familyi={G\bibinitperiod},
           given={Xavier},
           giveni={X\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {JMLR Workshop and Conference Proceedings}%
      }
      \strng{namehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{fullhash}{02af15243279c938a0a5ca766835bcd4}
      \strng{bibnamehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{authorbibnamehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{authornamehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{authorfullhash}{02af15243279c938a0a5ca766835bcd4}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
      \field{booktitle}{Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{day}{31}
      \field{eventtitle}{Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{issn}{1938-7228}
      \field{langid}{english}
      \field{month}{3}
      \field{title}{Understanding the Difficulty of Training Deep Feedforward Neural Networks}
      \field{urlday}{4}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{249\bibrangedash 256}
      \range{pages}{8}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/587IPNDT/Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v9/glorot10a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v9/glorot10a.html
      \endverb
    \endentry
    \entry{gowerSGDGeneralAnalysis2019}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=b8950914a990e888e3eb856ff4edf20b}{%
           family={Gower},
           familyi={G\bibinitperiod},
           given={Robert\bibnamedelima Mansel},
           giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=086c9622489669fe60b7a4364d1e8a7c}{%
           family={Loizou},
           familyi={L\bibinitperiod},
           given={Nicolas},
           giveni={N\bibinitperiod}}}%
        {{hash=ef0d49292bf94cd9765f8db932e59522}{%
           family={Qian},
           familyi={Q\bibinitperiod},
           given={Xun},
           giveni={X\bibinitperiod}}}%
        {{hash=b8083cc5779ee9892f1e8bb6d7c37f07}{%
           family={Sailanbayev},
           familyi={S\bibinitperiod},
           given={Alibek},
           giveni={A\bibinitperiod}}}%
        {{hash=db2bb4d75c559da2eb3acb8f376d94b8}{%
           family={Shulgin},
           familyi={S\bibinitperiod},
           given={Egor},
           giveni={E\bibinitperiod}}}%
        {{hash=972f2a6c228c82719a54368d49838fd7}{%
           family={Richtárik},
           familyi={R\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{1260d18a028fdf9f30614524219bcb8c}
      \strng{fullhash}{fca2713e200adc6b153b9a62639d1f88}
      \strng{bibnamehash}{1260d18a028fdf9f30614524219bcb8c}
      \strng{authorbibnamehash}{1260d18a028fdf9f30614524219bcb8c}
      \strng{authornamehash}{1260d18a028fdf9f30614524219bcb8c}
      \strng{authorfullhash}{fca2713e200adc6b153b9a62639d1f88}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form minibatches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.}
      \field{booktitle}{Proceedings of the 36th {{International Conference}} on {{Machine Learning}}}
      \field{day}{24}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{5}
      \field{shorttitle}{{{SGD}}}
      \field{title}{{{SGD}}: {{General Analysis}} and {{Improved Rates}}}
      \field{urlday}{5}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{5200\bibrangedash 5209}
      \range{pages}{10}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/5JL58NLC/Gower et al. - 2019 - SGD General Analysis and Improved Rates.pdf;/home/jonathan/Zotero/storage/9QVY34YG/Gower et al_2019_SGD.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v97/qian19b.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v97/qian19b.html
      \endverb
      \keyw{convergence,SGD,theory,thesis,toread}
    \endentry
    \entry{goyalAccurateLargeMinibatch2018}{misc}{}
      \name{author}{9}{}{%
        {{hash=d87be7c19fa16049347907c9820816c6}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Priya},
           giveni={P\bibinitperiod}}}%
        {{hash=ecd149fdcb3e0503881d49e545744c3d}{%
           family={Dollár},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=c3ca2ee41cf25a9ed55b70a29aca9c9a}{%
           family={Noordhuis},
           familyi={N\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=fe2c91d7f7f1eb9fba5b4b2349a02fbb}{%
           family={Wesolowski},
           familyi={W\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=fb0c5ef5bd0623316e6c5b00f3202e24}{%
           family={Kyrola},
           familyi={K\bibinitperiod},
           given={Aapo},
           giveni={A\bibinitperiod}}}%
        {{hash=ead37c072da8d4bf054810a1c3011177}{%
           family={Tulloch},
           familyi={T\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=9fce03efe6b3331a1b93ed2e7c0da9d5}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Yangqing},
           giveni={Y\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{fullhash}{ffe8205ee34aca8734032bf6b02a24f1}
      \strng{bibnamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authorbibnamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authornamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authorfullhash}{ffe8205ee34aca8734032bf6b02a24f1}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.}
      \field{day}{30}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{4}
      \field{number}{arXiv:1706.02677}
      \field{shorttitle}{Accurate, {{Large Minibatch SGD}}}
      \field{title}{Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}}
      \field{urlday}{4}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1706.02677
      \endverb
      \verb{eprint}
      \verb 1706.02677
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/777HVGS6/Goyal et al_2018_Accurate, Large Minibatch SGD.pdf;/home/jonathan/Zotero/storage/RMACCGNI/1706.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.02677
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.02677
      \endverb
    \endentry
    \entry{hairerGeometricNumericalIntegration2013}{book}{}
      \name{author}{3}{}{%
        {{hash=83e9309b497414bd7c7550110d37b3d7}{%
           family={Hairer},
           familyi={H\bibinitperiod},
           given={Ernst},
           giveni={E\bibinitperiod}}}%
        {{hash=9a61cdcbc8a5b325ba3aa8e80f7d1900}{%
           family={Lubich},
           familyi={L\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=6715424cc951a02c09b3082a661642ab}{%
           family={Wanner},
           familyi={W\bibinitperiod},
           given={Gerhard},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Science \& Business Media}%
      }
      \strng{namehash}{6b65ffa33296e7c8499bc72738749672}
      \strng{fullhash}{6b65ffa33296e7c8499bc72738749672}
      \strng{bibnamehash}{6b65ffa33296e7c8499bc72738749672}
      \strng{authorbibnamehash}{6b65ffa33296e7c8499bc72738749672}
      \strng{authornamehash}{6b65ffa33296e7c8499bc72738749672}
      \strng{authorfullhash}{6b65ffa33296e7c8499bc72738749672}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Numerical methods that preserve properties of Hamiltonian systems, reversible systems, differential equations on manifolds and problems with highly oscillatory solutions are the subject of this book. A complete self-contained theory of symplectic and symmetric methods, which include Runge-Kutta, composition, splitting, multistep and various specially designed integrators, is presented and their construction and practical merits are discussed. The long-time behaviour of the numerical solutions is studied using a backward error analysis (modified equations) combined with KAM theory. The book is illustrated by many figures, it treats applications from physics and astronomy and contains many numerical experiments and comparisons of different approaches.}
      \field{day}{9}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-3-662-05018-7}
      \field{langid}{english}
      \field{month}{3}
      \field{pagetotal}{526}
      \field{shorttitle}{Geometric {{Numerical Integration}}}
      \field{title}{Geometric {{Numerical Integration}}: {{Structure-Preserving Algorithms}} for {{Ordinary Differential Equations}}}
      \field{year}{2013}
      \field{dateera}{ce}
      \verb{eprint}
      \verb cPTxCAAAQBAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/U37UJ7I9/Hairer et al_2013_Geometric Numerical Integration.pdf
      \endverb
    \endentry
    \entry{heDelvingDeepRectifiers2015a}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
        {{hash=5e72bc22dbcf0984c6d113d280e36990}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Xiangyu},
           giveni={X\bibinitperiod}}}%
        {{hash=bb295293acacd54387339079ebbe4ead}{%
           family={Ren},
           familyi={R\bibinitperiod},
           given={Shaoqing},
           giveni={S\bibinitperiod}}}%
        {{hash=f85751488058842b5777c7b4074077b5}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{fullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \strng{bibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorbibnamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authornamehash}{6edb98fe38401d2fe4a026f5ce6e8451}
      \strng{authorfullhash}{42c4b52dc3a62cebabbc11c73e1afb53}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.}
      \field{booktitle}{2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})}
      \field{eventtitle}{2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})}
      \field{issn}{2380-7504}
      \field{month}{12}
      \field{shorttitle}{Delving {{Deep}} into {{Rectifiers}}}
      \field{title}{Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{pages}{1026\bibrangedash 1034}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/ICCV.2015.123
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/XUUPBNZ2/He et al_2015_Delving Deep into Rectifiers.pdf;/home/jonathan/Zotero/storage/QM8P6TCZ/7410480.html
      \endverb
    \endentry
    \entry{jumperHighlyAccurateProtein2021}{article}{}
      \name{author}{34}{}{%
        {{hash=10a7a824028a842a17d901b4ec09993d}{%
           family={Jumper},
           familyi={J\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=173072a12c87679d6e0aeed7dbfb2abc}{%
           family={Evans},
           familyi={E\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=4295e4094c426f01903ac60155866130}{%
           family={Pritzel},
           familyi={P\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=3e0f493c1ce741283206bf1f0d3c12f5}{%
           family={Green},
           familyi={G\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=79e7b7b6d4522585cad02503c7d5c53c}{%
           family={Figurnov},
           familyi={F\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=8e46da9de9e53ea5d37089897d69cdd9}{%
           family={Ronneberger},
           familyi={R\bibinitperiod},
           given={Olaf},
           giveni={O\bibinitperiod}}}%
        {{hash=c65f7c717e43948afa4c164334478ff0}{%
           family={Tunyasuvunakool},
           familyi={T\bibinitperiod},
           given={Kathryn},
           giveni={K\bibinitperiod}}}%
        {{hash=8fdcc93ec2f81694fe0e05f9515307f0}{%
           family={Bates},
           familyi={B\bibinitperiod},
           given={Russ},
           giveni={R\bibinitperiod}}}%
        {{hash=314115f4f8838d7bcb17334f981615d0}{%
           family={Žídek},
           familyi={Ž\bibinitperiod},
           given={Augustin},
           giveni={A\bibinitperiod}}}%
        {{hash=4b3acabda3fd87e1abd111ed903b2d51}{%
           family={Potapenko},
           familyi={P\bibinitperiod},
           given={Anna},
           giveni={A\bibinitperiod}}}%
        {{hash=9bb6bd16bbcdb2bb998f8bc4038abede}{%
           family={Bridgland},
           familyi={B\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=10928a06d1ffe00108ee7f051eef29a6}{%
           family={Meyer},
           familyi={M\bibinitperiod},
           given={Clemens},
           giveni={C\bibinitperiod}}}%
        {{hash=c8db325f01e156e511c9a87a5e890d22}{%
           family={Kohl},
           familyi={K\bibinitperiod},
           given={Simon\bibnamedelimb A.\bibnamedelimi A.},
           giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=8e2b07cf777b4e100518006a0306d430}{%
           family={Ballard},
           familyi={B\bibinitperiod},
           given={Andrew\bibnamedelima J.},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=699f0d8b3b8c59a0bf815183b5a587d1}{%
           family={Cowie},
           familyi={C\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=a4ad88ba00428c727fd7a7295f097c6d}{%
           family={Romera-Paredes},
           familyi={R\bibinithyphendelim P\bibinitperiod},
           given={Bernardino},
           giveni={B\bibinitperiod}}}%
        {{hash=948ac64053194ba4926e57c400b43dae}{%
           family={Nikolov},
           familyi={N\bibinitperiod},
           given={Stanislav},
           giveni={S\bibinitperiod}}}%
        {{hash=e44ef077603c4c2159908f44186dedfa}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Rishub},
           giveni={R\bibinitperiod}}}%
        {{hash=30e117447522ea5c589b531025c81621}{%
           family={Adler},
           familyi={A\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod}}}%
        {{hash=329e447b0059cd0bc92227b1dd08c81e}{%
           family={Back},
           familyi={B\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
        {{hash=4e381e44037009b1cd834d794735c311}{%
           family={Petersen},
           familyi={P\bibinitperiod},
           given={Stig},
           giveni={S\bibinitperiod}}}%
        {{hash=d805812e442bf9b99ef9b60be3389113}{%
           family={Reiman},
           familyi={R\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=19819a31d99ee8ba869ec9b7fee14613}{%
           family={Clancy},
           familyi={C\bibinitperiod},
           given={Ellen},
           giveni={E\bibinitperiod}}}%
        {{hash=00bb8dcd0d7dc6963e2b9481ce8e7cdf}{%
           family={Zielinski},
           familyi={Z\bibinitperiod},
           given={Michal},
           giveni={M\bibinitperiod}}}%
        {{hash=c06ac6a6fdcf2ee2092d61fc58c1f21b}{%
           family={Steinegger},
           familyi={S\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=d33020bce7d227c8dd58677580d8df0f}{%
           family={Pacholska},
           familyi={P\bibinitperiod},
           given={Michalina},
           giveni={M\bibinitperiod}}}%
        {{hash=de7e7f4d56e896db85f56c06f77016a4}{%
           family={Berghammer},
           familyi={B\bibinitperiod},
           given={Tamas},
           giveni={T\bibinitperiod}}}%
        {{hash=0c0cd2e2fbeb23bb50214e8c94c61b08}{%
           family={Bodenstein},
           familyi={B\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{hash=494b568c5dc85ba8f3f409635f9c5f25}{%
           family={Vinyals},
           familyi={V\bibinitperiod},
           given={Oriol},
           giveni={O\bibinitperiod}}}%
        {{hash=211e54ca76df5d8c1de6957d48792545}{%
           family={Senior},
           familyi={S\bibinitperiod},
           given={Andrew\bibnamedelima W.},
           giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=d0d21ff02dda6c0a7d09ef7436bd329b}{%
           family={Kohli},
           familyi={K\bibinitperiod},
           given={Pushmeet},
           giveni={P\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Nature Publishing Group}%
      }
      \strng{namehash}{385b84ff9ec134408fea9aeba42f4525}
      \strng{fullhash}{fa5dce0980388f0460da0632f0c9ade1}
      \strng{bibnamehash}{385b84ff9ec134408fea9aeba42f4525}
      \strng{authorbibnamehash}{385b84ff9ec134408fea9aeba42f4525}
      \strng{authornamehash}{385b84ff9ec134408fea9aeba42f4525}
      \strng{authorfullhash}{fa5dce0980388f0460da0632f0c9ade1}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50~years9. Despite recent progress10–14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.}
      \field{issn}{1476-4687}
      \field{issue}{7873}
      \field{journaltitle}{Nature}
      \field{langid}{english}
      \field{month}{8}
      \field{number}{7873}
      \field{title}{Highly Accurate Protein Structure Prediction with {{AlphaFold}}}
      \field{urlday}{12}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{volume}{596}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{583\bibrangedash 589}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1038/s41586-021-03819-2
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/3XWK5JAZ/Jumper et al_2021_Highly accurate protein structure prediction with AlphaFold.pdf;/home/jonathan/Zotero/storage/5SSC5GX4/s41586-021-03819-2.html
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/s41586-021-03819-2
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/s41586-021-03819-2
      \endverb
      \keyw{motivation,thesis}
    \endentry
    \entry{kingmaAdamMethodStochastic2017}{misc}{}
      \name{author}{2}{}{%
        {{hash=b6fbd171848aad4edf3925543f1f1522}{%
           family={Kingma},
           familyi={K\bibinitperiod},
           given={Diederik\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=8aa66e8231cc2fdbe67aa4f18ca970c6}{%
           family={Ba},
           familyi={B\bibinitperiod},
           given={Jimmy},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{fullhash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{bibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorbibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authornamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorfullhash}{a09df9f123146b8e2c7f1134c9496932}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
      \field{day}{29}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{1}
      \field{number}{arXiv:1412.6980}
      \field{shorttitle}{Adam}
      \field{title}{Adam: {{A Method}} for {{Stochastic Optimization}}}
      \field{urlday}{4}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1412.6980
      \endverb
      \verb{eprint}
      \verb 1412.6980
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/BVHXFACA/Kingma_Ba_2017_Adam.pdf;/home/jonathan/Zotero/storage/H89Y849R/1412.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1412.6980
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1412.6980
      \endverb
    \endentry
    \entry{kloedenNumericalSolutionStochastic2013}{book}{}
      \name{author}{2}{}{%
        {{hash=7b32249850edcace60b1c231a0f5fb51}{%
           family={Kloeden},
           familyi={K\bibinitperiod},
           given={Peter\bibnamedelima E.},
           giveni={P\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=a4fb5859192206ba44cf72178b85abb6}{%
           family={Platen},
           familyi={P\bibinitperiod},
           given={Eckhard},
           giveni={E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Science \& Business Media}%
      }
      \strng{namehash}{0183f9b6362d5aa8d2faeea78d274490}
      \strng{fullhash}{0183f9b6362d5aa8d2faeea78d274490}
      \strng{bibnamehash}{0183f9b6362d5aa8d2faeea78d274490}
      \strng{authorbibnamehash}{0183f9b6362d5aa8d2faeea78d274490}
      \strng{authornamehash}{0183f9b6362d5aa8d2faeea78d274490}
      \strng{authorfullhash}{0183f9b6362d5aa8d2faeea78d274490}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The aim of this book is to provide an accessible introduction to stochastic differ ential equations and their applications together with a systematic presentation of methods available for their numerical solution. During the past decade there has been an accelerating interest in the de velopment of numerical methods for stochastic differential equations (SDEs). This activity has been as strong in the engineering and physical sciences as it has in mathematics, resulting inevitably in some duplication of effort due to an unfamiliarity with the developments in other disciplines. Much of the reported work has been motivated by the need to solve particular types of problems, for which, even more so than in the deterministic context, specific methods are required. The treatment has often been heuristic and ad hoc in character. Nevertheless, there are underlying principles present in many of the papers, an understanding of which will enable one to develop or apply appropriate numerical schemes for particular problems or classes of problems.}
      \field{day}{17}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-3-662-12616-5}
      \field{langid}{english}
      \field{month}{4}
      \field{pagetotal}{666}
      \field{title}{Numerical {{Solution}} of {{Stochastic Differential Equations}}}
      \field{year}{2013}
      \field{dateera}{ce}
      \verb{eprint}
      \verb r9r6CAAAQBAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/GSQ3ATQC/sde_theory.djvu
      \endverb
      \keyw{background,thesis}
    \endentry
    \entry{liStochasticModifiedEquations2019}{article}{}
      \name{author}{3}{}{%
        {{hash=64fe62ce2b10a4bc8ffec9881ed87985}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Qianxiao},
           giveni={Q\bibinitperiod}}}%
        {{hash=f6d5538a2db58ca22e371cdbeef3f83e}{%
           family={Tai},
           familyi={T\bibinitperiod},
           given={Cheng},
           giveni={C\bibinitperiod}}}%
        {{hash=268e9715905f2ab3cb20df636d3750c1}{%
           family={E},
           familyi={E\bibinitperiod},
           given={Weinan},
           giveni={W\bibinitperiod}}}%
      }
      \strng{namehash}{0049f3b8b809d78786a56fc4874c7f65}
      \strng{fullhash}{0049f3b8b809d78786a56fc4874c7f65}
      \strng{bibnamehash}{0049f3b8b809d78786a56fc4874c7f65}
      \strng{authorbibnamehash}{0049f3b8b809d78786a56fc4874c7f65}
      \strng{authornamehash}{0049f3b8b809d78786a56fc4874c7f65}
      \strng{authorfullhash}{0049f3b8b809d78786a56fc4874c7f65}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting.}
      \field{issn}{1533-7928}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{40}
      \field{shorttitle}{Stochastic {{Modified Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I}}}
      \field{title}{Stochastic {{Modified Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I Mathematical Foundations}}}
      \field{urlday}{19}
      \field{urlmonth}{8}
      \field{urlyear}{2022}
      \field{volume}{20}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 47}
      \range{pages}{47}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/CBBRN2E7/Li et al. - 2019 - Stochastic Modified Equations and Dynamics of Stoc.pdf
      \endverb
      \verb{urlraw}
      \verb http://jmlr.org/papers/v20/17-526.html
      \endverb
      \verb{url}
      \verb http://jmlr.org/papers/v20/17-526.html
      \endverb
      \keyw{SDE,SGD,theory,thesis}
    \endentry
    \entry{liConvergenceStochasticGradient2019}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=fbefb123f0fb2b74e98ede7afb2aa058}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Xiaoyu},
           giveni={X\bibinitperiod}}}%
        {{hash=35db660c992251cc50db3cedca6172f1}{%
           family={Orabona},
           familyi={O\bibinitperiod},
           given={Francesco},
           giveni={F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{5c6dafc7ca515d5a0141e7dc01b89612}
      \strng{fullhash}{5c6dafc7ca515d5a0141e7dc01b89612}
      \strng{bibnamehash}{5c6dafc7ca515d5a0141e7dc01b89612}
      \strng{authorbibnamehash}{5c6dafc7ca515d5a0141e7dc01b89612}
      \strng{authornamehash}{5c6dafc7ca515d5a0141e7dc01b89612}
      \strng{authorfullhash}{5c6dafc7ca515d5a0141e7dc01b89612}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice of the stepsizes. This has motivated a large body of research on adaptive stepsizes. However, there is currently a gap in our theoretical understanding of these methods, especially in the non-convex setting. In this paper, we start closing this gap: we theoretically analyze in the convex and non-convex settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions for these stepsizes to achieve almost sure asymptotic convergence of the gradients to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex setting. Moreover, we show that these stepsizes allow to automatically adapt to the level of noise of the stochastic gradients in both the convex and non-convex settings, interpolating between O(1/T) and O(1/sqrt(T)), up to logarithmic terms.}
      \field{booktitle}{Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{day}{11}
      \field{eventtitle}{The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{4}
      \field{title}{On the {{Convergence}} of {{Stochastic Gradient Descent}} with {{Adaptive Stepsizes}}}
      \field{urlday}{5}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{983\bibrangedash 992}
      \range{pages}{10}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/PH74N3RG/Li and Orabona - 2019 - On the Convergence of Stochastic Gradient Descent .pdf;/home/jonathan/Zotero/storage/S7U8TV3J/Li_Orabona_2019_On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v89/li19c.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v89/li19c.html
      \endverb
      \keyw{convergence,SGD,stepsize,thesis,toread}
    \endentry
    \entry{liValidityModelingSGD2021}{misc}{}
      \name{author}{3}{}{%
        {{hash=35069ab0f2565b95f02723d645a36cbd}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Zhiyuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=83997ef133cee9d7489f458ddb3c01fd}{%
           family={Malladi},
           familyi={M\bibinitperiod},
           given={Sadhika},
           giveni={S\bibinitperiod}}}%
        {{hash=8e08b59145b9531815169c7ed0a0b055}{%
           family={Arora},
           familyi={A\bibinitperiod},
           given={Sanjeev},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{acabdfdf9d3a3d356960a77b23063185}
      \strng{fullhash}{acabdfdf9d3a3d356960a77b23063185}
      \strng{bibnamehash}{acabdfdf9d3a3d356960a77b23063185}
      \strng{authorbibnamehash}{acabdfdf9d3a3d356960a77b23063185}
      \strng{authornamehash}{acabdfdf9d3a3d356960a77b23063185}
      \strng{authorfullhash}{acabdfdf9d3a3d356960a77b23063185}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{It is generally recognized that finite learning rate (LR), in contrast to infinitesimal LR, is important for good generalization in real-life deep nets. Most attempted explanations propose approximating finite-LR SGD with Ito Stochastic Differential Equations (SDEs), but formal justification for this approximation (e.g., (Li et al., 2019)) only applies to SGD with tiny LR. Experimental verification of the approximation appears computationally infeasible. The current paper clarifies the picture with the following contributions: (a) An efficient simulation algorithm SVAG that provably converges to the conventionally used Ito SDE approximation. (b) A theoretically motivated testable necessary condition for the SDE approximation and its most famous implication, the linear scaling rule (Goyal et al., 2017), to hold. (c) Experiments using this simulation to demonstrate that the previously proposed SDE approximation can meaningfully capture the training and generalization properties of common deep nets.}
      \field{day}{16}
      \field{month}{6}
      \field{number}{arXiv:2102.12470}
      \field{title}{On the {{Validity}} of {{Modeling SGD}} with {{Stochastic Differential Equations}} ({{SDEs}})}
      \field{urlday}{17}
      \field{urlmonth}{8}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2102.12470
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/T4H6WURH/Li et al. - 2021 - On the Validity of Modeling SGD with Stochastic Di.pdf;/home/jonathan/Zotero/storage/UR38RDHX/2102.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2102.12470
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2102.12470
      \endverb
      \keyw{paper,SDE,SGD,SVAG,thesis}
    \endentry
    \entry{mertikopoulosAlmostSureConvergence2020}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=e1238114e077a96e6a9a2849535aa0ac}{%
           family={Mertikopoulos},
           familyi={M\bibinitperiod},
           given={Panayotis},
           giveni={P\bibinitperiod}}}%
        {{hash=5596360646d9659db48105fb318012e6}{%
           family={Hallak},
           familyi={H\bibinitperiod},
           given={Nadav},
           giveni={N\bibinitperiod}}}%
        {{hash=d88f6ad57059179a65fecb043cddedb1}{%
           family={Kavis},
           familyi={K\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
        {{hash=3d62c5b86f1c1fd385a81d7f751ae855}{%
           family={Cevher},
           familyi={C\bibinitperiod},
           given={Volkan},
           giveni={V\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Red Hook, NY, USA}%
      }
      \list{publisher}{1}{%
        {Curran Associates Inc.}%
      }
      \strng{namehash}{a6e81a715ed6c7a9f502405739a6d2d1}
      \strng{fullhash}{a9399229b9df36ec42bbc057739d0b81}
      \strng{bibnamehash}{a6e81a715ed6c7a9f502405739a6d2d1}
      \strng{authorbibnamehash}{a6e81a715ed6c7a9f502405739a6d2d1}
      \strng{authornamehash}{a6e81a715ed6c7a9f502405739a6d2d1}
      \strng{authorfullhash}{a9399229b9df36ec42bbc057739d0b81}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper analyzes the trajectories of stochastic gradient descent (SGD) to help understand the algorithm's convergence properties in non-convex problems. We first show that the sequence of iterates generated by SGD remains bounded and converges with probability 1 under a very broad range of step-size schedules. Subsequently, going beyond existing positive probability guarantees, we show that SGD avoids strict saddle points/manifolds with probability 1 for the entire spectrum of step-size policies considered. Finally, we prove that the algorithm's rate of convergence to local minimizers with a positive-definite Hessian is O(1/np) if the method is employed with a Θ(1/np) step-size. This provides an important guideline for tuning the algorithm's step-size as it suggests that a cool-down phase with a vanishing step-size could lead to faster convergence; we demonstrate this heuristic using ResNet architectures on CIFAR.}
      \field{booktitle}{Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}}
      \field{day}{6}
      \field{isbn}{978-1-71382-954-6}
      \field{month}{12}
      \field{series}{{{NIPS}}'20}
      \field{title}{On the Almost Sure Convergence of Stochastic Gradient Descent in Non-Convex Problems}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{pages}{1117\bibrangedash 1128}
      \range{pages}{12}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/AYGGK6N8/Mertikopoulos et al_2020_On the almost sure convergence of stochastic gradient descent in non-convex.pdf
      \endverb
      \keyw{SGD,theory toread}
    \endentry
    \entry{moulinesNonAsymptoticAnalysisStochastic2011}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=32b961a3e53c735ea5488bf0af14a6da}{%
           family={Moulines},
           familyi={M\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=93da2819ab01d8a5e7bae39ce6f17c1f}{%
           family={Bach},
           familyi={B\bibinitperiod},
           given={Francis},
           giveni={F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{dfeaf744671e3d194e882f09b3f03467}
      \strng{fullhash}{dfeaf744671e3d194e882f09b3f03467}
      \strng{bibnamehash}{dfeaf744671e3d194e882f09b3f03467}
      \strng{authorbibnamehash}{dfeaf744671e3d194e882f09b3f03467}
      \strng{authornamehash}{dfeaf744671e3d194e882f09b3f03467}
      \strng{authorfullhash}{dfeaf744671e3d194e882f09b3f03467}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the minimization of a convex objective function defined on a Hilbert space, which is only available through unbiased estimates of its gradients. This problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a.\textasciitilde Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.\textasciitilde Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Non-{{Asymptotic Analysis}} of {{Stochastic Approximation Algorithms}} for {{Machine Learning}}}
      \field{urlday}{28}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{volume}{24}
      \field{year}{2011}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/DYU79DLD/Moulines_Bach_2011_Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine.pdf
      \endverb
      \verb{urlraw}
      \verb https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html
      \endverb
      \verb{url}
      \verb https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html
      \endverb
      \keyw{convergence,theory}
    \endentry
    \entry{nesterovLecturesConvexOptimization2018}{book}{}
      \name{author}{1}{}{%
        {{hash=8a41b35fe7b3d1725cb95bd7eec40b01}{%
           family={Nesterov},
           familyi={N\bibinitperiod},
           given={Yurii},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{fullhash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{bibnamehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{authorbibnamehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{authornamehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{authorfullhash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].}
      \field{day}{19}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-3-319-91578-4}
      \field{langid}{english}
      \field{month}{11}
      \field{pagetotal}{603}
      \field{title}{Lectures on {{Convex Optimization}}}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{eprint}
      \verb IPh6DwAAQBAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/DMSGV4QG/Nesterov_2018_Lectures on Convex Optimization.pdf
      \endverb
      \keyw{background,optimization,theory}
    \endentry
    \entry{nguyenSGDHogwildConvergence2018}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=599edac0e3691892a2a8f704d0f3072e}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Lam},
           giveni={L\bibinitperiod}}}%
        {{hash=70db9522f13ff81336aab84ea3b65f73}{%
           family={Nguyen},
           familyi={N\bibinitperiod},
           given={Phuong\bibnamedelima Ha},
           giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=90f519faf9c6c2031e393e32d792de69}{%
           family={Dijk},
           familyi={D\bibinitperiod},
           given={Marten},
           giveni={M\bibinitperiod}}}%
        {{hash=deb8076310b3d7e628147005cca384c9}{%
           family={Richtarik},
           familyi={R\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=227fd756890273896e67d801c215790f}{%
           family={Scheinberg},
           familyi={S\bibinitperiod},
           given={Katya},
           giveni={K\bibinitperiod}}}%
        {{hash=0111eb2e5d937cdff42a82c58eb5a73f}{%
           family={Takac},
           familyi={T\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{f8c77bf05347b5bf578b639c0415a23d}
      \strng{fullhash}{2653f7ef7aaa4034c35e5283f249a3c3}
      \strng{bibnamehash}{f8c77bf05347b5bf578b639c0415a23d}
      \strng{authorbibnamehash}{f8c77bf05347b5bf578b639c0415a23d}
      \strng{authornamehash}{f8c77bf05347b5bf578b639c0415a23d}
      \strng{authorfullhash}{2653f7ef7aaa4034c35e5283f249a3c3}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.}
      \field{booktitle}{Proceedings of the 35th {{International Conference}} on {{Machine Learning}}}
      \field{day}{3}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{{{SGD}} and {{Hogwild}}! {{Convergence Without}} the {{Bounded Gradients Assumption}}}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{pages}{3750\bibrangedash 3758}
      \range{pages}{9}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/A7GRMKW2/Nguyen et al. - 2018 - SGD and Hogwild! Convergence Without the Bounded G.pdf;/home/jonathan/Zotero/storage/QN8TDXNA/Nguyen et al_2018_SGD and Hogwild.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v80/nguyen18c.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v80/nguyen18c.html
      \endverb
    \endentry
    \entry{nocedalNumericalOptimization2006}{book}{}
      \name{author}{2}{}{%
        {{hash=88a342d927bf795b0d92af8a5613da31}{%
           family={Nocedal},
           familyi={N\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod}}}%
        {{hash=7763509f7c4b10ac0b0770c30259cb1d}{%
           family={Wright},
           familyi={W\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer New York}%
      }
      \strng{namehash}{ce7f3d831c22b27fad5b1d2afbe4ec28}
      \strng{fullhash}{ce7f3d831c22b27fad5b1d2afbe4ec28}
      \strng{bibnamehash}{ce7f3d831c22b27fad5b1d2afbe4ec28}
      \strng{authorbibnamehash}{ce7f3d831c22b27fad5b1d2afbe4ec28}
      \strng{authornamehash}{ce7f3d831c22b27fad5b1d2afbe4ec28}
      \strng{authorfullhash}{ce7f3d831c22b27fad5b1d2afbe4ec28}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side. There is a selected solutions manual for instructors for the new edition.}
      \field{day}{27}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-0-387-30303-1}
      \field{langid}{english}
      \field{month}{7}
      \field{pagetotal}{694}
      \field{title}{Numerical {{Optimization}}}
      \field{year}{2006}
      \field{dateera}{ce}
      \verb{eprint}
      \verb eNlPAAAAMAAJ
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/2TMCX6FC/Nocedal_Wright_2006_Numerical Optimization.pdf
      \endverb
    \endentry
    \entry{oksendalStochasticDifferentialEquations2003}{book}{}
      \name{author}{1}{}{%
        {{hash=08c642070d573e8ec607ded1724116ab}{%
           family={Øksendal},
           familyi={Ø\bibinitperiod},
           given={Bernt},
           giveni={B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Science \& Business Media}%
      }
      \strng{namehash}{08c642070d573e8ec607ded1724116ab}
      \strng{fullhash}{08c642070d573e8ec607ded1724116ab}
      \strng{bibnamehash}{08c642070d573e8ec607ded1724116ab}
      \strng{authorbibnamehash}{08c642070d573e8ec607ded1724116ab}
      \strng{authornamehash}{08c642070d573e8ec607ded1724116ab}
      \strng{authorfullhash}{08c642070d573e8ec607ded1724116ab}
      \field{sortinit}{Ø}
      \field{sortinithash}{2cd7140a07aea5341f9e2771efe90aae}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This edition contains detailed solutions of selected exercises. Many readers have requested this, because it makes the book more suitable for self-study. At the same time new exercises (without solutions) have beed added. They have all been placed in the end of each chapter, in order to facilitate the use of this edition together with previous ones. Several errors have been corrected and formulations have been improved. This has been made possible by the valuable comments from (in alphabetical order) Jon Bohlin, Mark Davis, Helge Holden, Patrick Jaillet, Chen Jing, Natalia Koroleva, MarioLefebvre, Alexander Matasov, Thilo Meyer-Brandis, Keigo Osawa, Bjørn Thunestvedt, Jan Ubøe and Yngve Williassen. I thank them all for helping to improve the book. My thanks also go to Dina Haraldsson, who once again has performed the typing and drawn the ?gures with great skill. Blindern, September 2002 Bernt Øksendal xv Preface to Corrected Printing, Fifth Edition The main corrections and improvements in this corrected printing are from Chapter 12. I have bene?tted from useful comments from a number of p- ple, including (in alphabetical order) Fredrik Dahl, Simone Deparis, Ulrich Haussmann, Yaozhong Hu, Marianne Huebner, Carl Peter Kirkebø, Ni- lay Kolev, Takashi Kumagai, Shlomo Levental, Geir Magnussen, Anders Øksendal, Jur ] gen Pottho?, Colin Rowat, Stig Sandnes, Lones Smith, S- suo Taniguchi and Bjørn Thunestvedt. I want to thank them all for helping me making the book better. I also want to thank Dina Haraldsson for pro?cient typing.}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-3-540-04758-2}
      \field{langid}{english}
      \field{pagetotal}{406}
      \field{shorttitle}{Stochastic {{Differential Equations}}}
      \field{title}{Stochastic {{Differential Equations}}: {{An Introduction}} with {{Applications}}}
      \field{year}{2003}
      \field{dateera}{ce}
      \verb{eprint}
      \verb VgQDWyihxKYC
      \endverb
    \endentry
    \entry{polyakMethodsSpeedingConvergence1964}{article}{}
      \name{author}{1}{}{%
        {{hash=86e8da8c20afcbab24c7fbcca00e3f0e}{%
           family={Polyak},
           familyi={P\bibinitperiod},
           given={B.\bibnamedelimi T.},
           giveni={B\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \strng{namehash}{86e8da8c20afcbab24c7fbcca00e3f0e}
      \strng{fullhash}{86e8da8c20afcbab24c7fbcca00e3f0e}
      \strng{bibnamehash}{86e8da8c20afcbab24c7fbcca00e3f0e}
      \strng{authorbibnamehash}{86e8da8c20afcbab24c7fbcca00e3f0e}
      \strng{authornamehash}{86e8da8c20afcbab24c7fbcca00e3f0e}
      \strng{authorfullhash}{86e8da8c20afcbab24c7fbcca00e3f0e}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.}
      \field{day}{1}
      \field{issn}{0041-5553}
      \field{journaltitle}{USSR Computational Mathematics and Mathematical Physics}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{5}
      \field{shortjournal}{USSR Computational Mathematics and Mathematical Physics}
      \field{title}{Some Methods of Speeding up the Convergence of Iteration Methods}
      \field{urlday}{15}
      \field{urlmonth}{11}
      \field{urlyear}{2022}
      \field{volume}{4}
      \field{year}{1964}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 17}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1016/0041-5553(64)90137-5
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/CLB5BQTY/0041555364901375.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0041555364901375
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0041555364901375
      \endverb
    \endentry
    \entry{rameshHierarchicalTextConditionalImage2022}{misc}{}
      \name{author}{5}{}{%
        {{hash=82063a12702e7b2c026ae0ff03b8f102}{%
           family={Ramesh},
           familyi={R\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod}}}%
        {{hash=4164e43d8cf919f5e3f8d80f5ea23f36}{%
           family={Dhariwal},
           familyi={D\bibinitperiod},
           given={Prafulla},
           giveni={P\bibinitperiod}}}%
        {{hash=fc19c0dc057e2a02419916b39da6895b}{%
           family={Nichol},
           familyi={N\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=73c06c1d85385c5a690e667e9d6f64ea}{%
           family={Chu},
           familyi={C\bibinitperiod},
           given={Casey},
           giveni={C\bibinitperiod}}}%
        {{hash=fb15a691583ec94aafe0be6e7da4878f}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{01fa2e22064cfc88b7963c1f70eeaa5e}
      \strng{fullhash}{fce744ae15bcc5a3c7b36835e58e24e9}
      \strng{bibnamehash}{01fa2e22064cfc88b7963c1f70eeaa5e}
      \strng{authorbibnamehash}{01fa2e22064cfc88b7963c1f70eeaa5e}
      \strng{authornamehash}{01fa2e22064cfc88b7963c1f70eeaa5e}
      \strng{authorfullhash}{fce744ae15bcc5a3c7b36835e58e24e9}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.}
      \field{day}{12}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{4}
      \field{number}{arXiv:2204.06125}
      \field{title}{Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}}
      \field{urlday}{12}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2204.06125
      \endverb
      \verb{eprint}
      \verb 2204.06125
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/FDTGXDWS/Ramesh et al_2022_Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf;/home/jonathan/Zotero/storage/6RXSLEHY/2204.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2204.06125
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2204.06125
      \endverb
      \keyw{motivation,thesis}
    \endentry
    \entry{robbinsStochasticApproximationMethod1951}{article}{}
      \name{author}{2}{}{%
        {{hash=163566b92b332258782e61e3d217a2bb}{%
           family={Robbins},
           familyi={R\bibinitperiod},
           given={Herbert},
           giveni={H\bibinitperiod}}}%
        {{hash=95751f7f614bc2af42f1a078b3e2ba61}{%
           family={Monro},
           familyi={M\bibinitperiod},
           given={Sutton},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Institute of Mathematical Statistics}%
      }
      \strng{namehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{fullhash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{bibnamehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{authorbibnamehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{authornamehash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \strng{authorfullhash}{5d85b58c5cbfcc8fc266cf4f5f6fbdbb}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = \textbackslash theta\$ of the equation \$M(x) = \textbackslash alpha\$, where \$\textbackslash alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,\textbackslash cdots\$ in such a way that \$x\_n\$ will tend to \$\textbackslash theta\$ in probability.}
      \field{issn}{0003-4851, 2168-8990}
      \field{journaltitle}{The Annals of Mathematical Statistics}
      \field{month}{9}
      \field{number}{3}
      \field{title}{A {{Stochastic Approximation Method}}}
      \field{urlday}{29}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{volume}{22}
      \field{year}{1951}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{400\bibrangedash 407}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1214/aoms/1177729586
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/7UGIIDFM/Robbins_Monro_1951_A Stochastic Approximation Method.pdf;/home/jonathan/Zotero/storage/SMM3N24C/1177729586.html
      \endverb
      \verb{urlraw}
      \verb https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full
      \endverb
      \verb{url}
      \verb https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full
      \endverb
    \endentry
    \entry{santambrogioEuclideanMetricWasserstein2017}{article}{}
      \name{author}{1}{}{%
        {{hash=deb5f5e08d40c7c61a19403771acf3c3}{%
           family={Santambrogio},
           familyi={S\bibinitperiod},
           given={Filippo},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{deb5f5e08d40c7c61a19403771acf3c3}
      \strng{fullhash}{deb5f5e08d40c7c61a19403771acf3c3}
      \strng{bibnamehash}{deb5f5e08d40c7c61a19403771acf3c3}
      \strng{authorbibnamehash}{deb5f5e08d40c7c61a19403771acf3c3}
      \strng{authornamehash}{deb5f5e08d40c7c61a19403771acf3c3}
      \strng{authorfullhash}{deb5f5e08d40c7c61a19403771acf3c3}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{This is an expository paper on the theory of gradient flows, and in particular of those PDEs which can be interpreted as gradient flows for the Wasserstein metric on the space of probability measures (a distance induced by optimal transport). The starting point is the Euclidean theory, and then its generalization to metric spaces, according to the work of Ambrosio, Gigli and Savaré. Then comes an independent exposition of the Wasserstein theory, with a short introduction to the optimal transport tools that are needed and to the notion of geodesic convexity, followed by a precise description of the Jordan–Kinderlehrer–Otto scheme and a sketch of proof to obtain its convergence in the easiest cases. A discussion of which equations are gradient flows PDEs and of numerical methods based on these ideas is also provided. The paper ends with a new, theoretical, development, due to Ambrosio, Gigli, Savaré, Kuwada and Ohta: the study of the heat flow in metric measure spaces.}
      \field{day}{1}
      \field{issn}{1664-3615}
      \field{journaltitle}{Bulletin of Mathematical Sciences}
      \field{langid}{english}
      \field{month}{4}
      \field{number}{1}
      \field{shortjournal}{Bull. Math. Sci.}
      \field{shorttitle}{\{\vphantom\}{{Euclidean}}, Metric, and {{Wasserstein}}\vphantom\{\} Gradient Flows}
      \field{title}{\{\vphantom\}{{Euclidean}}, Metric, and {{Wasserstein}}\vphantom\{\} Gradient Flows: An Overview}
      \field{urlday}{23}
      \field{urlmonth}{11}
      \field{urlyear}{2022}
      \field{volume}{7}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{87\bibrangedash 154}
      \range{pages}{68}
      \verb{doi}
      \verb 10.1007/s13373-017-0101-1
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/BX46TE3W/Santambrogio_2017_ Euclidean, metric, and Wasserstein gradient flows.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s13373-017-0101-1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s13373-017-0101-1
      \endverb
      \keyw{gradient flow,theory}
    \endentry
    \entry{schrittwieserMasteringAtariGo2020}{article}{}
      \name{author}{12}{}{%
        {{hash=8fad8df927bc0014c0bd6a9feb7aa71d}{%
           family={Schrittwieser},
           familyi={S\bibinitperiod},
           given={Julian},
           giveni={J\bibinitperiod}}}%
        {{hash=af540e84ef1ecdaa70b1f7c90f59fd7d}{%
           family={Antonoglou},
           familyi={A\bibinitperiod},
           given={Ioannis},
           giveni={I\bibinitperiod}}}%
        {{hash=80c63e95a9e243591a33a7e3156f1d78}{%
           family={Hubert},
           familyi={H\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=9d16b7284df92c9adaee86c37ab992df}{%
           family={Simonyan},
           familyi={S\bibinitperiod},
           given={Karen},
           giveni={K\bibinitperiod}}}%
        {{hash=50d24de916599d306c5cb1a77156e4b9}{%
           family={Sifre},
           familyi={S\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=80f59bf87c57eee7dab96e04c0a83c30}{%
           family={Schmitt},
           familyi={S\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=4131bd14e5ca890278ecd351e356dc34}{%
           family={Guez},
           familyi={G\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=67c36471603b6de0347c489b0f8b05b0}{%
           family={Lockhart},
           familyi={L\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
        {{hash=368b9b2de627b852658c433b062d4e1e}{%
           family={Graepel},
           familyi={G\bibinitperiod},
           given={Thore},
           giveni={T\bibinitperiod}}}%
        {{hash=3a6fdf4df9a25f1d2d506ad9e86e1f6c}{%
           family={Lillicrap},
           familyi={L\bibinitperiod},
           given={Timothy},
           giveni={T\bibinitperiod}}}%
        {{hash=89dbd30410c2085cd059f32c57d4593e}{%
           family={Silver},
           familyi={S\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{ba8dd9d821587916ffa0d5d85ba62066}
      \strng{fullhash}{a2393dd56fc0d1677b29875d3184f845}
      \strng{bibnamehash}{ba8dd9d821587916ffa0d5d85ba62066}
      \strng{authorbibnamehash}{ba8dd9d821587916ffa0d5d85ba62066}
      \strng{authornamehash}{ba8dd9d821587916ffa0d5d85ba62066}
      \strng{authorfullhash}{a2393dd56fc0d1677b29875d3184f845}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.}
      \field{day}{24}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arxiv}
      \field{issn}{0028-0836, 1476-4687}
      \field{journaltitle}{Nature}
      \field{month}{12}
      \field{number}{7839}
      \field{shortjournal}{Nature}
      \field{title}{Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}}
      \field{urlday}{12}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{volume}{588}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{604\bibrangedash 609}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1038/s41586-020-03051-4
      \endverb
      \verb{eprint}
      \verb 1911.08265
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/Y4H7QWLF/Schrittwieser et al_2020_Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf;/home/jonathan/Zotero/storage/4QJ4BJ6Y/1911.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1911.08265
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1911.08265
      \endverb
      \keyw{motivation,thesis}
    \endentry
    \entry{sebbouhAlmostSureConvergence2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=f945ea2364f348a67141fc274a4a4a0c}{%
           family={Sebbouh},
           familyi={S\bibinitperiod},
           given={Othmane},
           giveni={O\bibinitperiod}}}%
        {{hash=d740b920ee7b92aaa73e3450f6a6e085}{%
           family={Gower},
           familyi={G\bibinitperiod},
           given={Robert\bibnamedelima M.},
           giveni={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=a5551dfae979b8c2befb546fac59ffee}{%
           family={Defazio},
           familyi={D\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{4a9548a46c2451d899c3abf7f2cd4c1c}
      \strng{fullhash}{4a9548a46c2451d899c3abf7f2cd4c1c}
      \strng{bibnamehash}{4a9548a46c2451d899c3abf7f2cd4c1c}
      \strng{authorbibnamehash}{4a9548a46c2451d899c3abf7f2cd4c1c}
      \strng{authornamehash}{4a9548a46c2451d899c3abf7f2cd4c1c}
      \strng{authorfullhash}{4a9548a46c2451d899c3abf7f2cd4c1c}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study stochastic gradient descent (SGD) and the stochastic heavy ball method (SHB, otherwise known as the momentum method) for the general stochastic approximation problem. For SGD, in the convex and smooth setting, we provide the first \textbackslash emph\{almost sure\} asymptotic convergence \textbackslash emph\{rates\} for a weighted average of the iterates . More precisely, we show that the convergence rate of the function values is arbitrarily close to o(1/k−−√)o(1/k)o(1/\textbackslash sqrt\{k\}), and is exactly o(1/k)o(1/k)o(1/k) in the so-called overparametrized case. We show that these results still hold when using a decreasing step size version of stochastic line search and stochastic Polyak stepsizes, thereby giving the first proof of convergence of these methods in the non-overparametrized regime. Using a substantially different analysis, we show that these rates hold for SHB as well, but at the last iterate. This distinction is important because it is the last iterate of SGD and SHB which is used in practice. We also show that the last iterate of SHB converges to a minimizer \textbackslash emph\{almost surely\}. Additionally, we prove that the function values of the deterministic HB converge at a o(1/k)o(1/k)o(1/k) rate, which is faster than the previously known O(1/k)O(1/k)O(1/k). Finally, in the nonconvex setting, we prove similar rates on the lowest gradient norm along the trajectory of SGD.}
      \field{booktitle}{Proceedings of {{Thirty Fourth Conference}} on {{Learning Theory}}}
      \field{day}{21}
      \field{eventtitle}{Conference on {{Learning Theory}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Almost Sure Convergence Rates for {{Stochastic Gradient Descent}} and {{Stochastic Heavy Ball}}}
      \field{urlday}{5}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{3935\bibrangedash 3971}
      \range{pages}{37}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/2NINK37W/Sebbouh et al_2021_Almost sure convergence rates for Stochastic Gradient Descent and Stochastic.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v134/sebbouh21a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v134/sebbouh21a.html
      \endverb
      \keyw{convergence,SGD,theory,thesis,toread}
    \endentry
    \entry{uhlenbeckTheoryBrownianMotion1930}{article}{}
      \name{author}{2}{}{%
        {{hash=3cb5f635974bd39fd8858d930a97e4c8}{%
           family={Uhlenbeck},
           familyi={U\bibinitperiod},
           given={G.\bibnamedelimi E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=6aa3305d0ced76439870cb07bc3c48c6}{%
           family={Ornstein},
           familyi={O\bibinitperiod},
           given={L.\bibnamedelimi S.},
           giveni={L\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {American Physical Society}%
      }
      \strng{namehash}{160392c4717bd1ae477c777e26beeebc}
      \strng{fullhash}{160392c4717bd1ae477c777e26beeebc}
      \strng{bibnamehash}{160392c4717bd1ae477c777e26beeebc}
      \strng{authorbibnamehash}{160392c4717bd1ae477c777e26beeebc}
      \strng{authornamehash}{160392c4717bd1ae477c777e26beeebc}
      \strng{authorfullhash}{160392c4717bd1ae477c777e26beeebc}
      \field{sortinit}{U}
      \field{sortinithash}{6901a00e45705986ee5e7ca9fd39adca}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u−u0exp(−βt) and s−u0β[1−exp(−βt)] where u0 is the initial velocity and β the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and Fürth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when β is much larger than the frequency and for values of t≫β−1, the formula takes the form of that previously given by Smoluchowski.}
      \field{day}{1}
      \field{journaltitle}{Physical Review}
      \field{month}{9}
      \field{number}{5}
      \field{shortjournal}{Phys. Rev.}
      \field{title}{On the {{Theory}} of the {{Brownian Motion}}}
      \field{urlday}{12}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{volume}{36}
      \field{year}{1930}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{823\bibrangedash 841}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1103/PhysRev.36.823
      \endverb
      \verb{file}
      \verb /home/jonathan/Zotero/storage/FQQXCPHW/PhysRev.36.html
      \endverb
      \verb{urlraw}
      \verb https://link.aps.org/doi/10.1103/PhysRev.36.823
      \endverb
      \verb{url}
      \verb https://link.aps.org/doi/10.1103/PhysRev.36.823
      \endverb
      \keyw{SDE,thesis}
    \endentry
    \entry{vaswaniFastFasterConvergence2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=646217040d4cc6dff6965ac1b2b22ccf}{%
           family={Vaswani},
           familyi={V\bibinitperiod},
           given={Sharan},
           giveni={S\bibinitperiod}}}%
        {{hash=93da2819ab01d8a5e7bae39ce6f17c1f}{%
           family={Bach},
           familyi={B\bibinitperiod},
           given={Francis},
           giveni={F\bibinitperiod}}}%
        {{hash=6ec4971d3a33801f19547675e0e66e5e}{%
           family={Schmidt},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{a482dbb7c8a6ea24bd95fac7340d44d2}
      \strng{fullhash}{a482dbb7c8a6ea24bd95fac7340d44d2}
      \strng{bibnamehash}{a482dbb7c8a6ea24bd95fac7340d44d2}
      \strng{authorbibnamehash}{a482dbb7c8a6ea24bd95fac7340d44d2}
      \strng{authornamehash}{a482dbb7c8a6ea24bd95fac7340d44d2}
      \strng{authorfullhash}{a482dbb7c8a6ea24bd95fac7340d44d2}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely, resulting in zero training loss. For such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. Under this condition, we prove that constant step-size stochastic gradient descent (SGD) with Nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. We also show that this condition implies that SGD can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. Under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. Given this weaker condition, we prove that SGD with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. Under additional assumptions, the above results enable us to prove an O(1/k2)O(1/k2)O(1/k\^2) mistake bound for kkk iterations of a stochastic perceptron algorithm using the squared-hinge loss. Finally, we validate our theoretical findings with experiments on synthetic and real datasets.}
      \field{booktitle}{Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{day}{11}
      \field{eventtitle}{The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{4}
      \field{title}{Fast and {{Faster Convergence}} of {{SGD}} for {{Over-Parameterized Models}} and an {{Accelerated Perceptron}}}
      \field{urlday}{7}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1195\bibrangedash 1204}
      \range{pages}{10}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/A3MP9FRI/Vaswani et al_2019_Fast and Faster Convergence of SGD for Over-Parameterized Models and an.pdf;/home/jonathan/Zotero/storage/TAFFELXT/Vaswani et al. - 2019 - Fast and Faster Convergence of SGD for Over-Parame.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v89/vaswani19a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v89/vaswani19a.html
      \endverb
      \keyw{SGD,thesis,toread}
    \endentry
    \entry{wilsonMarginalValueAdaptive2017}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=f1a3315701af5f6e61157202fd95a376}{%
           family={Wilson},
           familyi={W\bibinitperiod},
           given={Ashia\bibnamedelima C},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=a38e53f2691eb44e52e78c8e72de65a7}{%
           family={Roelofs},
           familyi={R\bibinitperiod},
           given={Rebecca},
           giveni={R\bibinitperiod}}}%
        {{hash=8b00e95c184f76a063851ed2b67c9301}{%
           family={Stern},
           familyi={S\bibinitperiod},
           given={Mitchell},
           giveni={M\bibinitperiod}}}%
        {{hash=34451c97b5a5bad6fcb6513573d1f94d}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nati},
           giveni={N\bibinitperiod}}}%
        {{hash=3503059e1c0c778913607c87d8c5173a}{%
           family={Recht},
           familyi={R\bibinitperiod},
           given={Benjamin},
           giveni={B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{8131958b5024bf37b7a3923ce6364896}
      \strng{fullhash}{559a2cb8b9ec7cdcb30bc73d2041feb9}
      \strng{bibnamehash}{8131958b5024bf37b7a3923ce6364896}
      \strng{authorbibnamehash}{8131958b5024bf37b7a3923ce6364896}
      \strng{authornamehash}{8131958b5024bf37b7a3923ce6364896}
      \strng{authorfullhash}{559a2cb8b9ec7cdcb30bc73d2041feb9}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}}
      \field{urlday}{4}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{volume}{30}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/KD3I6N73/Wilson et al_2017_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html
      \endverb
      \verb{url}
      \verb https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html
      \endverb
    \endentry
    \entry{zhouStochasticMirrorDescent2017}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=92bcf64f6a434d65d69d4718e65a964b}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Zhengyuan},
           giveni={Z\bibinitperiod}}}%
        {{hash=e1238114e077a96e6a9a2849535aa0ac}{%
           family={Mertikopoulos},
           familyi={M\bibinitperiod},
           given={Panayotis},
           giveni={P\bibinitperiod}}}%
        {{hash=24d79d32df4f0bf49ed468479df9b85d}{%
           family={Bambos},
           familyi={B\bibinitperiod},
           given={Nicholas},
           giveni={N\bibinitperiod}}}%
        {{hash=ec94d94cc487dd71939a90cbeaaf47d0}{%
           family={Boyd},
           familyi={B\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=bbb2938067028135a82ee516bd25dece}{%
           family={Glynn},
           familyi={G\bibinitperiod},
           given={Peter\bibnamedelima W},
           giveni={P\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{7eb30bec1cd2d4a9f43e2d14aeb3d8b7}
      \strng{fullhash}{a8c723d01dd69bdbb0e063d3430725db}
      \strng{bibnamehash}{7eb30bec1cd2d4a9f43e2d14aeb3d8b7}
      \strng{authorbibnamehash}{7eb30bec1cd2d4a9f43e2d14aeb3d8b7}
      \strng{authornamehash}{7eb30bec1cd2d4a9f43e2d14aeb3d8b7}
      \strng{authorfullhash}{a8c723d01dd69bdbb0e063d3430725db}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we examine a class of non-convex stochastic optimization problems which we call variationally coherent, and which properly includes pseudo-/quasiconvex and star-convex optimization problems. To solve such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case), and we show that the last iterate of SMD converges to the problem’s solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather, variational coherence, a much weaker requirement, suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Stochastic {{Mirror Descent}} in {{Variationally Coherent Optimization Problems}}}
      \field{urlday}{29}
      \field{urlmonth}{9}
      \field{urlyear}{2022}
      \field{volume}{30}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/jonathan/Zotero/storage/6CE883YF/Zhou et al_2017_Stochastic Mirror Descent in Variationally Coherent Optimization Problems.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2017/hash/e6ba70fc093b4ce912d769ede1ceeba8-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2017/hash/e6ba70fc093b4ce912d769ede1ceeba8-Abstract.html
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

