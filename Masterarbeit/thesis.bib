@book{aggarwalNeuralNetworksDeep2018,
  title = {Neural {{Networks}} and {{Deep Learning}}: {{A Textbook}}},
  shorttitle = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Aggarwal, Charu C.},
  date = {2018-08-25},
  eprint = {achqDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer}},
  abstract = {This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories: The basics of neural networks:  Many traditional machine learning models can be understood as special cases of neural networks. An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10. The book is written for graduate students, researchers, and practitioners. Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.},
  isbn = {978-3-319-94463-0},
  langid = {english},
  pagetotal = {512}
}

@book{ahmadTextbookOrdinaryDifferential2015,
  title = {A {{Textbook}} on {{Ordinary Differential Equations}}},
  author = {Ahmad, Shair and Ambrosetti, Antonio},
  date = {2015-06-05},
  eprint = {1vHLCQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer}},
  abstract = {This book offers readers a primer on the theory and applications of Ordinary Differential Equations. The style used is simple, yet thorough and rigorous. Each chapter ends with a broad set of exercises that range from the routine to the more challenging and thought-provoking. Solutions to selected exercises can be found at the end of the book. The book contains many interesting examples on topics such as electric circuits, the pendulum equation, the logistic equation, the Lotka-Volterra system, the Laplace Transform, etc., which introduce students to a number of interesting aspects of the theory and applications. The work is mainly intended for students of Mathematics, Physics, Engineering, Computer Science and other areas of the natural and social sciences that use ordinary differential equations, and who have a firm grasp of Calculus and a minimal understanding of the basic concepts used in Linear Algebra. It also studies a few more advanced topics, such as Stability Theory and Boundary Value Problems, which may be suitable for more advanced undergraduate or first-year graduate students. The second edition has been revised to correct minor errata, and features a number of carefully selected new exercises, together with more detailed explanations of some of the topics.A complete Solutions Manual, containing solutions to all the exercises published in the book, is available. Instructors who wish to adopt the book may request the manual by writing directly to one of the authors.},
  isbn = {978-3-319-16408-3},
  langid = {english},
  pagetotal = {337},
  file = {/home/jonathan/Zotero/storage/FKUTC4WQ/Ahmad_Ambrosetti_2015_A Textbook on Ordinary Differential Equations.pdf}
}

@inproceedings{aliImplicitRegularizationStochastic2020,
  title = {The {{Implicit Regularization}} of {{Stochastic Gradient Flow}} for {{Least Squares}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Ali, Alnur and Dobriban, Edgar and Tibshirani, Ryan},
  date = {2020-11-21},
  pages = {233--244},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/ali20a.html},
  urldate = {2022-10-06},
  abstract = {We study the implicit regularization of mini-batch stochastic gradient descent, when applied to the fundamental problem of least squares regression. We leverage a continuous-time stochastic differential equation having the same moments as stochastic gradient descent, which we call stochastic gradient flow. We give a bound on the excess risk of stochastic gradient flow at time ttt, over ridge regression with tuning parameter λ=1/tλ=1/t\textbackslash lambda = 1/t. The bound may be computed from explicit constants (e.g., the mini-batch size, step size, number of iterations), revealing precisely how these quantities drive the excess risk. Numerical examples show the bound can be small, indicating a tight relationship between the two estimators. We give a similar result relating the coefficients of stochastic gradient flow and ridge. These results hold under no conditions on the data matrix XXX, and across the entire optimization path (not just at convergence).},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {SDE,SGD},
  file = {/home/jonathan/Zotero/storage/GG7CLXNG/Ali et al_2020_The Implicit Regularization of Stochastic Gradient Flow for Least Squares.pdf;/home/jonathan/Zotero/storage/ZNZSW9UP/Ali et al. - 2020 - The Implicit Regularization of Stochastic Gradient.pdf}
}

@inproceedings{allen-zhuConvergenceTheoryDeep2019,
  title = {A {{Convergence Theory}} for {{Deep Learning}} via {{Over-Parameterization}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  date = {2019-05-24},
  pages = {242--252},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  urldate = {2022-09-21},
  abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100\% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps ~ e\^\{-T\}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/D3Y8I392/Allen-Zhu et al_2019_A Convergence Theory for Deep Learning via Over-Parameterization.pdf}
}

@book{baldiEquazioniDifferenzialiStocastiche2000,
  title = {Equazioni differenziali stocastiche e applicazioni},
  author = {Baldi, Paolo},
  date = {2000},
  eprint = {qkHMPQAACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Pitagora}},
  isbn = {978-88-371-1211-0},
  langid = {italian},
  pagetotal = {367}
}

@inproceedings{barrettImplicitGradientRegularization2021,
  title = {Implicit {{Gradient Regularization}}},
  author = {Barrett, David and Dherin, Benoit},
  date = {2021-03-11},
  url = {https://openreview.net/forum?id=3q5IqUrkcF},
  urldate = {2022-12-05},
  abstract = {Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly regularize models by penalizing gradient descent trajectories that have large loss gradients. We call this Implicit Gradient Regularization (IGR) and we use backward error analysis to calculate the size of this regularization. We confirm empirically that implicit gradient regularization biases gradient descent toward flat minima, where test errors are small and solutions are robust to noisy parameter perturbations. Furthermore, we demonstrate that the implicit gradient regularization term can be used as an explicit regularizer, allowing us to control this gradient regularization directly. More broadly, our work indicates that backward error analysis is a useful theoretical approach to the perennial question of how learning rate, model size, and parameter regularization interact to determine the properties of overparameterized models optimized with gradient descent.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/6KI7QFEE/Barrett_Dherin_2021_Implicit Gradient Regularization.pdf;/home/jonathan/Zotero/storage/FR4HFHZ5/forum.html}
}

@misc{blancImplicitRegularizationDeep2020,
  title = {Implicit Regularization for Deep Neural Networks Driven by an {{Ornstein-Uhlenbeck}} like Process},
  author = {Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  date = {2020-07-22},
  number = {arXiv:1904.09080},
  eprint = {1904.09080},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.09080},
  url = {http://arxiv.org/abs/1904.09080},
  urldate = {2022-09-04},
  abstract = {We consider networks, trained via stochastic gradient descent to minimize \$\textbackslash ell\_2\$ loss, with the training labels perturbed by independent noise at each iteration. We characterize the behavior of the training dynamics near any parameter vector that achieves zero training error, in terms of an implicit regularization term corresponding to the sum over the data points, of the squared \$\textbackslash ell\_2\$ norm of the gradient of the model with respect to the parameter vector, evaluated at each data point. This holds for networks of any connectivity, width, depth, and choice of activation function. We interpret this implicit regularization term for three simple settings: matrix sensing, two layer ReLU networks trained on one-dimensional data, and two layer networks with sigmoid activations trained on a single datapoint. For these settings, we show why this new and general implicit regularization effect drives the networks towards "simple" models.},
  archiveprefix = {arXiv},
  keywords = {noise,sgd,thesis},
  file = {/home/jonathan/Zotero/storage/CSMWKWUX/Blanc et al_2020_Implicit regularization for deep neural networks driven by an.pdf;/home/jonathan/Zotero/storage/RP4R4RJM/1904.html}
}

@online{blogUnderstandingNeuralTangent,
  title = {Understanding the {{Neural Tangent Kernel}}},
  author = {Blog, Rajat's},
  url = {https://rajatvd.github.io/NTK/},
  urldate = {2022-11-22},
  abstract = {My attempt at distilling the ideas behind the neural tangent kernel that is making waves in recent theoretical deep learning research.},
  file = {/home/jonathan/Zotero/storage/MST9YM5D/NTK.html}
}

@article{bottouOptimizationMethodsLargeScale2018,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2018-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {60},
  number = {2},
  pages = {223--311},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/16M1080173},
  url = {https://epubs.siam.org/doi/10.1137/16M1080173},
  urldate = {2022-09-05},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/HVTDRSTJ/Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf}
}

@article{bottouOptimizationMethodsLargeScale2018a,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2018-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {60},
  number = {2},
  pages = {223--311},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/16M1080173},
  url = {https://epubs.siam.org/doi/10.1137/16M1080173},
  urldate = {2022-09-19},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  file = {/home/jonathan/Zotero/storage/57QW8YNH/Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf}
}

@book{boydConvexOptimization2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2004-03-08},
  eprint = {IUZdAAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.},
  isbn = {978-1-107-39400-1},
  langid = {english},
  pagetotal = {744},
  keywords = {background,optimization,theory,thesis},
  file = {/home/jonathan/Zotero/storage/I8Z26ZZ5/Boyd_Vandenberghe_2004_Convex Optimization.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-09-12},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  version = {4},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/ZFJVLUBD/Brown et al_2020_Language Models are Few-Shot Learners.pdf;/home/jonathan/Zotero/storage/Q6TQ82US/2005.html}
}

@inproceedings{brownLanguageModelsAre2020a,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2022-12-14},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {/home/jonathan/Zotero/storage/X7S8GYUC/Brown et al_2020_Language Models are Few-Shot Learners.pdf}
}

@book{capassoIntroductionContinuousTimeStochastic2012,
  title = {An {{Introduction}} to {{Continuous-Time Stochastic Processes}}: {{Theory}}, {{Models}}, and {{Applications}} to {{Finance}}, {{Biology}}, and {{Medicine}}},
  shorttitle = {An {{Introduction}} to {{Continuous-Time Stochastic Processes}}},
  author = {Capasso, Vincenzo and Bakstein, David},
  date = {2012-07-27},
  eprint = {h83lVwvgJnoC},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Expanding on the first edition of An Introduction to Continuous-Time Stochastic Processes, this concisely written book is a rigorous and self-contained introduction to the theory of continuous-time stochastic processes. A balance of theory and applications, the work features concrete examples of modeling real-world problems from biology, medicine, industrial applications, finance, and insurance using stochastic methods. No previous knowledge of stochastic processes is required.},
  isbn = {978-0-8176-8346-7},
  langid = {english},
  pagetotal = {438},
  keywords = {background,SDE,theory,thesis},
  file = {/home/jonathan/Zotero/storage/UI9X2DBV/Capasso_Bakstein_2012_An Introduction to Continuous-Time Stochastic Processes.pdf}
}

@inproceedings{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Choromanska, Anna and Henaff, MIkael and Mathieu, Michael and Arous, Gerard Ben and LeCun, Yann},
  date = {2015-02-21},
  pages = {192--204},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v38/choromanska15.html},
  urldate = {2022-09-05},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {nonconvexity,thesis,toread},
  file = {/home/jonathan/Zotero/storage/9V9GU2YZ/Choromanska et al_2015_The Loss Surfaces of Multilayer Networks.pdf}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  date = {2009-06},
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/home/jonathan/Zotero/storage/6JXMR36I/Deng et al_2009_ImageNet.pdf;/home/jonathan/Zotero/storage/C34WFZS9/5206848.html}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {61},
  pages = {2121--2159},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v12/duchi11a.html},
  urldate = {2022-12-04},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  file = {/home/jonathan/Zotero/storage/E9ZRV7U6/Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf}
}

@book{durrettProbabilityTheoryExamples2019,
  title = {Probability: {{Theory}} and {{Examples}}},
  shorttitle = {Probability},
  author = {Durrett, Rick},
  date = {2019-04-18},
  eprint = {b22MDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {This lively introduction to measure-theoretic probability theory covers laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. Concentrating on results that are the most useful for applications, this comprehensive treatment is a rigorous graduate text and reference. Operating under the philosophy that the best way to learn probability is to see it in action, the book contains extended examples that apply the theory to concrete applications. This fifth edition contains a new chapter on multidimensional Brownian motion and its relationship to partial differential equations (PDEs), an advanced topic that is finding new applications. Setting the foundation for this expansion, Chapter 7 now features a proof of Itô's formula. Key exercises that previously were simply proofs left to the reader have been directly inserted into the text as lemmas. The new edition re-instates discussion about the central limit theorem for martingales and stationary sequences.},
  isbn = {978-1-108-47368-2},
  langid = {english},
  pagetotal = {433},
  keywords = {background},
  file = {/home/jonathan/Zotero/storage/UKLZ9CJD/Durrett_2019_Probability.pdf}
}

@book{eAppliedStochasticAnalysis2021,
  title = {Applied {{Stochastic Analysis}}},
  author = {E, Weinan and Li, Tiejun and Vanden-Eijnden, Eric},
  date = {2021-09-22},
  eprint = {YVpQEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{American Mathematical Soc.}},
  abstract = {This is a textbook for advanced undergraduate students and beginning graduate students in applied mathematics. It presents the basic mathematical foundations of stochastic analysis (probability theory and stochastic processes) as well as some important practical tools and applications (e.g., the connection with differential equations, numerical methods, path integrals, random fields, statistical physics, chemical kinetics, and rare events). The book strikes a nice balance between mathematical formalism and intuitive arguments, a style that is most suited for applied mathematicians. Readers can learn both the rigorous treatment of stochastic analysis as well as practical applications in modeling and simulation. Numerous exercises nicely supplement the main exposition.},
  isbn = {978-1-4704-6569-8},
  langid = {english},
  pagetotal = {329},
  keywords = {background,thesis},
  file = {/home/jonathan/Zotero/storage/G3DGXFTY/E et al. - 2021 - Applied Stochastic Analysis.pdf}
}

@online{EladHazanNeurIPS,
  title = {Elad {{Hazan}} | {{NeurIPS}}},
  url = {https://videos.neurips.cc/topic/gradient%20descent/video/slideslive-38922823},
  urldate = {2022-10-28},
  file = {/home/jonathan/Zotero/storage/3HK9C9L7/slideslive-38922823.html}
}

@book{evansPartialDifferentialEquations2010,
  title = {Partial {{Differential Equations}}},
  author = {Evans, Lawrence C.},
  date = {2010},
  eprint = {Xnu0o_EJrCQC},
  eprinttype = {googlebooks},
  publisher = {{American Mathematical Soc.}},
  abstract = {This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, a significantly expanded bibliography.  About the First Edition: I have used this book for both regular PDE and topics courses. It has a wonderful combination of insight and technical detail...Evans' book is evidence of his mastering of the field and the clarity of presentation (Luis Caffarelli, University of Texas) It is fun to teach from Evans' book. It explains many of the essential ideas and techniques of partial differential equations ...Every graduate student in analysis should read it. (David Jerison, MIT) I use Partial Differential Equations to prepare my students for their Topic exam, which is a requirement before starting working on their dissertation. The book provides an excellent account of PDE's ...I am very happy with the preparation it provides my students. (Carlos Kenig, University of Chicago) Evans' book has already attained the status of a classic. It is a clear choice for students just learning the subject, as well as for experts who wish to broaden their knowledge ...An outstanding reference for many aspects of the field. (Rafe Mazzeo, Stanford University.},
  isbn = {978-0-8218-4974-3},
  langid = {english},
  pagetotal = {778},
  keywords = {book,pde},
  file = {/home/jonathan/Zotero/storage/LLUR3PVZ/Evans_2010_Partial Differential Equations.pdf}
}

@inproceedings{fontaineConvergenceRatesApproximation2021,
  title = {Convergence Rates and Approximation Results for {{SGD}} and Its Continuous-Time Counterpart},
  booktitle = {Proceedings of {{Thirty Fourth Conference}} on {{Learning Theory}}},
  author = {Fontaine, Xavier and Bortoli, Valentin De and Durmus, Alain},
  date = {2021-07-21},
  pages = {1965--2058},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v134/fontaine21a.html},
  urldate = {2022-09-05},
  abstract = {This paper proposes a thorough theoretical analysis of Stochastic Gradient Descent (SGD) with non-increasing step sizes.  First, we show that the recursion defining SGD can be provably approximated by solutions of a time inhomogeneous Stochastic Differential Equation (SDE) using an appropriate coupling. In the specific case of a batch noise we refine our results using recent advances in Stein’s method. Then, motivated by recent analyses of deterministic and stochastic optimization methods by their continuous counterpart, we study the long-time behavior of the continuous processes at hand and establish non-asymptotic bounds. To that purpose, we develop new comparison techniques which are of independent interest. Adapting these techniques to the discrete setting, we show that the same results hold for the corresponding SGD sequences.  In our analysis, we notably improve non-asymptotic bounds in the convex setting for SGD under weaker assumptions than the ones considered in previous works. Finally, we also establish finite-time convergence results under various conditions, including relaxations of the famous Ł\{ojasciewicz\} inequality, which can be applied to a class of non-convex functions.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/F5NSEG8L/Fontaine et al_2021_Convergence rates and approximation results for SGD and its continuous-time.pdf}
}

@book{freidlinRandomPerturbationsDynamical1998,
  title = {Random {{Perturbations}} of {{Dynamical Systems}}},
  author = {Freidlin, Mark I. and Freĭdlin, Mark Iosifovich and Ventcelʹ, Aleksandr D. and Wentzell, Alexander D.},
  date = {1998},
  eprint = {0yE74YEXpWEC},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {This text is a treatment of various kinds of limit theorems for stochastic processes defined as a result of random perturbations of dynamical systems. Apart from the long-time behaviour of the perturbed system, exit problems, metastable states, optimal stabilization, and asymptotics of stationary distributions are considered in detail. The author's main tools are the large deviation theory, the central limit theorem for stochastic processes, and the averaging principle. The results allow for explicit calculations of the asymptotics of many interesting characteristics of the perturbed system, and most of these results are closely conncected with PDE. This second edition contains expansions on the averaging principle, a new chapter on random perturbations of Hamiltonian systems, along with results on fast oscillating perturbations of systems with conservations laws. New sections on wave front propagation in semilinear PDE and on random perturbations of certain infinite-dimensional dynamical systems have been incorporated into the chapter on sharpenings and generalizations.},
  isbn = {978-0-387-98362-2},
  langid = {english},
  pagetotal = {448},
  file = {/home/jonathan/Zotero/storage/4N5CI6SI/Freidlin et al_1998_Random Perturbations of Dynamical Systems.pdf}
}

@unpublished{geoffreyhintonnitishsrivastavaandkevinswer-NeuralNetworksMachine2012,
  title = {Neural Networks for Machine Learning Lecture 6a Overview of Mini-Batch Gradient Descent},
  author = {{Geoffrey Hinton, Nitish Srivastava, and Kevin Swer-} and {sky}},
  date = {2012}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010-03-31},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate = {2022-12-04},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/587IPNDT/Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf}
}

@book{golubMatrixComputations1996,
  title = {Matrix {{Computations}}},
  author = {Golub, Gene H. and Loan, Charles F. Van and Loan, Charles F. Van and Golub},
  date = {1996-10-15},
  eprint = {mlOa7wPX6OYC},
  eprinttype = {googlebooks},
  publisher = {{JHU Press}},
  abstract = {Revised and updated, the third edition of Golub and Van Loan's classic text in computer science provides essential information about the mathematical background and algorithmic skills required for the production of numerical software. This new edition includes thoroughly revised chapters on matrix multiplication problems and parallel matrix computations, expanded treatment of CS decomposition, an updated overview of floating point arithmetic, a more accurate rendition of the modified Gram-Schmidt process, and new material devoted to GMRES, QMR, and other methods designed to handle the sparse unsymmetric linear system problem.},
  isbn = {978-0-8018-5414-9},
  langid = {english},
  pagetotal = {734}
}

@misc{goukRegularisationNeuralNetworks2020,
  title = {Regularisation of {{Neural Networks}} by {{Enforcing Lipschitz Continuity}}},
  author = {Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael J.},
  date = {2020-08-09},
  number = {arXiv:1804.04368},
  eprint = {1804.04368},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1804.04368},
  url = {http://arxiv.org/abs/1804.04368},
  urldate = {2022-12-08},
  abstract = {We investigate the effect of explicitly enforcing the Lipschitz continuity of neural networks with respect to their inputs. To this end, we provide a simple technique for computing an upper bound to the Lipschitz constant---for multiple \$p\$-norms---of a feed forward neural network composed of commonly used layer types. Our technique is then used to formulate training a neural network with a bounded Lipschitz constant as a constrained optimisation problem that can be solved using projected stochastic gradient methods. Our evaluation study shows that the performance of the resulting models exceeds that of models trained with other common regularisers. We also provide evidence that the hyperparameters are intuitive to tune, demonstrate how the choice of norm for computing the Lipschitz constant impacts the resulting model, and show that the performance gains provided by our method are particularly noticeable when only a small amount of training data is available.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/KHNXHGPL/Gouk et al_2020_Regularisation of Neural Networks by Enforcing Lipschitz Continuity.pdf;/home/jonathan/Zotero/storage/WCLWMRSD/1804.html}
}

@article{gowerConvergenceTheoremsGradient,
  title = {Convergence {{Theorems}} for {{Gradient Descent}}},
  author = {Gower, Robert M.},
  file = {/home/jonathan/Zotero/storage/Z63B7RJ3/Gower_Convergence Theorems for Gradient Descent.pdf}
}

@inproceedings{gowerSGDGeneralAnalysis2019,
  title = {{{SGD}}: {{General Analysis}} and {{Improved Rates}}},
  shorttitle = {{{SGD}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richtárik, Peter},
  date = {2019-05-24},
  pages = {5200--5209},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/qian19b.html},
  urldate = {2022-09-05},
  abstract = {We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form minibatches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/5JL58NLC/Gower et al. - 2019 - SGD General Analysis and Improved Rates.pdf;/home/jonathan/Zotero/storage/9QVY34YG/Gower et al_2019_SGD.pdf}
}

@misc{goyalAccurateLargeMinibatch2018,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  date = {2018-04-30},
  number = {arXiv:1706.02677},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.02677},
  url = {http://arxiv.org/abs/1706.02677},
  urldate = {2022-12-04},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/777HVGS6/Goyal et al_2018_Accurate, Large Minibatch SGD.pdf;/home/jonathan/Zotero/storage/RMACCGNI/1706.html}
}

@misc{granziolLearningRatesFunction2021,
  title = {Learning {{Rates}} as a {{Function}} of {{Batch Size}}: {{A Random Matrix Theory Approach}} to {{Neural Network Training}}},
  shorttitle = {Learning {{Rates}} as a {{Function}} of {{Batch Size}}},
  author = {Granziol, Diego and Zohren, Stefan and Roberts, Stephen},
  date = {2021-11-05},
  number = {arXiv:2006.09092},
  eprint = {2006.09092},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.09092},
  url = {http://arxiv.org/abs/2006.09092},
  urldate = {2022-12-06},
  abstract = {We study the effect of mini-batching on the loss landscape of deep neural networks using spiked, field-dependent random matrix theory. We demonstrate that the magnitude of the extremal values of the batch Hessian are larger than those of the empirical Hessian. We also derive similar results for the Generalised Gauss-Newton matrix approximation of the Hessian. As a consequence of our theorems we derive an analytical expressions for the maximal learning rates as a function of batch size, informing practical training regimens for both stochastic gradient descent (linear scaling) and adaptive algorithms, such as Adam (square root scaling), for smooth, non-convex deep neural networks. Whilst the linear scaling for stochastic gradient descent has been derived under more restrictive conditions, which we generalise, the square root scaling rule for adaptive optimisers is, to our knowledge, completely novel. \%For stochastic second-order methods and adaptive methods, we derive that the minimal damping coefficient is proportional to the ratio of the learning rate to batch size. We validate our claims on the VGG/WideResNet architectures on the CIFAR-\$100\$ and ImageNet datasets. Based on our investigations of the sub-sampled Hessian we develop a stochastic Lanczos quadrature based on the fly learning rate and momentum learner, which avoids the need for expensive multiple evaluations for these key hyper-parameters and shows good preliminary results on the Pre-Residual Architecure for CIFAR-\$100\$.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/NS7NR52P/Granziol et al_2021_Learning Rates as a Function of Batch Size.pdf;/home/jonathan/Zotero/storage/KYXFYS4H/2006.html}
}

@inproceedings{gurbuzbalabanHeavyTailPhenomenonSGD2021,
  title = {The {{Heavy-Tail Phenomenon}} in {{SGD}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  date = {2021-07-01},
  pages = {3964--3975},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/gurbuzbalaban21a.html},
  urldate = {2022-09-21},
  abstract = {In recent years, various notions of capacity and complexity have been proposed for characterizing the generalization properties of stochastic gradient descent (SGD) in deep learning. Some of the popular notions that correlate well with the performance on unseen data are (i) the ‘flatness’ of the local minimum found by SGD, which is related to the eigenvalues of the Hessian, (ii) the ratio of the stepsize ηη\textbackslash eta to the batch-size bbb, which essentially controls the magnitude of the stochastic gradient noise, and (iii) the ‘tail-index’, which measures the heaviness of the tails of the network weights at convergence. In this paper, we argue that these three seemingly unrelated perspectives for generalization are deeply linked to each other. We claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters ηη\textbackslash eta and bbb, the SGD iterates will converge to a \textbackslash emph\{heavy-tailed\} stationary distribution. We rigorously prove this claim in the setting of quadratic optimization: we show that even in a simple linear regression problem with independent and identically distributed data whose distribution has finite moments of all order, the iterates can be heavy-tailed with infinite variance. We further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. We then translate our results into insights about the behavior of SGD in deep learning. We support our theory with experiments conducted on synthetic data, fully connected, and convolutional neural networks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/CQJFEJGU/Gurbuzbalaban et al_2021_The Heavy-Tail Phenomenon in SGD.pdf;/home/jonathan/Zotero/storage/Q8LU9RL6/Gurbuzbalaban et al. - 2021 - The Heavy-Tail Phenomenon in SGD.pdf}
}

@book{hairerGeometricNumericalIntegration2013,
  title = {Geometric {{Numerical Integration}}: {{Structure-Preserving Algorithms}} for {{Ordinary Differential Equations}}},
  shorttitle = {Geometric {{Numerical Integration}}},
  author = {Hairer, Ernst and Lubich, Christian and Wanner, Gerhard},
  date = {2013-03-09},
  eprint = {cPTxCAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Numerical methods that preserve properties of Hamiltonian systems, reversible systems, differential equations on manifolds and problems with highly oscillatory solutions are the subject of this book. A complete self-contained theory of symplectic and symmetric methods, which include Runge-Kutta, composition, splitting, multistep and various specially designed integrators, is presented and their construction and practical merits are discussed. The long-time behaviour of the numerical solutions is studied using a backward error analysis (modified equations) combined with KAM theory. The book is illustrated by many figures, it treats applications from physics and astronomy and contains many numerical experiments and comparisons of different approaches.},
  isbn = {978-3-662-05018-7},
  langid = {english},
  pagetotal = {526},
  file = {/home/jonathan/Zotero/storage/U37UJ7I9/Hairer et al_2013_Geometric Numerical Integration.pdf}
}

@article{hardtGradientDescentLearns2018,
  title = {Gradient {{Descent Learns Linear Dynamical Systems}}},
  author = {Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {19},
  number = {29},
  pages = {1--44},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v19/16-465.html},
  urldate = {2022-09-20},
  abstract = {We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.},
  keywords = {SGD,toread},
  file = {/home/jonathan/Zotero/storage/MACHQHNK/Hardt et al_2018_Gradient Descent Learns Linear Dynamical Systems.pdf}
}

@inproceedings{heDelvingDeepRectifiers2015a,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12},
  pages = {1026--1034},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2015.123},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  file = {/home/jonathan/Zotero/storage/XUUPBNZ2/He et al_2015_Delving Deep into Rectifiers.pdf;/home/jonathan/Zotero/storage/QM8P6TCZ/7410480.html}
}

@misc{jastrzebskiThreeFactorsInfluencing2018,
  title = {Three {{Factors Influencing Minima}} in {{SGD}}},
  author = {Jastrzębski, Stanisław and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  date = {2018-09-13},
  number = {arXiv:1711.04623},
  eprint = {1711.04623},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.04623},
  url = {http://arxiv.org/abs/1711.04623},
  urldate = {2022-11-02},
  abstract = {We investigate the dynamical and convergent properties of stochastic gradient descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization, remains an open question. In order to tackle this problem we investigate the previously proposed approximation of SGD by a stochastic differential equation (SDE). We theoretically argue that three factors - learning rate, batch size and gradient covariance - influence the minima found by SGD. In particular we find that the ratio of learning rate to batch size is a key determinant of SGD dynamics and of the width of the final minima, and that higher values of the ratio lead to wider minima and often better generalization. We confirm these findings experimentally. Further, we include experiments which show that learning rate schedules can be replaced with batch size schedules and that the ratio of learning rate to batch size is an important factor influencing the memorization process.},
  archiveprefix = {arXiv},
  keywords = {sde,sgd},
  file = {/home/jonathan/Zotero/storage/TCZIFLJI/Jastrzębski et al_2018_Three Factors Influencing Minima in SGD.pdf;/home/jonathan/Zotero/storage/S752SZZ6/1711.html}
}

@inproceedings{johnsonAcceleratingStochasticGradient2013,
  title = {Accelerating {{Stochastic Gradient Descent}} Using {{Predictive Variance Reduction}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Johnson, Rie and Zhang, Tong},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html},
  urldate = {2022-09-05},
  abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we  prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG).  However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
  keywords = {SGD,SVRG,thesis,toread},
  file = {/home/jonathan/Zotero/storage/IVD968QK/Johnson_Zhang_2013_Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.pdf}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  date = {2021-08},
  journaltitle = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  urldate = {2022-09-12},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50~years9. Despite recent progress10–14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  issue = {7873},
  langid = {english},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/3XWK5JAZ/Jumper et al_2021_Highly accurate protein structure prediction with AlphaFold.pdf;/home/jonathan/Zotero/storage/5SSC5GX4/s41586-021-03819-2.html}
}

@misc{kawaguchiGeneralizationDeepLearning2020,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  date = {2020-07-27},
  number = {arXiv:1710.05468},
  eprint = {1710.05468},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.05468},
  url = {http://arxiv.org/abs/1710.05468},
  urldate = {2022-09-22},
  abstract = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.},
  archiveprefix = {arXiv},
  keywords = {SGD},
  file = {/home/jonathan/Zotero/storage/52EUC9LU/Kawaguchi et al_2020_Generalization in Deep Learning.pdf;/home/jonathan/Zotero/storage/7LVYN9Y7/1710.html}
}

@misc{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  date = {2017-02-09},
  number = {arXiv:1609.04836},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.04836},
  url = {http://arxiv.org/abs/1609.04836},
  urldate = {2022-09-05},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arXiv},
  keywords = {magnitude,noise,thesis},
  file = {/home/jonathan/Zotero/storage/WC8MQ9VK/Keskar et al_2017_On Large-Batch Training for Deep Learning.pdf;/home/jonathan/Zotero/storage/DV8AFCPB/1609.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2022-12-04},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/BVHXFACA/Kingma_Ba_2017_Adam.pdf;/home/jonathan/Zotero/storage/H89Y849R/1412.html}
}

@book{kloedenNumericalSolutionStochastic2013,
  title = {Numerical {{Solution}} of {{Stochastic Differential Equations}}},
  author = {Kloeden, Peter E. and Platen, Eckhard},
  date = {2013-04-17},
  eprint = {r9r6CAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The aim of this book is to provide an accessible introduction to stochastic differ ential equations and their applications together with a systematic presentation of methods available for their numerical solution. During the past decade there has been an accelerating interest in the de velopment of numerical methods for stochastic differential equations (SDEs). This activity has been as strong in the engineering and physical sciences as it has in mathematics, resulting inevitably in some duplication of effort due to an unfamiliarity with the developments in other disciplines. Much of the reported work has been motivated by the need to solve particular types of problems, for which, even more so than in the deterministic context, specific methods are required. The treatment has often been heuristic and ad hoc in character. Nevertheless, there are underlying principles present in many of the papers, an understanding of which will enable one to develop or apply appropriate numerical schemes for particular problems or classes of problems.},
  isbn = {978-3-662-12616-5},
  langid = {english},
  pagetotal = {666},
  keywords = {background,thesis},
  file = {/home/jonathan/Zotero/storage/GSQ3ATQC/sde_theory.djvu}
}

@article{knuthBigOmicronBig1976,
  title = {Big {{Omicron}} and Big {{Omega}} and Big {{Theta}}},
  author = {Knuth, Donald E.},
  date = {1976-04-01},
  journaltitle = {ACM SIGACT News},
  shortjournal = {SIGACT News},
  volume = {8},
  number = {2},
  pages = {18--24},
  issn = {0163-5700},
  doi = {10.1145/1008328.1008329},
  url = {https://doi.org/10.1145/1008328.1008329},
  urldate = {2022-12-13},
  file = {/home/jonathan/Zotero/storage/K4YE9XPT/Knuth_1976_Big Omicron and big Omega and big Theta.pdf}
}

@book{koenigsbergerAnalysis2006,
  title = {Analysis 2},
  author = {Königsberger, Konrad},
  date = {2006-07-16},
  eprint = {x9qbBgAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer-Verlag}},
  abstract = {Dieser zweite Band Analysis, der nunmehr in fünfter, korrigierter Auflage vorliegt, behandelt die Differential- und Integralrechnung im Rn sowie Differentialgleichungen und Elemente der Funktionentheorie. Zu seinen Besonderheiten gehören eine neue, einfache Einführung des Lebesgueintegrals und eine Version des Gaußschen Integralsatzes, die Integrationsbereiche in großer Allgemeinheit zugrunde legt. Ein umfangreiches Kapitel ist dem Kalkül der Differentialformen samt Satz von Stokes gewidmet und als Einstieg in die Theorie der differenzierbaren Mannigfaltigkeiten konzipiert. Historische und biographische Anmerkungen bereichern den Text. Zahlreiche Abbildungen und Beispiele unterstützen das Verständnis. Zu jedem Kapitel wird eine Reihe von Aufgaben bereitgestellt. Insgesamt ein Lehrbuch, das sich als Begleittext zu einer Vorlesung wie auch zum Selbststudium hervorragend eignet.},
  isbn = {978-3-540-35077-4},
  langid = {ngerman},
  pagetotal = {471}
}

@inreference{KolmogorovSmirnovTest2022,
  title = {Kolmogorov–{{Smirnov}} Test},
  booktitle = {Wikipedia},
  date = {2022-06-10T10:54:21Z},
  url = {https://en.wikipedia.org/w/index.php?title=Kolmogorov%E2%80%93Smirnov_test&oldid=1092451253},
  urldate = {2022-09-26},
  abstract = {In statistics, the Kolmogorov–Smirnov test (K-S test or KS test) is a nonparametric test of the equality of continuous (or discontinuous, see Section 2.2), one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test).   In essence, the test answers the question "What is the probability that this collection of samples could have been drawn from that probability distribution?" or, in the second case, "What is the probability that these two sets of samples were drawn from the same (but unknown) probability distribution?". It is named after  Andrey Kolmogorov and Nikolai Smirnov. The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the sample is drawn from the reference distribution (in the one-sample case) or that the samples are drawn from the same distribution (in the two-sample case). In the one-sample case, the distribution considered under the null hypothesis may be continuous (see Section 2), purely discrete or mixed (see Section 2.2). In the two-sample case (see Section 3), the distribution considered under the null hypothesis is a continuous distribution but is otherwise unrestricted. However, the two sample test can also be performed under more general conditions that allow for discontinuity, heterogeneity and dependence across samples.The two-sample K–S test is one of the most useful and general nonparametric methods for comparing two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples. The Kolmogorov–Smirnov test can be modified to serve as a goodness of fit test. In the special case of testing for normality of the distribution, samples are standardized and compared with a standard normal distribution. This is equivalent to setting the mean and variance of the reference distribution equal to the sample estimates, and it is known that using these to define the specific reference distribution changes the null distribution of the test statistic (see Test with estimated parameters). Various studies have found that, even in this corrected form, the test is less powerful for testing normality than the Shapiro–Wilk test or Anderson–Darling test. However, these other tests have their own disadvantages. For instance the Shapiro–Wilk test is known not to work well in samples with many identical values.},
  langid = {english},
  annotation = {Page Version ID: 1092451253},
  file = {/home/jonathan/Zotero/storage/VB2EFUDH/Kolmogorov–Smirnov_test.html}
}

@misc{krizhevskyOneWeirdTrick2014,
  title = {One Weird Trick for Parallelizing Convolutional Neural Networks},
  author = {Krizhevsky, Alex},
  date = {2014-04-26},
  number = {arXiv:1404.5997},
  eprint = {1404.5997},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1404.5997},
  url = {http://arxiv.org/abs/1404.5997},
  urldate = {2022-12-09},
  abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/XQNNZ76T/Krizhevsky_2014_One weird trick for parallelizing convolutional neural networks.pdf;/home/jonathan/Zotero/storage/P8P7IRW7/1404.html}
}

@book{kushnerStochasticApproximationRecursive2003,
  title = {Stochastic {{Approximation}} and {{Recursive Algorithms}} and {{Applications}}},
  author = {Kushner, Harold and Yin, G. George},
  date = {2003-07-17},
  eprint = {EC2w1SaPb7YC},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The basic stochastic approximation algorithms introduced by Robbins and MonroandbyKieferandWolfowitzintheearly1950shavebeenthesubject of an enormous literature, both theoretical and applied. This is due to the large number of applications and the interesting theoretical issues in the analysis of “dynamically de?ned” stochastic processes. The basic paradigm is a stochastic di?erence equation such as ? = ? + Y , where ? takes n+1 n n n n its values in some Euclidean space, Y is a random variable, and the “step n size” {$>$} 0 is small and might go to zero as n??. In its simplest form, n ? is a parameter of a system, and the random vector Y is a function of n “noise-corrupted” observations taken on the system when the parameter is set to ? . One recursively adjusts the parameter so that some goal is met n asymptotically. Thisbookisconcernedwiththequalitativeandasymptotic properties of such recursive algorithms in the diverse forms in which they arise in applications. There are analogous continuous time algorithms, but the conditions and proofs are generally very close to those for the discrete time case. The original work was motivated by the problem of ?nding a root of a continuous function g ̄(?), where the function is not known but the - perimenter is able to take “noisy” measurements at any desired value of ?. Recursive methods for root ?nding are common in classical numerical analysis, and it is reasonable to expect that appropriate stochastic analogs would also perform well.},
  isbn = {978-0-387-00894-3},
  langid = {english},
  pagetotal = {485}
}

@article{latzAnalysisStochasticGradient2021,
  title = {Analysis of Stochastic Gradient Descent in Continuous Time},
  author = {Latz, Jonas},
  date = {2021-05-09},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {31},
  number = {4},
  pages = {39},
  issn = {1573-1375},
  doi = {10.1007/s11222-021-10016-8},
  url = {https://doi.org/10.1007/s11222-021-10016-8},
  urldate = {2022-10-06},
  abstract = {Stochastic gradient descent is an optimisation method that combines classical gradient descent with random subsampling within the target functional. In this work, we introduce the stochastic gradient process as a continuous-time representation of stochastic gradient descent. The stochastic gradient process is a dynamical system that is coupled with a continuous-time Markov process living on a finite state space. The dynamical system—a gradient flow—represents the gradient descent part, the process on the finite state space represents the random subsampling. Processes of this type are, for instance, used to model clonal populations in fluctuating environments. After introducing it, we study theoretical properties of the stochastic gradient process: We show that it converges weakly to the gradient flow with respect to the full target function, as the learning rate approaches zero. We give conditions under which the stochastic gradient process with constant learning rate is exponentially ergodic in the Wasserstein sense. Then we study the case, where the learning rate goes to zero sufficiently slowly and the single target functions are strongly convex. In this case, the process converges weakly to the point mass concentrated in the global minimum of the full target function; indicating consistency of the method. We conclude after a discussion of discretisation strategies for the stochastic gradient process and numerical experiments.},
  langid = {english},
  keywords = {SDE,SGD},
  file = {/home/jonathan/Zotero/storage/T8J5L2X9/Latz_2021_Analysis of stochastic gradient descent in continuous time.pdf}
}

@article{latzAnalysisStochasticGradient2021a,
  title = {Analysis of Stochastic Gradient Descent in Continuous Time},
  author = {Latz, Jonas},
  date = {2021-05-09},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {31},
  number = {4},
  pages = {39},
  issn = {1573-1375},
  doi = {10.1007/s11222-021-10016-8},
  url = {https://doi.org/10.1007/s11222-021-10016-8},
  urldate = {2022-12-03},
  abstract = {Stochastic gradient descent is an optimisation method that combines classical gradient descent with random subsampling within the target functional. In this work, we introduce the stochastic gradient process as a continuous-time representation of stochastic gradient descent. The stochastic gradient process is a dynamical system that is coupled with a continuous-time Markov process living on a finite state space. The dynamical system—a gradient flow—represents the gradient descent part, the process on the finite state space represents the random subsampling. Processes of this type are, for instance, used to model clonal populations in fluctuating environments. After introducing it, we study theoretical properties of the stochastic gradient process: We show that it converges weakly to the gradient flow with respect to the full target function, as the learning rate approaches zero. We give conditions under which the stochastic gradient process with constant learning rate is exponentially ergodic in the Wasserstein sense. Then we study the case, where the learning rate goes to zero sufficiently slowly and the single target functions are strongly convex. In this case, the process converges weakly to the point mass concentrated in the global minimum of the full target function; indicating consistency of the method. We conclude after a discussion of discretisation strategies for the stochastic gradient process and numerical experiments.},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/FW6WZIYI/Latz_2021_Analysis of stochastic gradient descent in continuous time.pdf}
}

@inproceedings{leeDeepNeuralNetworks2022,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  date = {2022-03-30},
  url = {https://openreview.net/forum?id=B1EA-M-0Z},
  urldate = {2022-12-04},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide, deep, networks and GPs with a particular covariance function. We further develop a computationally efficient pipeline to compute this covariance function. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We observe that the trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and that the GP-based predictions typically outperform those of finite-width networks. Finally we connect the prior distribution over weights and variances in our GP formulation to the recent development of signal propagation in random neural networks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/7MFNQDPB/Lee et al_2022_Deep Neural Networks as Gaussian Processes.pdf;/home/jonathan/Zotero/storage/WNYHHZXX/forum.html}
}

@inproceedings{leeGradientDescentOnly2016,
  title = {Gradient {{Descent Only Converges}} to {{Minimizers}}},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  date = {2016-06-06},
  pages = {1246--1257},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v49/lee16.html},
  urldate = {2022-09-05},
  abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  keywords = {convergence,GD,theory,thesis},
  file = {/home/jonathan/Zotero/storage/MMWYWT6Z/Lee et al_2016_Gradient Descent Only Converges to Minimizers.pdf}
}

@inproceedings{leeWideNeuralNetworks2019,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2019/hash/0d1a9651497a38d8b1c3871c84528bd4-Abstract.html},
  urldate = {2022-12-04},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  file = {/home/jonathan/Zotero/storage/LPZLP844/Lee et al_2019_Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.pdf}
}

@inproceedings{liConvergenceStochasticGradient2019,
  title = {On the {{Convergence}} of {{Stochastic Gradient Descent}} with {{Adaptive Stepsizes}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Li, Xiaoyu and Orabona, Francesco},
  date = {2019-04-11},
  pages = {983--992},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/li19c.html},
  urldate = {2022-09-05},
  abstract = {Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice of the stepsizes. This has motivated a large body of research on adaptive stepsizes. However, there is currently a gap in our theoretical understanding of these methods, especially in the non-convex setting. In this paper, we start closing this gap: we theoretically analyze in the convex and non-convex settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions for these stepsizes to achieve almost sure asymptotic convergence of the gradients to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex setting. Moreover, we show that these stepsizes allow to automatically adapt to the level of noise of the stochastic gradients in both the convex and non-convex settings, interpolating between O(1/T) and O(1/sqrt(T)), up to logarithmic terms.},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {convergence,SGD,stepsize,thesis,toread},
  file = {/home/jonathan/Zotero/storage/PH74N3RG/Li and Orabona - 2019 - On the Convergence of Stochastic Gradient Descent .pdf;/home/jonathan/Zotero/storage/S7U8TV3J/Li_Orabona_2019_On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes.pdf}
}

@inproceedings{liExplainingRegularizationEffect2019,
  title = {Towards {{Explaining}} the {{Regularization Effect}} of {{Initial Large Learning Rate}} in {{Training Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html},
  urldate = {2022-09-08},
  abstract = {Stochastic gradient descent with a large initial learning rate is widely used for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon, we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes low-noise, hard-to-fit patterns, it generalizes worse on hard-to-generalize, easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate, but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer, as it relies too much on the patch early on.},
  keywords = {normalization,thesis},
  file = {/home/jonathan/Zotero/storage/5WR7JDSF/Li et al_2019_Towards Explaining the Regularization Effect of Initial Large Learning Rate in.pdf}
}

@misc{liExponentialLearningRate2019,
  title = {An {{Exponential Learning Rate Schedule}} for {{Deep Learning}}},
  author = {Li, Zhiyuan and Arora, Sanjeev},
  date = {2019-11-21},
  number = {arXiv:1910.07454},
  eprint = {1910.07454},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.07454},
  url = {http://arxiv.org/abs/1910.07454},
  urldate = {2022-08-22},
  abstract = {Intriguing empirical evidence exists that deep learning can work well with exoticschedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN, which is ubiquitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN. 1. Training can be done using SGD with momentum and an exponentially increasing learning rate schedule, i.e., learning rate increases by some \$(1 +\textbackslash alpha)\$ factor in every epoch for some \$\textbackslash alpha {$>$}0\$. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As expected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization. 2. Mathematical explanation of the success of the above rate schedule: a rigorous proof that it is equivalent to the standard setting of BN + SGD + StandardRate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization, LayerNormalization, Instance Norm, etc. 3. A worked-out toy example illustrating the above linkage of hyper-parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.},
  archiveprefix = {arXiv},
  keywords = {normalization,thesis,toread},
  file = {/home/jonathan/Zotero/storage/A8RDPUPW/Li and Arora - 2019 - An Exponential Learning Rate Schedule for Deep Lea.pdf;/home/jonathan/Zotero/storage/37RXGWXR/1910.html}
}

@misc{liReconcilingModernDeep2020a,
  title = {Reconciling {{Modern Deep Learning}} with {{Traditional Optimization Analyses}}: {{The Intrinsic Learning Rate}}},
  shorttitle = {Reconciling {{Modern Deep Learning}} with {{Traditional Optimization Analyses}}},
  author = {Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  date = {2020-10-06},
  number = {arXiv:2010.02916},
  eprint = {2010.02916},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.02916},
  url = {http://arxiv.org/abs/2010.02916},
  urldate = {2022-11-24},
  abstract = {Recent works (e.g., (Li and Arora, 2020)) suggest that the use of popular normalization schemes (including Batch Normalization) in today's deep learning can move it far from a traditional optimization viewpoint, e.g., use of exponentially increasing learning rates. The current paper highlights other ways in which behavior of normalized nets departs from traditional viewpoints, and then initiates a formal framework for studying their mathematics via suitable adaptation of the conventional framework namely, modeling SGD-induced training trajectory via a suitable stochastic differential equation (SDE) with a noise term that captures gradient noise. This yields: (a) A new ' intrinsic learning rate' parameter that is the product of the normal learning rate and weight decay factor. Analysis of the SDE shows how the effective speed of learning varies and equilibrates over time under the control of intrinsic LR. (b) A challenge -- via theory and experiments -- to popular belief that good generalization requires large learning rates at the start of training. (c) New experiments, backed by mathematical intuition, suggesting the number of steps to equilibrium (in function space) scales as the inverse of the intrinsic learning rate, as opposed to the exponential time convergence bound implied by SDE analysis. We name it the Fast Equilibrium Conjecture and suggest it holds the key to why Batch Normalization is effective.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/FFLVM75V/Li et al_2020_Reconciling Modern Deep Learning with Traditional Optimization Analyses.pdf;/home/jonathan/Zotero/storage/4WF2IRPT/2010.html}
}

@misc{liStochasticModifiedEquations2017,
  title = {Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms},
  author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
  date = {2017-06-20},
  number = {arXiv:1511.06251},
  eprint = {1511.06251},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.06251},
  url = {http://arxiv.org/abs/1511.06251},
  urldate = {2022-08-19},
  abstract = {We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.},
  archiveprefix = {arXiv},
  keywords = {background,SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/4Q6EBRI6/Li et al. - 2017 - Stochastic modified equations and adaptive stochas.pdf;/home/jonathan/Zotero/storage/WVBZ3XYB/1511.html}
}

@article{liStochasticModifiedEquations2019,
  title = {Stochastic {{Modified Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I Mathematical Foundations}}},
  shorttitle = {Stochastic {{Modified Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I}}},
  author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {40},
  pages = {1--47},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/17-526.html},
  urldate = {2022-08-19},
  abstract = {We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting.},
  keywords = {SDE,SGD,theory,thesis},
  file = {/home/jonathan/Zotero/storage/CBBRN2E7/Li et al. - 2019 - Stochastic Modified Equations and Dynamics of Stoc.pdf}
}

@misc{liValidityModelingSGD2021,
  title = {On the {{Validity}} of {{Modeling SGD}} with {{Stochastic Differential Equations}} ({{SDEs}})},
  author = {Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
  date = {2021-06-16},
  number = {arXiv:2102.12470},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.12470},
  url = {http://arxiv.org/abs/2102.12470},
  urldate = {2022-08-17},
  abstract = {It is generally recognized that finite learning rate (LR), in contrast to infinitesimal LR, is important for good generalization in real-life deep nets. Most attempted explanations propose approximating finite-LR SGD with Ito Stochastic Differential Equations (SDEs), but formal justification for this approximation (e.g., (Li et al., 2019)) only applies to SGD with tiny LR. Experimental verification of the approximation appears computationally infeasible. The current paper clarifies the picture with the following contributions: (a) An efficient simulation algorithm SVAG that provably converges to the conventionally used Ito SDE approximation. (b) A theoretically motivated testable necessary condition for the SDE approximation and its most famous implication, the linear scaling rule (Goyal et al., 2017), to hold. (c) Experiments using this simulation to demonstrate that the previously proposed SDE approximation can meaningfully capture the training and generalization properties of common deep nets.},
  keywords = {paper,SDE,SGD,SVAG,thesis},
  file = {/home/jonathan/Zotero/storage/T4H6WURH/Li et al. - 2021 - On the Validity of Modeling SGD with Stochastic Di.pdf;/home/jonathan/Zotero/storage/UR38RDHX/2102.html}
}

@misc{liWhatHappensSGD2022,
  title = {What {{Happens}} after {{SGD Reaches Zero Loss}}? --{{A Mathematical Framework}}},
  shorttitle = {What {{Happens}} after {{SGD Reaches Zero Loss}}?},
  author = {Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  date = {2022-07-28},
  number = {arXiv:2110.06914},
  eprint = {2110.06914},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.06914},
  url = {http://arxiv.org/abs/2110.06914},
  urldate = {2022-08-26},
  abstract = {Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function \$L\$ can form a manifold. Intuitively, with a sufficiently small learning rate \$\textbackslash eta\$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such a regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, \$\textbackslash mathrm\{tr\}[\textbackslash nabla\^2 L]\$. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold -- i.e., the "implicit bias" -- using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a global analysis of the implicit bias valid for \$\textbackslash eta\^\{-2\}\$ steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for \$\textbackslash eta\^\{-1.6\}\$ steps and (2) allowing arbitrary noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires \$O(\textbackslash kappa\textbackslash ln d)\$ samples for learning an \$\textbackslash kappa\$-sparse overparametrized linear model in \$\textbackslash mathbb\{R\}\^d\$ (Woodworth et al., 2020), while GD initialized in the kernel regime requires \$\textbackslash Omega(d)\$ samples. This upper bound is minimax optimal and improves the previous \$\textbackslash tilde\{O\}(\textbackslash kappa\^2)\$ upper bound (HaoChen et al., 2020).},
  archiveprefix = {arXiv},
  keywords = {SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/NRWKRCPX/Li et al_2022_What Happens after SGD Reaches Zero Loss.pdf;/home/jonathan/Zotero/storage/PNPHABKM/2110.html}
}

@book{lycheNumericalLinearAlgebra2020,
  title = {Numerical {{Linear Algebra}} and {{Matrix Factorizations}}},
  author = {Lyche, Tom},
  date = {2020-03-03},
  eprint = {e9tKywEACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer International Publishing}},
  abstract = {After reading this book, students should be able to analyze computational problems in linear algebra such as linear systems, least squares- and eigenvalue problems, and to develop their own algorithms for solving them.  Since these problems can be large and difficult to handle, much can be gained by understanding and taking advantage of special structures. This in turn requires a good grasp of basic numerical linear algebra and matrix factorizations. Factoring a matrix into a product of simpler matrices is a crucial tool in numerical linear algebra, because it allows us to tackle complex problems by solving a sequence of easier ones.  The main characteristics of this book are as follows:  It is self-contained, only assuming that readers have completed first-year calculus and an introductory course on linear algebra, and that they have some experience with solving mathematical problems on a computer. The book provides detailed proofs of virtually all results. Further, its respective parts can be used independently, making it suitable for self-study.  The book consists of 15 chapters, divided into five thematically oriented parts. The chapters are designed for a one-week-per-chapter, one-semester course. To facilitate self-study, an introductory chapter includes a brief review of linear algebra.},
  isbn = {978-3-030-36467-0},
  langid = {english},
  pagetotal = {371}
}

@incollection{lycheSingularValueDecomposition2020,
  title = {The {{Singular Value Decomposition}}},
  booktitle = {Numerical {{Linear Algebra}} and {{Matrix Factorizations}}},
  author = {Lyche, Tom},
  editor = {Lyche, Tom},
  date = {2020},
  series = {Texts in {{Computational Science}} and {{Engineering}}},
  pages = {153--168},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-36468-7_7},
  url = {https://doi.org/10.1007/978-3-030-36468-7_7},
  urldate = {2022-12-13},
  abstract = {The singular value decomposition and the reduced form called the singular value factorization are useful both for theory and practice. Some of their applications include solving over-determined equations, principal component analysis in statistics, numerical determination of the rank of a matrix, algorithms used in search engines, and the theory of matrices.},
  isbn = {978-3-030-36468-7},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/2H2EPAN2/Lyche_2020_The Singular Value Decomposition.pdf}
}

@misc{malladiSDEsScalingRules2022a,
  title = {On the {{SDEs}} and {{Scaling Rules}} for {{Adaptive Gradient Algorithms}}},
  author = {Malladi, Sadhika and Lyu, Kaifeng and Panigrahi, Abhishek and Arora, Sanjeev},
  date = {2022-05-20},
  number = {arXiv:2205.10287},
  eprint = {2205.10287},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10287},
  url = {http://arxiv.org/abs/2205.10287},
  urldate = {2022-12-06},
  abstract = {Approximating Stochastic Gradient Descent (SGD) as a Stochastic Differential Equation (SDE) has allowed researchers to enjoy the benefits of studying a continuous optimization trajectory while carefully preserving the stochasticity of SGD. Analogous study of adaptive gradient methods, such as RMSprop and Adam, has been challenging because there were no rigorously proven SDE approximations for these methods. This paper derives the SDE approximations for RMSprop and Adam, giving theoretical guarantees of their correctness as well as experimental validation of their applicability to common large-scaling vision and language settings. A key practical result is the derivation of a \$\textbackslash textit\{square root scaling rule\}\$ to adjust the optimization hyperparameters of RMSprop and Adam when changing batch size, and its empirical validation in deep learning settings.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/GCXJGM5T/Malladi et al_2022_On the SDEs and Scaling Rules for Adaptive Gradient Algorithms.pdf;/home/jonathan/Zotero/storage/GYB85S2E/2205.html}
}

@misc{malladiSDEsScalingRules2022b,
  title = {On the {{SDEs}} and {{Scaling Rules}} for {{Adaptive Gradient Algorithms}}},
  author = {Malladi, Sadhika and Lyu, Kaifeng and Panigrahi, Abhishek and Arora, Sanjeev},
  date = {2022-05-20},
  number = {arXiv:2205.10287},
  eprint = {2205.10287},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10287},
  url = {http://arxiv.org/abs/2205.10287},
  urldate = {2022-12-08},
  abstract = {Approximating Stochastic Gradient Descent (SGD) as a Stochastic Differential Equation (SDE) has allowed researchers to enjoy the benefits of studying a continuous optimization trajectory while carefully preserving the stochasticity of SGD. Analogous study of adaptive gradient methods, such as RMSprop and Adam, has been challenging because there were no rigorously proven SDE approximations for these methods. This paper derives the SDE approximations for RMSprop and Adam, giving theoretical guarantees of their correctness as well as experimental validation of their applicability to common large-scaling vision and language settings. A key practical result is the derivation of a \$\textbackslash textit\{square root scaling rule\}\$ to adjust the optimization hyperparameters of RMSprop and Adam when changing batch size, and its empirical validation in deep learning settings.},
  archiveprefix = {arXiv},
  keywords = {application},
  file = {/home/jonathan/Zotero/storage/5SURDANW/Malladi et al_2022_On the SDEs and Scaling Rules for Adaptive Gradient Algorithms.pdf;/home/jonathan/Zotero/storage/59MIGC23/2205.html}
}

@inproceedings{maPowerInterpolationUnderstanding2018,
  title = {The {{Power}} of {{Interpolation}}: {{Understanding}} the {{Effectiveness}} of {{SGD}} in {{Modern Over-parametrized Learning}}},
  shorttitle = {The {{Power}} of {{Interpolation}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  date = {2018-07-03},
  pages = {3325--3334},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/ma18a.html},
  urldate = {2022-09-07},
  abstract = {In this paper we aim to formally explain the phenomenon of fast convergence of Stochastic Gradient Descent (SGD) observed in modern machine learning. The key observation is that most modern learning architectures are over-parametrized and are trained to interpolate the data by driving the empirical loss (classification and regression) close to zero. While it is still unclear why these interpolated solutions perform well on test data, we show that these regimes allow for fast convergence of SGD, comparable in number of iterations to full gradient descent. For convex loss functions we obtain an exponential convergence bound for mini-batch SGD parallel to that for full gradient descent. We show that there is a critical batch size m∗m∗m\^* such that: (a) SGD iteration with mini-batch size m≤m∗m≤m∗m\textbackslash leq m\^* is nearly equivalent to mmm iterations of mini-batch size 111 (linear scaling regime). (b) SGD iteration with mini-batch m{$>$}m∗m{$>$}m∗m{$>$} m\^* is nearly equivalent to a full gradient descent iteration (saturation regime). Moreover, for the quadratic loss, we derive explicit expressions for the optimal mini-batch and step size and explicitly characterize the two regimes above. The critical mini-batch size can be viewed as the limit for effective mini-batch parallelization. It is also nearly independent of the data size, implying O(n)O(n)O(n) acceleration over GD per unit of computation. We give experimental evidence on real data which closely follows our theoretical analyses. Finally, we show how our results fit in the recent developments in training deep neural networks and discuss connections to adaptive rates for SGD and variance reduction.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/P7GFI2LM/Ma et al_2018_The Power of Interpolation.pdf}
}

@inproceedings{maQualitativeStudyDynamic2022a,
  title = {A {{Qualitative Study}} of the {{Dynamic Behavior}} for {{Adaptive Gradient Algorithms}}},
  booktitle = {Proceedings of the 2nd {{Mathematical}} and {{Scientific Machine Learning Conference}}},
  author = {Ma, Chao and Wu, Lei and E, Weinan},
  date = {2022-04-30},
  pages = {671--692},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v145/ma22a.html},
  urldate = {2022-12-08},
  abstract = {The dynamic behavior of RMSprop and Adam algorithms is studied through a combination of careful numerical experiments and theoretical explanations. Three types of qualitative features are observed in the training loss curve: fast initial convergence, oscillations, and large spikes in the late phase. The sign gradient descent (signGD) flow, which is the limit of Adam when taking the learning rate to 0 while keeping the momentum parameters fixed, is used to explain the fast initial convergence. For the late phase of Adam, three different types of qualitative patterns are observed depending on the choice of the hyper-parameters: oscillations, spikes, and divergence. In particular, Adam converges much smoother and even faster when the values of the two momentum factors are close to each other. This observation is particularly important for scientific computing tasks, for which the training process usually proceeds into the high precision regime.},
  eventtitle = {Mathematical and {{Scientific Machine Learning}}},
  langid = {english},
  keywords = {application},
  file = {/home/jonathan/Zotero/storage/CCXD2M3Z/Ma et al_2022_A Qualitative Study of the Dynamic Behavior for Adaptive Gradient Algorithms.pdf}
}

@inproceedings{matthewsGaussianProcessBehaviour2022,
  title = {Gaussian {{Process Behaviour}} in {{Wide Deep Neural Networks}}},
  author = {Matthews, Alexander G. de G. and Hron, Jiri and Rowland, Mark and Turner, Richard E. and Ghahramani, Zoubin},
  date = {2022-02-10},
  url = {https://openreview.net/forum?id=H1-nGgWC-},
  urldate = {2022-12-04},
  abstract = {Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between Gaussian processes with a recursive kernel definition and random wide fully connected feedforward networks with more than one hidden layer. We exhibit limiting procedures under which finite deep networks will converge in distribution to the corresponding Gaussian process. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then exhibit situations where existing Bayesian deep networks are close to Gaussian processes in terms of the key quantities of interest. Any Gaussian process has a flat representation. Since this behaviour may be undesirable in certain situations we discuss ways in which it might be prevented.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/UJS6Z22A/Matthews et al_2022_Gaussian Process Behaviour in Wide Deep Neural Networks.pdf;/home/jonathan/Zotero/storage/KEZQC5T4/forum.html}
}

@inproceedings{mertikopoulosAlmostSureConvergence2020,
  title = {On the Almost Sure Convergence of Stochastic Gradient Descent in Non-Convex Problems},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Mertikopoulos, Panayotis and Hallak, Nadav and Kavis, Ali and Cevher, Volkan},
  date = {2020-12-06},
  series = {{{NIPS}}'20},
  pages = {1117--1128},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {This paper analyzes the trajectories of stochastic gradient descent (SGD) to help understand the algorithm's convergence properties in non-convex problems. We first show that the sequence of iterates generated by SGD remains bounded and converges with probability 1 under a very broad range of step-size schedules. Subsequently, going beyond existing positive probability guarantees, we show that SGD avoids strict saddle points/manifolds with probability 1 for the entire spectrum of step-size policies considered. Finally, we prove that the algorithm's rate of convergence to local minimizers with a positive-definite Hessian is O(1/np) if the method is employed with a Θ(1/np) step-size. This provides an important guideline for tuning the algorithm's step-size as it suggests that a cool-down phase with a vanishing step-size could lead to faster convergence; we demonstrate this heuristic using ResNet architectures on CIFAR.},
  isbn = {978-1-71382-954-6},
  keywords = {SGD,theory toread},
  file = {/home/jonathan/Zotero/storage/AYGGK6N8/Mertikopoulos et al_2020_On the almost sure convergence of stochastic gradient descent in non-convex.pdf}
}

@inproceedings{moulinesNonAsymptoticAnalysisStochastic2011,
  title = {Non-{{Asymptotic Analysis}} of {{Stochastic Approximation Algorithms}} for {{Machine Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Moulines, Eric and Bach, Francis},
  date = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html},
  urldate = {2022-09-28},
  abstract = {We consider the minimization of a convex objective function defined on a Hilbert space, which is only available through unbiased estimates of  its gradients.  This problem includes  standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the  convergence of two well-known algorithms, stochastic gradient descent (a.k.a.\textasciitilde Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.\textasciitilde Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.},
  keywords = {convergence,theory},
  file = {/home/jonathan/Zotero/storage/DYU79DLD/Moulines_Bach_2011_Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine.pdf}
}

@book{nesterovLecturesConvexOptimization2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  date = {2018-11-19},
  eprint = {IPh6DwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer}},
  abstract = {It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].},
  isbn = {978-3-319-91578-4},
  langid = {english},
  pagetotal = {603},
  keywords = {background,optimization,theory},
  file = {/home/jonathan/Zotero/storage/DMSGV4QG/Nesterov_2018_Lectures on Convex Optimization.pdf}
}

@inproceedings{nguyenSGDHogwildConvergence2018,
  title = {{{SGD}} and {{Hogwild}}! {{Convergence Without}} the {{Bounded Gradients Assumption}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Nguyen, Lam and Nguyen, Phuong Ha and Dijk, Marten and Richtarik, Peter and Scheinberg, Katya and Takac, Martin},
  date = {2018-07-03},
  pages = {3750--3758},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/nguyen18c.html},
  abstract = {Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/A7GRMKW2/Nguyen et al. - 2018 - SGD and Hogwild! Convergence Without the Bounded G.pdf;/home/jonathan/Zotero/storage/QN8TDXNA/Nguyen et al_2018_SGD and Hogwild.pdf}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen},
  date = {2006-07-27},
  eprint = {eNlPAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer New York}},
  abstract = {Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side. There is a selected solutions manual for instructors for the new edition.},
  isbn = {978-0-387-30303-1},
  langid = {english},
  pagetotal = {694},
  file = {/home/jonathan/Zotero/storage/2TMCX6FC/Nocedal_Wright_2006_Numerical Optimization.pdf}
}

@book{oksendalStochasticDifferentialEquations2003,
  title = {Stochastic {{Differential Equations}}: {{An Introduction}} with {{Applications}}},
  shorttitle = {Stochastic {{Differential Equations}}},
  author = {Øksendal, Bernt},
  date = {2003},
  eprint = {VgQDWyihxKYC},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {This edition contains detailed solutions of selected exercises. Many readers have requested this, because it makes the book more suitable for self-study. At the same time new exercises (without solutions) have beed added. They have all been placed in the end of each chapter, in order to facilitate the use of this edition together with previous ones. Several errors have been corrected and formulations have been improved. This has been made possible by the valuable comments from (in alphabetical order) Jon Bohlin, Mark Davis, Helge Holden, Patrick Jaillet, Chen Jing, Natalia Koroleva, MarioLefebvre, Alexander Matasov, Thilo Meyer-Brandis, Keigo Osawa, Bjørn Thunestvedt, Jan Ubøe and Yngve Williassen. I thank them all for helping to improve the book. My thanks also go to Dina Haraldsson, who once again has performed the typing and drawn the ?gures with great skill. Blindern, September 2002 Bernt Øksendal xv Preface to Corrected Printing, Fifth Edition The main corrections and improvements in this corrected printing are from Chapter 12. I have bene?tted from useful comments from a number of p- ple, including (in alphabetical order) Fredrik Dahl, Simone Deparis, Ulrich Haussmann, Yaozhong Hu, Marianne Huebner, Carl Peter Kirkebø, Ni- lay Kolev, Takashi Kumagai, Shlomo Levental, Geir Magnussen, Anders Øksendal, Jur ] gen Pottho?, Colin Rowat, Stig Sandnes, Lones Smith, S- suo Taniguchi and Bjørn Thunestvedt. I want to thank them all for helping me making the book better. I also want to thank Dina Haraldsson for pro?cient typing.},
  isbn = {978-3-540-04758-2},
  langid = {english},
  pagetotal = {406}
}

@online{OptimizationNeuralNetworks,
  title = {Optimization of {{Neural Networks}}},
  url = {https://www.deeplearningbook.org/contents/optimization.html},
  urldate = {2022-09-05},
  keywords = {optimization,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/Y5P6KEJ8/optimization.html}
}

@inproceedings{orvietoContinuoustimeModelsStochastic2019a,
  title = {Continuous-Time {{Models}} for {{Stochastic Optimization Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Orvieto, Antonio and Lucchi, Aurelien},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/9cd78264cf2cd821ba651485c111a29a-Abstract.html},
  urldate = {2022-09-05},
  abstract = {We propose new continuous-time formulations for first-order stochastic optimization algorithms such as mini-batch gradient descent and variance-reduced methods. We exploit these continuous-time models, together with simple Lyapunov analysis as well as tools from stochastic calculus, in order to derive convergence bounds for various types of non-convex functions. Guided by such analysis, we show that the same Lyapunov arguments hold in discrete-time, leading to matching rates. In addition, we use these models and Ito calculus to infer novel insights on the dynamics of SGD, proving that a decreasing learning rate acts as time warping or, equivalently, as landscape stretching.},
  keywords = {SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/MUPV8J9P/NeurIPS2019_SDEs_app.pdf;/home/jonathan/Zotero/storage/ZWXNPMF9/Orvieto_Lucchi_2019_Continuous-time Models for Stochastic Optimization Algorithms.pdf}
}

@misc{panigrahiNonGaussianityStochasticGradient2019,
  title = {Non-{{Gaussianity}} of {{Stochastic Gradient Noise}}},
  author = {Panigrahi, Abhishek and Somani, Raghav and Goyal, Navin and Netrapalli, Praneeth},
  date = {2019-10-25},
  number = {arXiv:1910.09626},
  eprint = {1910.09626},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.09626},
  url = {http://arxiv.org/abs/1910.09626},
  urldate = {2022-08-17},
  abstract = {What enables Stochastic Gradient Descent (SGD) to achieve better generalization than Gradient Descent (GD) in Neural Network training? This question has attracted much attention. In this paper, we study the distribution of the Stochastic Gradient Noise (SGN) vectors during the training. We observe that for batch sizes 256 and above, the distribution is best described as Gaussian at-least in the early phases of training. This holds across data-sets, architectures, and other choices.},
  archiveprefix = {arXiv},
  keywords = {noise,paper,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/C4QWEPDC/Panigrahi et al. - 2019 - Non-Gaussianity of Stochastic Gradient Noise.pdf;/home/jonathan/Zotero/storage/3KBMT38J/1910.html}
}

@inproceedings{pesmeImplicitBiasSGD2021,
  title = {Implicit {{Bias}} of {{SGD}} for {{Diagonal Linear Networks}}: A {{Provable Benefit}} of {{Stochasticity}}},
  shorttitle = {Implicit {{Bias}} of {{SGD}} for {{Diagonal Linear Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  date = {2021},
  volume = {34},
  pages = {29218--29230},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/f4661398cb1a3abd3ffe58600bf11322-Abstract.html},
  urldate = {2022-10-06},
  abstract = {Understanding the implicit bias of training algorithms is of crucial importance in order to explain the success of overparametrised neural networks. In this paper, we study the dynamics of stochastic gradient descent over diagonal linear networks through its continuous time version, namely stochastic gradient flow. We explicitly characterise the solution chosen by the stochastic flow and prove that it always enjoys better generalisation properties than that of gradient flow.Quite surprisingly, we show that the convergence speed of the training loss controls the magnitude of the biasing effect: the slower the convergence, the better the bias. To fully complete our analysis, we provide convergence guarantees for the dynamics. We also give experimental results which support our theoretical claims. Our findings highlight the fact that structured noise can induce better generalisation and they help explain the greater performances of stochastic gradient  descent over gradient descent observed in practice.},
  keywords = {SDE,SGD},
  file = {/home/jonathan/Zotero/storage/HHSEUNDC/Pesme et al_2021_Implicit Bias of SGD for Diagonal Linear Networks.pdf}
}

@article{polyakMethodsSpeedingConvergence1964,
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, B. T.},
  date = {1964-01-01},
  journaltitle = {USSR Computational Mathematics and Mathematical Physics},
  shortjournal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  number = {5},
  pages = {1--17},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(64)90137-5},
  url = {https://www.sciencedirect.com/science/article/pii/0041555364901375},
  urldate = {2022-11-15},
  abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/CLB5BQTY/0041555364901375.html}
}

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022-04-12},
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2022-09-12},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/FDTGXDWS/Ramesh et al_2022_Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf;/home/jonathan/Zotero/storage/6RXSLEHY/2204.html}
}

@online{ReviewNeurIPSPaper,
  title = {Review for {{NeurIPS}} Paper: {{On}} the {{Almost Sure Convergence}} of {{Stochastic Gradient Descent}} in {{Non-Convex Problems}}},
  url = {https://papers.nips.cc/paper/2020/file/0cb5ebb1b34ec343dfe135db691e4a85-Review.html},
  urldate = {2022-09-20},
  file = {/home/jonathan/Zotero/storage/N2A8WXPN/0cb5ebb1b34ec343dfe135db691e4a85-Review.html}
}

@incollection{robbinsCONVERGENCETHEOREMNON1971,
  title = {A {{CONVERGENCE THEOREM FOR NON NEGATIVE ALMOST SUPERMARTINGALES AND SOME APPLICATIONS}}**{{Research}} Supported by {{NIH Grant}} 5-{{R01-GM-16895-03}} and {{ONR Grant N00014-67-A-0108-0018}}.},
  booktitle = {Optimizing {{Methods}} in {{Statistics}}},
  author = {Robbins, H. and Siegmund, D.},
  editor = {Rustagi, Jagdish S.},
  date = {1971-01-01},
  pages = {233--257},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-604550-5.50015-8},
  url = {https://www.sciencedirect.com/science/article/pii/B9780126045505500158},
  urldate = {2022-12-10},
  abstract = {This chapter discusses a convergence theorem for nonnegative almost supermartingales and some applications. It discusses a unified treatment of a number of almost sure convergence theorems by exploiting the fact that the processes involved possess a common almost supermartingale properties. The inequalities are simple and useful generalizations of well-known results in martingale theory. Dvoretzky proved a general convergence theorem that includes Blum's result for the Robbins–Monro process and the corresponding result for the Kiefer–Wolfowitz method for estimating the maximum of a regression function as special cases.},
  isbn = {978-0-12-604550-5},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/7HH5LHP5/B9780126045505500158.html}
}

@article{robbinsConvergenceTheoremNon1985,
  title = {A {{Convergence Theorem}} for {{Non Negative Almost Supermartingales}} and {{Some Applications}}},
  author = {Robbins, H. and Siegmund, D.},
  date = {1985-01-01},
  journaltitle = {Herbert Robbins Selected Papers},
  pages = {111--135},
  publisher = {{Springer New York}},
  doi = {10.1007/978-1-4612-5110-1_10},
  url = {https://doi.org/10.1007%2F978-1-4612-5110-1_10},
  urldate = {2022-11-15},
  abstract = {The purpose of this paper is to give a unified treatment of a number of almost sure convergence theorems by ex},
  file = {/home/jonathan/Zotero/storage/2MWIL2ED/Robbins_Siegmund_1985_A Convergence Theorem for Non Negative Almost Supermartingales and Some.pdf}
}

@article{robbinsStochasticApproximationMethod1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  date = {1951-09},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full},
  urldate = {2022-09-29},
  abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = \textbackslash theta\$ of the equation \$M(x) = \textbackslash alpha\$, where \$\textbackslash alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,\textbackslash cdots\$ in such a way that \$x\_n\$ will tend to \$\textbackslash theta\$ in probability.},
  file = {/home/jonathan/Zotero/storage/7UGIIDFM/Robbins_Monro_1951_A Stochastic Approximation Method.pdf;/home/jonathan/Zotero/storage/SMM3N24C/1177729586.html}
}

@inproceedings{rotskoffParametersInteractingParticles2018,
  title = {Parameters as Interacting Particles: Long Time Convergence and Asymptotic Error Scaling of Neural Networks},
  shorttitle = {Parameters as Interacting Particles},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rotskoff, Grant and Vanden-Eijnden, Eric},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/196f5641aa9dc87067da4ff90fd81e7b-Abstract.html},
  urldate = {2022-11-02},
  keywords = {toread},
  file = {/home/jonathan/Zotero/storage/KNIGHVUC/Rotskoff_Vanden-Eijnden_2018_Parameters as interacting particles.pdf}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2022-11-15},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/RC72B7C9/Rumelhart et al_1986_Learning representations by back-propagating errors.pdf;/home/jonathan/Zotero/storage/ES6RNRSW/323533a0.html}
}

@article{santambrogioEuclideanMetricWasserstein2017,
  title = {\{\vphantom\}{{Euclidean}}, Metric, and {{Wasserstein}}\vphantom\{\} Gradient Flows: An Overview},
  shorttitle = {\{\vphantom\}{{Euclidean}}, Metric, and {{Wasserstein}}\vphantom\{\} Gradient Flows},
  author = {Santambrogio, Filippo},
  date = {2017-04-01},
  journaltitle = {Bulletin of Mathematical Sciences},
  shortjournal = {Bull. Math. Sci.},
  volume = {7},
  number = {1},
  pages = {87--154},
  issn = {1664-3615},
  doi = {10.1007/s13373-017-0101-1},
  url = {https://doi.org/10.1007/s13373-017-0101-1},
  urldate = {2022-11-23},
  abstract = {This is an expository paper on the theory of gradient flows, and in particular of those PDEs which can be interpreted as gradient flows for the Wasserstein metric on the space of probability measures (a distance induced by optimal transport). The starting point is the Euclidean theory, and then its generalization to metric spaces, according to the work of Ambrosio, Gigli and Savaré. Then comes an independent exposition of the Wasserstein theory, with a short introduction to the optimal transport tools that are needed and to the notion of geodesic convexity, followed by a precise description of the Jordan–Kinderlehrer–Otto scheme and a sketch of proof to obtain its convergence in the easiest cases. A discussion of which equations are gradient flows PDEs and of numerical methods based on these ideas is also provided. The paper ends with a new, theoretical, development, due to Ambrosio, Gigli, Savaré, Kuwada and Ohta: the study of the heat flow in metric measure spaces.},
  langid = {english},
  keywords = {gradient flow,theory},
  file = {/home/jonathan/Zotero/storage/BX46TE3W/Santambrogio_2017_ Euclidean, metric, and Wasserstein gradient flows.pdf}
}

@article{schrittwieserMasteringAtariGo2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2020-12-24},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  url = {http://arxiv.org/abs/1911.08265},
  urldate = {2022-09-12},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/Y4H7QWLF/Schrittwieser et al_2020_Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf;/home/jonathan/Zotero/storage/4QJ4BJ6Y/1911.html}
}

@inproceedings{sebbouhAlmostSureConvergence2021,
  title = {Almost Sure Convergence Rates for {{Stochastic Gradient Descent}} and {{Stochastic Heavy Ball}}},
  booktitle = {Proceedings of {{Thirty Fourth Conference}} on {{Learning Theory}}},
  author = {Sebbouh, Othmane and Gower, Robert M. and Defazio, Aaron},
  date = {2021-07-21},
  pages = {3935--3971},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v134/sebbouh21a.html},
  urldate = {2022-09-05},
  abstract = {We study stochastic gradient descent (SGD) and the stochastic heavy ball method (SHB, otherwise known as the momentum method) for the general stochastic approximation problem.  For SGD, in the convex and smooth setting, we provide the first \textbackslash emph\{almost sure\} asymptotic convergence \textbackslash emph\{rates\} for a weighted average of the iterates . More precisely, we show that the convergence rate of the function values is arbitrarily close to o(1/k−−√)o(1/k)o(1/\textbackslash sqrt\{k\}), and is exactly o(1/k)o(1/k)o(1/k) in the so-called overparametrized case. We show that these results still hold when using a decreasing step size version of stochastic line search and stochastic Polyak stepsizes, thereby giving the first proof of convergence of these methods in the non-overparametrized regime.  Using a substantially different analysis, we show that these rates hold for SHB as well, but at the last iterate. This distinction is important because it is the last iterate of SGD and SHB which is used in practice. We also show that the last iterate of SHB converges to a minimizer \textbackslash emph\{almost surely\}. Additionally, we prove that the function values of the deterministic HB converge at a o(1/k)o(1/k)o(1/k) rate, which is faster than the previously known O(1/k)O(1/k)O(1/k).  Finally, in the nonconvex setting, we prove similar rates on the lowest gradient norm along the trajectory of SGD.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/2NINK37W/Sebbouh et al_2021_Almost sure convergence rates for Stochastic Gradient Descent and Stochastic.pdf}
}

@article{shapiroConvergenceAnalysisGradient1996,
  title = {Convergence Analysis of Gradient Descent Stochastic Algorithms},
  author = {Shapiro, A. and Wardi, Y.},
  date = {1996-11-01},
  journaltitle = {Journal of Optimization Theory and Applications},
  shortjournal = {J Optim Theory Appl},
  volume = {91},
  number = {2},
  pages = {439--454},
  issn = {1573-2878},
  doi = {10.1007/BF02190104},
  url = {https://doi.org/10.1007/BF02190104},
  urldate = {2022-09-05},
  abstract = {This paper proves convergence of a sample-path based stochastic gradient-descent algorithm for optimizing expected-value performance measures in discrete event systems. The algorithm uses increasing precision at successive iterations, and it moves against the direction of a generalized gradient of the computed sample performance function. Two convergence results are established: one, for the case where the expected-value function is continuously differentiable; and the other, when that function is nondifferentiable but the sample performance functions are convex. The proofs are based on a version of the uniform law of large numbers which is provable for many discrete event systems where infinitesimal perturbation analysis is known to be strongly consistent.},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/NJ4IHM7A/Shapiro_Wardi_1996_Convergence analysis of gradient descent stochastic algorithms.pdf}
}

@inproceedings{simsekliTailIndexAnalysisStochastic2019,
  title = {A {{Tail-Index Analysis}} of {{Stochastic Gradient Noise}} in {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  date = {2019-05-24},
  pages = {5827--5837},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/simsekli19a.html},
  urldate = {2022-08-17},
  abstract = {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed αα\textbackslash alpha-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Lévy motion. Such SDEs can incur ‘jumps’, which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the αα\textbackslash alpha-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {noise,paper,SDE,SGD,thesis},
  file = {/home/jonathan/Zotero/storage/B8IU5MQL/Simsekli et al. - 2019 - A Tail-Index Analysis of Stochastic Gradient Noise.pdf}
}

@misc{smithOriginImplicitRegularization2021,
  title = {On the {{Origin}} of {{Implicit Regularization}} in {{Stochastic Gradient Descent}}},
  author = {Smith, Samuel L. and Dherin, Benoit and Barrett, David G. T. and De, Soham},
  date = {2021-01-28},
  number = {arXiv:2101.12176},
  eprint = {2101.12176},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.12176},
  url = {http://arxiv.org/abs/2101.12176},
  urldate = {2022-11-08},
  abstract = {For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.},
  archiveprefix = {arXiv},
  keywords = {important},
  file = {/home/jonathan/Zotero/storage/UNH3BFXY/Smith et al_2021_On the Origin of Implicit Regularization in Stochastic Gradient Descent.pdf;/home/jonathan/Zotero/storage/2XTWBQR6/2101.html}
}

@article{soudryImplicitBiasGradient2018,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {19},
  number = {70},
  pages = {1--57},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v19/18-188.html},
  urldate = {2022-09-04},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
  keywords = {sgd,thesis,toread},
  file = {/home/jonathan/Zotero/storage/RFIJYHVS/Soudry et al_2018_The Implicit Bias of Gradient Descent on Separable Data.pdf}
}

@inproceedings{thomasInterplayNoiseCurvature2020,
  title = {On the Interplay between Noise and Curvature and Its Effect on Optimization and Generalization},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Thomas, Valentin and Pedregosa, Fabian and Merriënboer, Bart and Manzagol, Pierre-Antoine and Bengio, Yoshua and Roux, Nicolas Le},
  date = {2020-06-03},
  pages = {3503--3513},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v108/thomas20a.html},
  urldate = {2022-08-17},
  abstract = {The speed at which one can minimize an expected loss using stochastic methods depends on two properties: the curvature of the loss and the variance of the gradients. While most previous works focus on one or the other of these properties, we explore how their interaction affects optimization speed. Further, as the ultimate goal is good generalization performance, we clarify how both curvature and noise are relevant to properly estimate the generalization gap. Realizing that the limitations of some existing works stems from a confusion between these matrices, we also clarify the distinction between the Fisher matrix, the Hessian, and the covariance matrix of the gradients.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {fisher,paper,thesis},
  file = {/home/jonathan/Zotero/storage/4K8IEQ7T/Thomas et al. - 2020 - On the interplay between noise and curvature and i.pdf;/home/jonathan/Zotero/storage/HVRYFTD6/Thomas et al. - 2020 - On the interplay between noise and curvature and i.pdf}
}

@video{uclcentreforartificialintelligenceOriginImplicitRegularization2021,
  title = {On the {{Origin}} of {{Implicit Regularization}} in {{Stochastic Gradient Descent}}},
  editor = {{UCL Centre for Artificial Intelligence}},
  date = {2021-04-29},
  url = {https://www.youtube.com/watch?v=pZnZSxOttN0},
  urldate = {2022-11-09},
  editortype = {director}
}

@article{uhlenbeckTheoryBrownianMotion1930,
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.},
  date = {1930-09-01},
  journaltitle = {Physical Review},
  shortjournal = {Phys. Rev.},
  volume = {36},
  number = {5},
  pages = {823--841},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRev.36.823},
  url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
  urldate = {2022-09-12},
  abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u−u0exp(−βt) and s−u0β[1−exp(−βt)] where u0 is the initial velocity and β the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and Fürth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when β is much larger than the frequency and for values of t≫β−1, the formula takes the form of that previously given by Smoluchowski.},
  keywords = {SDE,thesis},
  file = {/home/jonathan/Zotero/storage/FQQXCPHW/PhysRev.36.html}
}

@inproceedings{vaswaniFastFasterConvergence2019,
  title = {Fast and {{Faster Convergence}} of {{SGD}} for {{Over-Parameterized Models}} and an {{Accelerated Perceptron}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  date = {2019-04-11},
  pages = {1195--1204},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/vaswani19a.html},
  urldate = {2022-09-07},
  abstract = {Modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely,  resulting in zero training loss. For such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. Under this condition, we prove that constant step-size stochastic gradient descent (SGD) with Nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. We also show that this condition implies that SGD can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. Under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. Given this weaker condition, we prove that SGD with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. Under additional assumptions, the above results enable us to prove an O(1/k2)O(1/k2)O(1/k\^2) mistake bound for kkk iterations of a stochastic perceptron algorithm using the squared-hinge loss. Finally, we validate our theoretical findings with experiments on synthetic and real datasets.},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/A3MP9FRI/Vaswani et al_2019_Fast and Faster Convergence of SGD for Over-Parameterized Models and an.pdf;/home/jonathan/Zotero/storage/TAFFELXT/Vaswani et al. - 2019 - Fast and Faster Convergence of SGD for Over-Parame.pdf}
}

@inproceedings{wilsonMarginalValueAdaptive2017,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html},
  urldate = {2022-12-04},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks.  Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD).  We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half.  We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  file = {/home/jonathan/Zotero/storage/KD3I6N73/Wilson et al_2017_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf}
}

@misc{wuGroupNormalization2018,
  title = {Group {{Normalization}}},
  author = {Wu, Yuxin and He, Kaiming},
  date = {2018-06-11},
  number = {arXiv:1803.08494},
  eprint = {1803.08494},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.08494},
  url = {http://arxiv.org/abs/1803.08494},
  urldate = {2022-08-17},
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  archiveprefix = {arXiv},
  keywords = {GN,NGD,paper,thesis,toread},
  file = {/home/jonathan/Zotero/storage/D7BRVQUJ/Wu and He - 2018 - Group Normalization.pdf;/home/jonathan/Zotero/storage/VK3T6FK6/1803.html}
}

@misc{wuNoisyGradientDescent2020,
  title = {On the {{Noisy Gradient Descent}} That {{Generalizes}} as {{SGD}}},
  author = {Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  date = {2020-06-19},
  number = {arXiv:1906.07405},
  eprint = {1906.07405},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.07405},
  url = {http://arxiv.org/abs/1906.07405},
  urldate = {2022-08-17},
  abstract = {The gradient noise of SGD is considered to play a central role in the observed strong generalization abilities of deep learning. While past studies confirm that the magnitude and the covariance structure of gradient noise are critical for regularization, it remains unclear whether or not the class of noise distributions is important. In this work we provide negative results by showing that noises in classes different from the SGD noise can also effectively regularize gradient descent. Our finding is based on a novel observation on the structure of the SGD noise: it is the multiplication of the gradient matrix and a sampling noise that arises from the mini-batch sampling procedure. Moreover, the sampling noises unify two kinds of gradient regularizing noises that belong to the Gaussian class: the one using (scaled) Fisher as covariance and the one using the gradient covariance of SGD as covariance. Finally, thanks to the flexibility of choosing noise class, an algorithm is proposed to perform noisy gradient descent that generalizes well, the variant of which even benefits large batch SGD training without hurting generalization.},
  archiveprefix = {arXiv},
  keywords = {important,paper,read,SDE,SGD,thesis},
  file = {/home/jonathan/Zotero/storage/5ATMKEPS/Wu et al. - 2020 - On the Noisy Gradient Descent that Generalizes as .pdf;/home/jonathan/Zotero/storage/G9K3M5XG/1906.html}
}

@inproceedings{wuNoisyGradientDescent2020a,
  title = {On the {{Noisy Gradient Descent}} That {{Generalizes}} as {{SGD}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  date = {2020-11-21},
  pages = {10367--10376},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/wu20c.html},
  urldate = {2022-12-08},
  abstract = {The gradient noise of SGD is considered to play a central role in the observed strong generalization abilities of deep learning. While past studies confirm that the magnitude and the covariance structure of gradient noise are critical for regularization, it remains unclear whether or not the class of noise distributions is important. In this work we provide negative results by showing that noises in classes different from the SGD noise can also effectively regularize gradient descent. Our finding is based on a novel observation on the structure of the SGD noise: it is the multiplication of the gradient matrix and a sampling noise that arises from the mini-batch sampling procedure. Moreover, the sampling noises unify two kinds of gradient regularizing noises that belong to the Gaussian class: the one using (scaled) Fisher as covariance and the one using the gradient covariance of SGD as covariance. Finally, thanks to the flexibility of choosing noise class, an algorithm is proposed to perform noisy gradient descent that generalizes well, the variant of which even benefits large batch SGD training without hurting generalization.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {noise,sgd},
  file = {/home/jonathan/Zotero/storage/7CY9GY27/Wu et al. - 2020 - On the Noisy Gradient Descent that Generalizes as .pdf;/home/jonathan/Zotero/storage/Y48DCEGJ/Wu et al. - 2020 - On the Noisy Gradient Descent that Generalizes as .pdf}
}

@misc{xieDiffusionTheoryDeep2021,
  title = {A {{Diffusion Theory For Deep Learning Dynamics}}: {{Stochastic Gradient Descent Exponentially Favors Flat Minima}}},
  shorttitle = {A {{Diffusion Theory For Deep Learning Dynamics}}},
  author = {Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  date = {2021-01-15},
  number = {arXiv:2002.03495},
  eprint = {2002.03495},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2002.03495},
  urldate = {2022-08-17},
  abstract = {Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.},
  archiveprefix = {arXiv},
  keywords = {paper,thesis,toread},
  file = {/home/jonathan/Zotero/storage/Z2AF6F2A/Xie et al. - 2021 - A Diffusion Theory For Deep Learning Dynamics Sto.pdf;/home/jonathan/Zotero/storage/IJDMK2NK/2002.html}
}

@misc{xingWalkSGD2018,
  title = {A {{Walk}} with {{SGD}}},
  author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  date = {2018-05-29},
  number = {arXiv:1802.08770},
  eprint = {1802.08770},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.08770},
  url = {http://arxiv.org/abs/1802.08770},
  urldate = {2022-09-22},
  abstract = {We present novel empirical observations regarding how stochastic gradient descent (SGD) navigates the loss landscape of over-parametrized deep neural networks (DNNs). These observations expose the qualitatively different roles of learning rate and batch-size in DNN optimization and generalization. Specifically we study the DNN loss surface along the trajectory of SGD by interpolating the loss surface between parameters from consecutive \textbackslash textit\{iterations\} and tracking various metrics during training. We find that the loss interpolation between parameters before and after each training iteration's update is roughly convex with a minimum (\textbackslash textit\{valley floor\}) in between for most of the training. Based on this and other metrics, we deduce that for most of the training update steps, SGD moves in valley like regions of the loss surface by jumping from one valley wall to another at a height above the valley floor. This 'bouncing between walls at a height' mechanism helps SGD traverse larger distance for small batch sizes and large learning rates which we find play qualitatively different roles in the dynamics. While a large learning rate maintains a large height from the valley floor, a small batch size injects noise facilitating exploration. We find this mechanism is crucial for generalization because the valley floor has barriers and this exploration above the valley floor allows SGD to quickly travel far away from the initialization point (without being affected by barriers) and find flatter regions, corresponding to better generalization.},
  archiveprefix = {arXiv},
  keywords = {SGD},
  file = {/home/jonathan/Zotero/storage/6UKFY3RR/Xing et al_2018_A Walk with SGD.pdf;/home/jonathan/Zotero/storage/YKWU3IFC/1802.html}
}

@misc{yangUnifiedConvergenceAnalysis2016,
  title = {Unified {{Convergence Analysis}} of {{Stochastic Momentum Methods}} for {{Convex}} and {{Non-convex Optimization}}},
  author = {Yang, Tianbao and Lin, Qihang and Li, Zhe},
  date = {2016-05-04},
  number = {arXiv:1604.03257},
  eprint = {1604.03257},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1604.03257},
  url = {http://arxiv.org/abs/1604.03257},
  urldate = {2022-09-19},
  abstract = {Recently, \{\textbackslash it stochastic momentum\} methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods.},
  archiveprefix = {arXiv},
  keywords = {convergence,SGD,theory,toread},
  file = {/home/jonathan/Zotero/storage/WWY323Y4/Yang et al_2016_Unified Convergence Analysis of Stochastic Momentum Methods for Convex and.pdf;/home/jonathan/Zotero/storage/INBQWV6T/1604.html}
}

@misc{zengStochasticVarianceReduced2021,
  title = {On {{Stochastic Variance Reduced Gradient Method}} for {{Semidefinite Optimization}}},
  author = {Zeng, Jinshan and Zha, Yixuan and Ma, Ke and Yao, Yuan},
  date = {2021-01-01},
  number = {arXiv:2101.00236},
  eprint = {2101.00236},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.00236},
  url = {http://arxiv.org/abs/2101.00236},
  urldate = {2022-09-05},
  abstract = {The low-rank stochastic semidefinite optimization has attracted rising attention due to its wide range of applications. The nonconvex reformulation based on the low-rank factorization, significantly improves the computational efficiency but brings some new challenge to the analysis. The stochastic variance reduced gradient (SVRG) method has been regarded as one of the most effective methods. SVRG in general consists of two loops, where a reference full gradient is first evaluated in the outer loop and then used to yield a variance reduced estimate of the current gradient in the inner loop. Two options have been suggested to yield the output of the inner loop, where Option I sets the output as its last iterate, and Option II yields the output via random sampling from all the iterates in the inner loop. However, there is a significant gap between the theory and practice of SVRG when adapted to the stochastic semidefinite programming (SDP). SVRG practically works better with Option I, while most of existing theoretical results focus on Option II. In this paper, we fill this gap via exploiting a new semi-stochastic variant of the original SVRG with Option I adapted to the semidefinite optimization. Equipped with this, we establish the global linear submanifold convergence (i.e., converging exponentially fast to a submanifold of a global minimum under the orthogonal group action) of the proposed SVRG method, given a provable initialization scheme and under certain smoothness and restricted strongly convex assumptions. Our analysis includes the effects of the mini-batch size and update frequency in the inner loop as well as two practical step size strategies, the fixed and stabilized Barzilai-Borwein step sizes. Some numerical results in matrix sensing demonstrate the efficiency of proposed SVRG method outperforming Option II counterpart as well as others.},
  archiveprefix = {arXiv},
  keywords = {convergence,SVRG,thesis,toread},
  file = {/home/jonathan/Zotero/storage/A8HZFT97/Zeng et al_2021_On Stochastic Variance Reduced Gradient Method for Semidefinite Optimization.pdf;/home/jonathan/Zotero/storage/2UZC4DPC/2101.html}
}

@article{zhangUnderstandingDeepLearning2021,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2021-02-22},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  issn = {0001-0782},
  doi = {10.1145/3446776},
  url = {https://doi.org/10.1145/3446776},
  urldate = {2022-08-29},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
  keywords = {generalization,regularization,toread},
  file = {/home/jonathan/Zotero/storage/2PSPS7WB/Zhang et al_2021_Understanding deep learning (still) requires rethinking generalization.pdf}
}

@inproceedings{zhouStochasticMirrorDescent2017,
  title = {Stochastic {{Mirror Descent}} in {{Variationally Coherent Optimization Problems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Boyd, Stephen and Glynn, Peter W},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/e6ba70fc093b4ce912d769ede1ceeba8-Abstract.html},
  urldate = {2022-09-29},
  abstract = {In this paper, we examine a class of non-convex stochastic optimization problems which we call variationally coherent, and which properly includes pseudo-/quasiconvex and star-convex optimization problems. To solve such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case), and we show that the last iterate of SMD converges to the problem’s solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather, variational coherence, a much weaker requirement, suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.},
  file = {/home/jonathan/Zotero/storage/6CE883YF/Zhou et al_2017_Stochastic Mirror Descent in Variationally Coherent Optimization Problems.pdf}
}

@misc{zhouStochasticModifiedEquations2020,
  title = {Stochastic {{Modified Equations}} for {{Continuous Limit}} of {{Stochastic ADMM}}},
  author = {Zhou, Xiang and Yuan, Huizhuo and Li, Chris Junchi and Sun, Qingyun},
  date = {2020-03-07},
  number = {arXiv:2003.03532},
  eprint = {2003.03532},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.03532},
  url = {http://arxiv.org/abs/2003.03532},
  urldate = {2022-12-08},
  abstract = {Stochastic version of alternating direction method of multiplier (ADMM) and its variants (linearized ADMM, gradient-based ADMM) plays a key role for modern large scale machine learning problems. One example is the regularized empirical risk minimization problem. In this work, we put different variants of stochastic ADMM into a unified form, which includes standard, linearized and gradient-based ADMM with relaxation, and study their dynamics via a continuous-time model approach. We adapt the mathematical framework of stochastic modified equation (SME), and show that the dynamics of stochastic ADMM is approximated by a class of stochastic differential equations with small noise parameters in the sense of weak approximation. The continuous-time analysis would uncover important analytical insights into the behaviors of the discrete-time algorithm, which are non-trivial to gain otherwise. For example, we could characterize the fluctuation of the solution paths precisely, and decide optimal stopping time to minimize the variance of solution paths.},
  archiveprefix = {arXiv},
  file = {/home/jonathan/Zotero/storage/D9CT5QQ6/Zhou et al_2020_Stochastic Modified Equations for Continuous Limit of Stochastic ADMM.pdf;/home/jonathan/Zotero/storage/4WLEGQWK/2003.html}
}

@inproceedings{zhouTheoreticallyUnderstandingWhy2020,
  title = {Towards {{Theoretically Understanding Why Sgd Generalizes Better Than Adam}} in {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and E, Weinan},
  date = {2020},
  volume = {33},
  pages = {21285--21296},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/f3f27a324736617f20abbf2ffd806f6d-Abstract.html},
  urldate = {2022-11-02},
  abstract = {It is not clear yet why ADAM-alike adaptive gradient algorithms suffer from worse generalization performance than  SGD despite their faster training speed.   This work aims to provide understandings on this generalization gap by analyzing their local convergence behaviors. Specifically,  we observe the heavy tails of gradient noise in these algorithms. This motivates us to analyze these algorithms through their  Levy-driven stochastic differential equations (SDEs)  because of the similar convergence behaviors of an algorithm and its SDE. Then we establish the escaping time of these SDEs from a local basin. The result shows that (1) the escaping time of both SGD and ADAM\textasciitilde depends on the  Radon measure of the basin positively and the heaviness of gradient noise negatively;  (2) for the same basin, SGD enjoys smaller escaping time than ADAM, mainly because  (a) the geometry adaptation in ADAM\textasciitilde via adaptively scaling each gradient coordinate well diminishes the anisotropic structure in gradient noise and results in larger Radon measure of a basin; (b)  the exponential gradient average in ADAM\textasciitilde smooths its gradient and leads to lighter gradient noise tails than SGD.  So SGD is more locally unstable than ADAM\textasciitilde at sharp minima defined as the minima whose local basins have small Radon measure, and can better escape from them to flatter ones with larger Radon measure.   As flat minima here which often refer to the minima at flat or asymmetric basins/valleys often generalize better than sharp ones\textasciitilde\textbackslash cite\{keskar2016large,he2019asymmetric\}, our result explains the better generalization performance of SGD over ADAM.  Finally, experimental results confirm our heavy-tailed gradient noise assumption and theoretical affirmation.},
  keywords = {levy,sde,sgd},
  file = {/home/jonathan/Zotero/storage/726L9F95/Zhou et al_2020_Towards Theoretically Understanding Why Sgd Generalizes Better Than Adam in.pdf;/home/jonathan/Zotero/storage/F799XL4W/Zhou et al. - 2020 - Towards Theoretically Understanding Why Sgd Genera.pdf}
}

@misc{zhuAnisotropicNoiseStochastic2019,
  title = {The {{Anisotropic Noise}} in {{Stochastic Gradient Descent}}: {{Its Behavior}} of {{Escaping}} from {{Sharp Minima}} and {{Regularization Effects}}},
  shorttitle = {The {{Anisotropic Noise}} in {{Stochastic Gradient Descent}}},
  author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  date = {2019-06-10},
  number = {arXiv:1803.00195},
  eprint = {1803.00195},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.00195},
  url = {http://arxiv.org/abs/1803.00195},
  urldate = {2022-08-25},
  abstract = {Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).},
  archiveprefix = {arXiv},
  keywords = {SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/RPGC6YV3/Zhu et al_2019_The Anisotropic Noise in Stochastic Gradient Descent.pdf;/home/jonathan/Zotero/storage/9NTSUWQP/1803.html}
}
