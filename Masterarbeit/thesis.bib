
@inproceedings{allen-zhuConvergenceTheoryDeep2019,
  title = {A {{Convergence Theory}} for {{Deep Learning}} via {{Over-Parameterization}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  date = {2019-05-24},
  pages = {242--252},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/allen-zhu19a.html},
  urldate = {2022-09-21},
  abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100\% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps ~ e\^\{-T\}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/D3Y8I392/Allen-Zhu et al_2019_A Convergence Theory for Deep Learning via Over-Parameterization.pdf}
}

@misc{blancImplicitRegularizationDeep2020,
  title = {Implicit Regularization for Deep Neural Networks Driven by an {{Ornstein-Uhlenbeck}} like Process},
  author = {Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  date = {2020-07-22},
  number = {arXiv:1904.09080},
  eprint = {1904.09080},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.09080},
  url = {http://arxiv.org/abs/1904.09080},
  urldate = {2022-09-04},
  abstract = {We consider networks, trained via stochastic gradient descent to minimize \$\textbackslash ell\_2\$ loss, with the training labels perturbed by independent noise at each iteration. We characterize the behavior of the training dynamics near any parameter vector that achieves zero training error, in terms of an implicit regularization term corresponding to the sum over the data points, of the squared \$\textbackslash ell\_2\$ norm of the gradient of the model with respect to the parameter vector, evaluated at each data point. This holds for networks of any connectivity, width, depth, and choice of activation function. We interpret this implicit regularization term for three simple settings: matrix sensing, two layer ReLU networks trained on one-dimensional data, and two layer networks with sigmoid activations trained on a single datapoint. For these settings, we show why this new and general implicit regularization effect drives the networks towards "simple" models.},
  archiveprefix = {arXiv},
  keywords = {noise,sgd,thesis},
  file = {/home/jonathan/Zotero/storage/CSMWKWUX/Blanc et al_2020_Implicit regularization for deep neural networks driven by an.pdf;/home/jonathan/Zotero/storage/RP4R4RJM/1904.html}
}

@article{bottouOptimizationMethodsLargeScale2018,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2018-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {60},
  number = {2},
  pages = {223--311},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/16M1080173},
  url = {https://epubs.siam.org/doi/10.1137/16M1080173},
  urldate = {2022-09-05},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/HVTDRSTJ/Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf}
}

@article{bottouOptimizationMethodsLargeScale2018a,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2018-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {60},
  number = {2},
  pages = {223--311},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/16M1080173},
  url = {https://epubs.siam.org/doi/10.1137/16M1080173},
  urldate = {2022-09-19},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  file = {/home/jonathan/Zotero/storage/57QW8YNH/Bottou et al_2018_Optimization Methods for Large-Scale Machine Learning.pdf}
}

@book{boydConvexOptimization2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  date = {2004-03-08},
  eprint = {IUZdAAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.},
  isbn = {978-1-107-39400-1},
  langid = {english},
  pagetotal = {744},
  keywords = {background,optimization,theory,thesis},
  file = {/home/jonathan/Zotero/storage/I8Z26ZZ5/Boyd_Vandenberghe_2004_Convex Optimization.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-09-12},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  version = {4},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/ZFJVLUBD/Brown et al_2020_Language Models are Few-Shot Learners.pdf;/home/jonathan/Zotero/storage/Q6TQ82US/2005.html}
}

@book{capassoIntroductionContinuousTimeStochastic2012,
  title = {An {{Introduction}} to {{Continuous-Time Stochastic Processes}}: {{Theory}}, {{Models}}, and {{Applications}} to {{Finance}}, {{Biology}}, and {{Medicine}}},
  shorttitle = {An {{Introduction}} to {{Continuous-Time Stochastic Processes}}},
  author = {Capasso, Vincenzo and Bakstein, David},
  date = {2012-07-27},
  eprint = {h83lVwvgJnoC},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Expanding on the first edition of An Introduction to Continuous-Time Stochastic Processes, this concisely written book is a rigorous and self-contained introduction to the theory of continuous-time stochastic processes. A balance of theory and applications, the work features concrete examples of modeling real-world problems from biology, medicine, industrial applications, finance, and insurance using stochastic methods. No previous knowledge of stochastic processes is required.},
  isbn = {978-0-8176-8346-7},
  langid = {english},
  pagetotal = {438},
  keywords = {background,SDE,theory,thesis},
  file = {/home/jonathan/Zotero/storage/UI9X2DBV/Capasso_Bakstein_2012_An Introduction to Continuous-Time Stochastic Processes.pdf}
}

@inproceedings{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Choromanska, Anna and Henaff, MIkael and Mathieu, Michael and Arous, Gerard Ben and LeCun, Yann},
  date = {2015-02-21},
  pages = {192--204},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v38/choromanska15.html},
  urldate = {2022-09-05},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {nonconvexity,thesis,toread},
  file = {/home/jonathan/Zotero/storage/9V9GU2YZ/Choromanska et al_2015_The Loss Surfaces of Multilayer Networks.pdf}
}

@book{durrettProbabilityTheoryExamples2019,
  title = {Probability: {{Theory}} and {{Examples}}},
  shorttitle = {Probability},
  author = {Durrett, Rick},
  date = {2019-04-18},
  eprint = {b22MDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {This lively introduction to measure-theoretic probability theory covers laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. Concentrating on results that are the most useful for applications, this comprehensive treatment is a rigorous graduate text and reference. Operating under the philosophy that the best way to learn probability is to see it in action, the book contains extended examples that apply the theory to concrete applications. This fifth edition contains a new chapter on multidimensional Brownian motion and its relationship to partial differential equations (PDEs), an advanced topic that is finding new applications. Setting the foundation for this expansion, Chapter 7 now features a proof of Itô's formula. Key exercises that previously were simply proofs left to the reader have been directly inserted into the text as lemmas. The new edition re-instates discussion about the central limit theorem for martingales and stationary sequences.},
  isbn = {978-1-108-47368-2},
  langid = {english},
  pagetotal = {433},
  keywords = {background},
  file = {/home/jonathan/Zotero/storage/UKLZ9CJD/Durrett_2019_Probability.pdf}
}

@book{eAppliedStochasticAnalysis2021,
  title = {Applied {{Stochastic Analysis}}},
  author = {E, Weinan and Li, Tiejun and Vanden-Eijnden, Eric},
  date = {2021-09-22},
  eprint = {YVpQEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{American Mathematical Soc.}},
  abstract = {This is a textbook for advanced undergraduate students and beginning graduate students in applied mathematics. It presents the basic mathematical foundations of stochastic analysis (probability theory and stochastic processes) as well as some important practical tools and applications (e.g., the connection with differential equations, numerical methods, path integrals, random fields, statistical physics, chemical kinetics, and rare events). The book strikes a nice balance between mathematical formalism and intuitive arguments, a style that is most suited for applied mathematicians. Readers can learn both the rigorous treatment of stochastic analysis as well as practical applications in modeling and simulation. Numerous exercises nicely supplement the main exposition.},
  isbn = {978-1-4704-6569-8},
  langid = {english},
  pagetotal = {329},
  keywords = {background,thesis},
  file = {/home/jonathan/Zotero/storage/G3DGXFTY/E et al. - 2021 - Applied Stochastic Analysis.pdf}
}

@inproceedings{fontaineConvergenceRatesApproximation2021,
  title = {Convergence Rates and Approximation Results for {{SGD}} and Its Continuous-Time Counterpart},
  booktitle = {Proceedings of {{Thirty Fourth Conference}} on {{Learning Theory}}},
  author = {Fontaine, Xavier and Bortoli, Valentin De and Durmus, Alain},
  date = {2021-07-21},
  pages = {1965--2058},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v134/fontaine21a.html},
  urldate = {2022-09-05},
  abstract = {This paper proposes a thorough theoretical analysis of Stochastic Gradient Descent (SGD) with non-increasing step sizes.  First, we show that the recursion defining SGD can be provably approximated by solutions of a time inhomogeneous Stochastic Differential Equation (SDE) using an appropriate coupling. In the specific case of a batch noise we refine our results using recent advances in Stein’s method. Then, motivated by recent analyses of deterministic and stochastic optimization methods by their continuous counterpart, we study the long-time behavior of the continuous processes at hand and establish non-asymptotic bounds. To that purpose, we develop new comparison techniques which are of independent interest. Adapting these techniques to the discrete setting, we show that the same results hold for the corresponding SGD sequences.  In our analysis, we notably improve non-asymptotic bounds in the convex setting for SGD under weaker assumptions than the ones considered in previous works. Finally, we also establish finite-time convergence results under various conditions, including relaxations of the famous Ł\{ojasciewicz\} inequality, which can be applied to a class of non-convex functions.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/F5NSEG8L/Fontaine et al_2021_Convergence rates and approximation results for SGD and its continuous-time.pdf}
}

@article{gowerConvergenceTheoremsGradient,
  title = {Convergence {{Theorems}} for {{Gradient Descent}}},
  author = {Gower, Robert M.},
  file = {/home/jonathan/Zotero/storage/Z63B7RJ3/Gower_Convergence Theorems for Gradient Descent.pdf}
}

@inproceedings{gowerSGDGeneralAnalysis2019,
  title = {{{SGD}}: {{General Analysis}} and {{Improved Rates}}},
  shorttitle = {{{SGD}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richtárik, Peter},
  date = {2019-05-24},
  pages = {5200--5209},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/qian19b.html},
  urldate = {2022-09-05},
  abstract = {We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form minibatches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/5JL58NLC/Gower et al. - 2019 - SGD General Analysis and Improved Rates.pdf;/home/jonathan/Zotero/storage/9QVY34YG/Gower et al_2019_SGD.pdf}
}

@inproceedings{gurbuzbalabanHeavyTailPhenomenonSGD2021,
  title = {The {{Heavy-Tail Phenomenon}} in {{SGD}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  date = {2021-07-01},
  pages = {3964--3975},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/gurbuzbalaban21a.html},
  urldate = {2022-09-21},
  abstract = {In recent years, various notions of capacity and complexity have been proposed for characterizing the generalization properties of stochastic gradient descent (SGD) in deep learning. Some of the popular notions that correlate well with the performance on unseen data are (i) the ‘flatness’ of the local minimum found by SGD, which is related to the eigenvalues of the Hessian, (ii) the ratio of the stepsize ηη\textbackslash eta to the batch-size bbb, which essentially controls the magnitude of the stochastic gradient noise, and (iii) the ‘tail-index’, which measures the heaviness of the tails of the network weights at convergence. In this paper, we argue that these three seemingly unrelated perspectives for generalization are deeply linked to each other. We claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters ηη\textbackslash eta and bbb, the SGD iterates will converge to a \textbackslash emph\{heavy-tailed\} stationary distribution. We rigorously prove this claim in the setting of quadratic optimization: we show that even in a simple linear regression problem with independent and identically distributed data whose distribution has finite moments of all order, the iterates can be heavy-tailed with infinite variance. We further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. We then translate our results into insights about the behavior of SGD in deep learning. We support our theory with experiments conducted on synthetic data, fully connected, and convolutional neural networks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/CQJFEJGU/Gurbuzbalaban et al_2021_The Heavy-Tail Phenomenon in SGD.pdf;/home/jonathan/Zotero/storage/Q8LU9RL6/Gurbuzbalaban et al. - 2021 - The Heavy-Tail Phenomenon in SGD.pdf}
}

@article{hardtGradientDescentLearns2018,
  title = {Gradient {{Descent Learns Linear Dynamical Systems}}},
  author = {Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {19},
  number = {29},
  pages = {1--44},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v19/16-465.html},
  urldate = {2022-09-20},
  abstract = {We prove that stochastic gradient descent efficiently converges to the global optimizer of the maximum likelihood objective of an unknown linear time-invariant dynamical system from a sequence of noisy observations generated by the system. Even though the objective function is non-convex, we provide polynomial running time and sample complexity bounds under strong but natural assumptions. Linear systems identification has been studied for many decades, yet, to the best of our knowledge, these are the first polynomial guarantees for the problem we consider.},
  keywords = {SGD,toread},
  file = {/home/jonathan/Zotero/storage/MACHQHNK/Hardt et al_2018_Gradient Descent Learns Linear Dynamical Systems.pdf}
}

@inproceedings{johnsonAcceleratingStochasticGradient2013,
  title = {Accelerating {{Stochastic Gradient Descent}} Using {{Predictive Variance Reduction}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Johnson, Rie and Zhang, Tong},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html},
  urldate = {2022-09-05},
  abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we  prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG).  However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
  keywords = {SGD,SVRG,thesis,toread},
  file = {/home/jonathan/Zotero/storage/IVD968QK/Johnson_Zhang_2013_Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.pdf}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  date = {2021-08},
  journaltitle = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  url = {https://www.nature.com/articles/s41586-021-03819-2},
  urldate = {2022-09-12},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50~years9. Despite recent progress10–14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  issue = {7873},
  langid = {english},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/3XWK5JAZ/Jumper et al_2021_Highly accurate protein structure prediction with AlphaFold.pdf;/home/jonathan/Zotero/storage/5SSC5GX4/s41586-021-03819-2.html}
}

@misc{kawaguchiGeneralizationDeepLearning2020,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  date = {2020-07-27},
  number = {arXiv:1710.05468},
  eprint = {1710.05468},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.05468},
  url = {http://arxiv.org/abs/1710.05468},
  urldate = {2022-09-22},
  abstract = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.},
  archiveprefix = {arXiv},
  keywords = {SGD},
  file = {/home/jonathan/Zotero/storage/52EUC9LU/Kawaguchi et al_2020_Generalization in Deep Learning.pdf;/home/jonathan/Zotero/storage/7LVYN9Y7/1710.html}
}

@misc{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  date = {2017-02-09},
  number = {arXiv:1609.04836},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.04836},
  url = {http://arxiv.org/abs/1609.04836},
  urldate = {2022-09-05},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arXiv},
  keywords = {magnitude,noise,thesis},
  file = {/home/jonathan/Zotero/storage/WC8MQ9VK/Keskar et al_2017_On Large-Batch Training for Deep Learning.pdf;/home/jonathan/Zotero/storage/DV8AFCPB/1609.html}
}

@book{kloedenNumericalSolutionStochastic2013,
  title = {Numerical {{Solution}} of {{Stochastic Differential Equations}}},
  author = {Kloeden, Peter E. and Platen, Eckhard},
  date = {2013-04-17},
  eprint = {r9r6CAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The aim of this book is to provide an accessible introduction to stochastic differ ential equations and their applications together with a systematic presentation of methods available for their numerical solution. During the past decade there has been an accelerating interest in the de velopment of numerical methods for stochastic differential equations (SDEs). This activity has been as strong in the engineering and physical sciences as it has in mathematics, resulting inevitably in some duplication of effort due to an unfamiliarity with the developments in other disciplines. Much of the reported work has been motivated by the need to solve particular types of problems, for which, even more so than in the deterministic context, specific methods are required. The treatment has often been heuristic and ad hoc in character. Nevertheless, there are underlying principles present in many of the papers, an understanding of which will enable one to develop or apply appropriate numerical schemes for particular problems or classes of problems.},
  isbn = {978-3-662-12616-5},
  langid = {english},
  pagetotal = {666},
  keywords = {background,thesis},
  file = {/home/jonathan/Zotero/storage/GSQ3ATQC/sde_theory.djvu}
}

@inreference{KolmogorovSmirnovTest2022,
  title = {Kolmogorov–{{Smirnov}} Test},
  booktitle = {Wikipedia},
  date = {2022-06-10T10:54:21Z},
  url = {https://en.wikipedia.org/w/index.php?title=Kolmogorov%E2%80%93Smirnov_test&oldid=1092451253},
  urldate = {2022-09-26},
  abstract = {In statistics, the Kolmogorov–Smirnov test (K-S test or KS test) is a nonparametric test of the equality of continuous (or discontinuous, see Section 2.2), one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test).   In essence, the test answers the question "What is the probability that this collection of samples could have been drawn from that probability distribution?" or, in the second case, "What is the probability that these two sets of samples were drawn from the same (but unknown) probability distribution?". It is named after  Andrey Kolmogorov and Nikolai Smirnov. The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the sample is drawn from the reference distribution (in the one-sample case) or that the samples are drawn from the same distribution (in the two-sample case). In the one-sample case, the distribution considered under the null hypothesis may be continuous (see Section 2), purely discrete or mixed (see Section 2.2). In the two-sample case (see Section 3), the distribution considered under the null hypothesis is a continuous distribution but is otherwise unrestricted. However, the two sample test can also be performed under more general conditions that allow for discontinuity, heterogeneity and dependence across samples.The two-sample K–S test is one of the most useful and general nonparametric methods for comparing two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples. The Kolmogorov–Smirnov test can be modified to serve as a goodness of fit test. In the special case of testing for normality of the distribution, samples are standardized and compared with a standard normal distribution. This is equivalent to setting the mean and variance of the reference distribution equal to the sample estimates, and it is known that using these to define the specific reference distribution changes the null distribution of the test statistic (see Test with estimated parameters). Various studies have found that, even in this corrected form, the test is less powerful for testing normality than the Shapiro–Wilk test or Anderson–Darling test. However, these other tests have their own disadvantages. For instance the Shapiro–Wilk test is known not to work well in samples with many identical values.},
  langid = {english},
  annotation = {Page Version ID: 1092451253},
  file = {/home/jonathan/Zotero/storage/VB2EFUDH/Kolmogorov–Smirnov_test.html}
}

@book{kushnerStochasticApproximationRecursive2003,
  title = {Stochastic {{Approximation}} and {{Recursive Algorithms}} and {{Applications}}},
  author = {Kushner, Harold and Yin, G. George},
  date = {2003-07-17},
  eprint = {EC2w1SaPb7YC},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The basic stochastic approximation algorithms introduced by Robbins and MonroandbyKieferandWolfowitzintheearly1950shavebeenthesubject of an enormous literature, both theoretical and applied. This is due to the large number of applications and the interesting theoretical issues in the analysis of “dynamically de?ned” stochastic processes. The basic paradigm is a stochastic di?erence equation such as ? = ? + Y , where ? takes n+1 n n n n its values in some Euclidean space, Y is a random variable, and the “step n size” {$>$} 0 is small and might go to zero as n??. In its simplest form, n ? is a parameter of a system, and the random vector Y is a function of n “noise-corrupted” observations taken on the system when the parameter is set to ? . One recursively adjusts the parameter so that some goal is met n asymptotically. Thisbookisconcernedwiththequalitativeandasymptotic properties of such recursive algorithms in the diverse forms in which they arise in applications. There are analogous continuous time algorithms, but the conditions and proofs are generally very close to those for the discrete time case. The original work was motivated by the problem of ?nding a root of a continuous function g ̄(?), where the function is not known but the - perimenter is able to take “noisy” measurements at any desired value of ?. Recursive methods for root ?nding are common in classical numerical analysis, and it is reasonable to expect that appropriate stochastic analogs would also perform well.},
  isbn = {978-0-387-00894-3},
  langid = {english},
  pagetotal = {485}
}

@inproceedings{leeGradientDescentOnly2016,
  title = {Gradient {{Descent Only Converges}} to {{Minimizers}}},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  date = {2016-06-06},
  pages = {1246--1257},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v49/lee16.html},
  urldate = {2022-09-05},
  abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  keywords = {convergence,GD,theory,thesis},
  file = {/home/jonathan/Zotero/storage/MMWYWT6Z/Lee et al_2016_Gradient Descent Only Converges to Minimizers.pdf}
}

@inproceedings{liConvergenceStochasticGradient2019,
  title = {On the {{Convergence}} of {{Stochastic Gradient Descent}} with {{Adaptive Stepsizes}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Li, Xiaoyu and Orabona, Francesco},
  date = {2019-04-11},
  pages = {983--992},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/li19c.html},
  urldate = {2022-09-05},
  abstract = {Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice of the stepsizes. This has motivated a large body of research on adaptive stepsizes. However, there is currently a gap in our theoretical understanding of these methods, especially in the non-convex setting. In this paper, we start closing this gap: we theoretically analyze in the convex and non-convex settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions for these stepsizes to achieve almost sure asymptotic convergence of the gradients to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex setting. Moreover, we show that these stepsizes allow to automatically adapt to the level of noise of the stochastic gradients in both the convex and non-convex settings, interpolating between O(1/T) and O(1/sqrt(T)), up to logarithmic terms.},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {convergence,SGD,stepsize,thesis,toread},
  file = {/home/jonathan/Zotero/storage/PH74N3RG/Li and Orabona - 2019 - On the Convergence of Stochastic Gradient Descent .pdf;/home/jonathan/Zotero/storage/S7U8TV3J/Li_Orabona_2019_On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes.pdf}
}

@inproceedings{liExplainingRegularizationEffect2019,
  title = {Towards {{Explaining}} the {{Regularization Effect}} of {{Initial Large Learning Rate}} in {{Training Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html},
  urldate = {2022-09-08},
  abstract = {Stochastic gradient descent with a large initial learning rate is widely used for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon, we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes low-noise, hard-to-fit patterns, it generalizes worse on hard-to-generalize, easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate, but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer, as it relies too much on the patch early on.},
  keywords = {normalization,thesis},
  file = {/home/jonathan/Zotero/storage/5WR7JDSF/Li et al_2019_Towards Explaining the Regularization Effect of Initial Large Learning Rate in.pdf}
}

@misc{liExponentialLearningRate2019,
  title = {An {{Exponential Learning Rate Schedule}} for {{Deep Learning}}},
  author = {Li, Zhiyuan and Arora, Sanjeev},
  date = {2019-11-21},
  number = {arXiv:1910.07454},
  eprint = {1910.07454},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.07454},
  url = {http://arxiv.org/abs/1910.07454},
  urldate = {2022-08-22},
  abstract = {Intriguing empirical evidence exists that deep learning can work well with exoticschedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN, which is ubiquitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN. 1. Training can be done using SGD with momentum and an exponentially increasing learning rate schedule, i.e., learning rate increases by some \$(1 +\textbackslash alpha)\$ factor in every epoch for some \$\textbackslash alpha {$>$}0\$. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As expected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization. 2. Mathematical explanation of the success of the above rate schedule: a rigorous proof that it is equivalent to the standard setting of BN + SGD + StandardRate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization, LayerNormalization, Instance Norm, etc. 3. A worked-out toy example illustrating the above linkage of hyper-parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used.},
  archiveprefix = {arXiv},
  keywords = {normalization,thesis,toread},
  file = {/home/jonathan/Zotero/storage/A8RDPUPW/Li and Arora - 2019 - An Exponential Learning Rate Schedule for Deep Lea.pdf;/home/jonathan/Zotero/storage/37RXGWXR/1910.html}
}

@misc{liStochasticModifiedEquations2017,
  title = {Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms},
  author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
  date = {2017-06-20},
  number = {arXiv:1511.06251},
  eprint = {1511.06251},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.06251},
  url = {http://arxiv.org/abs/1511.06251},
  urldate = {2022-08-19},
  abstract = {We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.},
  archiveprefix = {arXiv},
  keywords = {background,SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/4Q6EBRI6/Li et al. - 2017 - Stochastic modified equations and adaptive stochas.pdf;/home/jonathan/Zotero/storage/WVBZ3XYB/1511.html}
}

@article{liStochasticModifiedEquations2019,
  title = {Stochastic {{Modified Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I Mathematical Foundations}}},
  shorttitle = {Stochastic {{Modified Equations}} and {{Dynamics}} of {{Stochastic Gradient Algorithms I}}},
  author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {40},
  pages = {1--47},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/17-526.html},
  urldate = {2022-08-19},
  abstract = {We develop the mathematical foundations of the stochastic modified equations (SME) framework for analyzing the dynamics of stochastic gradient algorithms, where the latter is approximated by a class of stochastic differential equations with small noise parameters. We prove that this approximation can be understood mathematically as an weak approximation, which leads to a number of precise and useful results on the approximations of stochastic gradient descent (SGD), momentum SGD and stochastic Nesterov's accelerated gradient method in the general setting of stochastic objectives. We also demonstrate through explicit calculations that this continuous-time approach can uncover important analytical insights into the stochastic gradient algorithms under consideration that may not be easy to obtain in a purely discrete-time setting.},
  keywords = {SDE,SGD,theory,thesis},
  file = {/home/jonathan/Zotero/storage/CBBRN2E7/Li et al. - 2019 - Stochastic Modified Equations and Dynamics of Stoc.pdf}
}

@misc{liValidityModelingSGD2021,
  title = {On the {{Validity}} of {{Modeling SGD}} with {{Stochastic Differential Equations}} ({{SDEs}})},
  author = {Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
  date = {2021-06-16},
  number = {arXiv:2102.12470},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.12470},
  url = {http://arxiv.org/abs/2102.12470},
  urldate = {2022-08-17},
  abstract = {It is generally recognized that finite learning rate (LR), in contrast to infinitesimal LR, is important for good generalization in real-life deep nets. Most attempted explanations propose approximating finite-LR SGD with Ito Stochastic Differential Equations (SDEs), but formal justification for this approximation (e.g., (Li et al., 2019)) only applies to SGD with tiny LR. Experimental verification of the approximation appears computationally infeasible. The current paper clarifies the picture with the following contributions: (a) An efficient simulation algorithm SVAG that provably converges to the conventionally used Ito SDE approximation. (b) A theoretically motivated testable necessary condition for the SDE approximation and its most famous implication, the linear scaling rule (Goyal et al., 2017), to hold. (c) Experiments using this simulation to demonstrate that the previously proposed SDE approximation can meaningfully capture the training and generalization properties of common deep nets.},
  keywords = {paper,SDE,SGD,SVAG,thesis},
  file = {/home/jonathan/Zotero/storage/T4H6WURH/Li et al. - 2021 - On the Validity of Modeling SGD with Stochastic Di.pdf;/home/jonathan/Zotero/storage/UR38RDHX/2102.html}
}

@misc{liWhatHappensSGD2022,
  title = {What {{Happens}} after {{SGD Reaches Zero Loss}}? --{{A Mathematical Framework}}},
  shorttitle = {What {{Happens}} after {{SGD Reaches Zero Loss}}?},
  author = {Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
  date = {2022-07-28},
  number = {arXiv:2110.06914},
  eprint = {2110.06914},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.06914},
  url = {http://arxiv.org/abs/2110.06914},
  urldate = {2022-08-26},
  abstract = {Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function \$L\$ can form a manifold. Intuitively, with a sufficiently small learning rate \$\textbackslash eta\$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such a regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, \$\textbackslash mathrm\{tr\}[\textbackslash nabla\^2 L]\$. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold -- i.e., the "implicit bias" -- using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a global analysis of the implicit bias valid for \$\textbackslash eta\^\{-2\}\$ steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for \$\textbackslash eta\^\{-1.6\}\$ steps and (2) allowing arbitrary noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires \$O(\textbackslash kappa\textbackslash ln d)\$ samples for learning an \$\textbackslash kappa\$-sparse overparametrized linear model in \$\textbackslash mathbb\{R\}\^d\$ (Woodworth et al., 2020), while GD initialized in the kernel regime requires \$\textbackslash Omega(d)\$ samples. This upper bound is minimax optimal and improves the previous \$\textbackslash tilde\{O\}(\textbackslash kappa\^2)\$ upper bound (HaoChen et al., 2020).},
  archiveprefix = {arXiv},
  keywords = {SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/NRWKRCPX/Li et al_2022_What Happens after SGD Reaches Zero Loss.pdf;/home/jonathan/Zotero/storage/PNPHABKM/2110.html}
}

@inproceedings{maPowerInterpolationUnderstanding2018,
  title = {The {{Power}} of {{Interpolation}}: {{Understanding}} the {{Effectiveness}} of {{SGD}} in {{Modern Over-parametrized Learning}}},
  shorttitle = {The {{Power}} of {{Interpolation}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  date = {2018-07-03},
  pages = {3325--3334},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/ma18a.html},
  urldate = {2022-09-07},
  abstract = {In this paper we aim to formally explain the phenomenon of fast convergence of Stochastic Gradient Descent (SGD) observed in modern machine learning. The key observation is that most modern learning architectures are over-parametrized and are trained to interpolate the data by driving the empirical loss (classification and regression) close to zero. While it is still unclear why these interpolated solutions perform well on test data, we show that these regimes allow for fast convergence of SGD, comparable in number of iterations to full gradient descent. For convex loss functions we obtain an exponential convergence bound for mini-batch SGD parallel to that for full gradient descent. We show that there is a critical batch size m∗m∗m\^* such that: (a) SGD iteration with mini-batch size m≤m∗m≤m∗m\textbackslash leq m\^* is nearly equivalent to mmm iterations of mini-batch size 111 (linear scaling regime). (b) SGD iteration with mini-batch m{$>$}m∗m{$>$}m∗m{$>$} m\^* is nearly equivalent to a full gradient descent iteration (saturation regime). Moreover, for the quadratic loss, we derive explicit expressions for the optimal mini-batch and step size and explicitly characterize the two regimes above. The critical mini-batch size can be viewed as the limit for effective mini-batch parallelization. It is also nearly independent of the data size, implying O(n)O(n)O(n) acceleration over GD per unit of computation. We give experimental evidence on real data which closely follows our theoretical analyses. Finally, we show how our results fit in the recent developments in training deep neural networks and discuss connections to adaptive rates for SGD and variance reduction.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/P7GFI2LM/Ma et al_2018_The Power of Interpolation.pdf}
}

@inproceedings{mertikopoulosAlmostSureConvergence2020,
  title = {On the Almost Sure Convergence of Stochastic Gradient Descent in Non-Convex Problems},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Mertikopoulos, Panayotis and Hallak, Nadav and Kavis, Ali and Cevher, Volkan},
  date = {2020-12-06},
  series = {{{NIPS}}'20},
  pages = {1117--1128},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {This paper analyzes the trajectories of stochastic gradient descent (SGD) to help understand the algorithm's convergence properties in non-convex problems. We first show that the sequence of iterates generated by SGD remains bounded and converges with probability 1 under a very broad range of step-size schedules. Subsequently, going beyond existing positive probability guarantees, we show that SGD avoids strict saddle points/manifolds with probability 1 for the entire spectrum of step-size policies considered. Finally, we prove that the algorithm's rate of convergence to local minimizers with a positive-definite Hessian is O(1/np) if the method is employed with a Θ(1/np) step-size. This provides an important guideline for tuning the algorithm's step-size as it suggests that a cool-down phase with a vanishing step-size could lead to faster convergence; we demonstrate this heuristic using ResNet architectures on CIFAR.},
  isbn = {978-1-71382-954-6},
  keywords = {SGD,theory toread},
  file = {/home/jonathan/Zotero/storage/AYGGK6N8/Mertikopoulos et al_2020_On the almost sure convergence of stochastic gradient descent in non-convex.pdf}
}

@inproceedings{moulinesNonAsymptoticAnalysisStochastic2011,
  title = {Non-{{Asymptotic Analysis}} of {{Stochastic Approximation Algorithms}} for {{Machine Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Moulines, Eric and Bach, Francis},
  date = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2011/hash/40008b9a5380fcacce3976bf7c08af5b-Abstract.html},
  urldate = {2022-09-28},
  abstract = {We consider the minimization of a convex objective function defined on a Hilbert space, which is only available through unbiased estimates of  its gradients.  This problem includes  standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the  convergence of two well-known algorithms, stochastic gradient descent (a.k.a.\textasciitilde Robbins-Monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.\textasciitilde Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.},
  keywords = {convergence,theory},
  file = {/home/jonathan/Zotero/storage/DYU79DLD/Moulines_Bach_2011_Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine.pdf}
}

@book{nesterovLecturesConvexOptimization2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  date = {2018-11-19},
  eprint = {IPh6DwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer}},
  abstract = {It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].},
  isbn = {978-3-319-91578-4},
  langid = {english},
  pagetotal = {603},
  keywords = {background,optimization,theory},
  file = {/home/jonathan/Zotero/storage/DMSGV4QG/Nesterov_2018_Lectures on Convex Optimization.pdf}
}

@inproceedings{nguyenSGDHogwildConvergence2018,
  title = {{{SGD}} and {{Hogwild}}! {{Convergence Without}} the {{Bounded Gradients Assumption}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Nguyen, Lam and Nguyen, Phuong Ha and Dijk, Marten and Richtarik, Peter and Scheinberg, Katya and Takac, Martin},
  date = {2018-07-03},
  pages = {3750--3758},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/nguyen18c.html},
  urldate = {2022-09-29},
  abstract = {Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/jonathan/Zotero/storage/A7GRMKW2/Nguyen et al. - 2018 - SGD and Hogwild! Convergence Without the Bounded G.pdf;/home/jonathan/Zotero/storage/QN8TDXNA/Nguyen et al_2018_SGD and Hogwild.pdf}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen},
  date = {2006-07-27},
  eprint = {eNlPAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer New York}},
  abstract = {Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side. There is a selected solutions manual for instructors for the new edition.},
  isbn = {978-0-387-30303-1},
  langid = {english},
  pagetotal = {694},
  file = {/home/jonathan/Zotero/storage/2TMCX6FC/Nocedal_Wright_2006_Numerical Optimization.pdf}
}

@online{OptimizationNeuralNetworks,
  title = {Optimization of {{Neural Networks}}},
  url = {https://www.deeplearningbook.org/contents/optimization.html},
  urldate = {2022-09-05},
  keywords = {optimization,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/Y5P6KEJ8/optimization.html}
}

@inproceedings{orvietoContinuoustimeModelsStochastic2019a,
  title = {Continuous-Time {{Models}} for {{Stochastic Optimization Algorithms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Orvieto, Antonio and Lucchi, Aurelien},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/9cd78264cf2cd821ba651485c111a29a-Abstract.html},
  urldate = {2022-09-05},
  abstract = {We propose new continuous-time formulations for first-order stochastic optimization algorithms such as mini-batch gradient descent and variance-reduced methods. We exploit these continuous-time models, together with simple Lyapunov analysis as well as tools from stochastic calculus, in order to derive convergence bounds for various types of non-convex functions. Guided by such analysis, we show that the same Lyapunov arguments hold in discrete-time, leading to matching rates. In addition, we use these models and Ito calculus to infer novel insights on the dynamics of SGD, proving that a decreasing learning rate acts as time warping or, equivalently, as landscape stretching.},
  keywords = {SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/MUPV8J9P/NeurIPS2019_SDEs_app.pdf;/home/jonathan/Zotero/storage/ZWXNPMF9/Orvieto_Lucchi_2019_Continuous-time Models for Stochastic Optimization Algorithms.pdf}
}

@misc{panigrahiNonGaussianityStochasticGradient2019,
  title = {Non-{{Gaussianity}} of {{Stochastic Gradient Noise}}},
  author = {Panigrahi, Abhishek and Somani, Raghav and Goyal, Navin and Netrapalli, Praneeth},
  date = {2019-10-25},
  number = {arXiv:1910.09626},
  eprint = {1910.09626},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.09626},
  url = {http://arxiv.org/abs/1910.09626},
  urldate = {2022-08-17},
  abstract = {What enables Stochastic Gradient Descent (SGD) to achieve better generalization than Gradient Descent (GD) in Neural Network training? This question has attracted much attention. In this paper, we study the distribution of the Stochastic Gradient Noise (SGN) vectors during the training. We observe that for batch sizes 256 and above, the distribution is best described as Gaussian at-least in the early phases of training. This holds across data-sets, architectures, and other choices.},
  archiveprefix = {arXiv},
  keywords = {noise,paper,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/C4QWEPDC/Panigrahi et al. - 2019 - Non-Gaussianity of Stochastic Gradient Noise.pdf;/home/jonathan/Zotero/storage/3KBMT38J/1910.html}
}

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022-04-12},
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2022-09-12},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/FDTGXDWS/Ramesh et al_2022_Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf;/home/jonathan/Zotero/storage/6RXSLEHY/2204.html}
}

@online{ReviewNeurIPSPaper,
  title = {Review for {{NeurIPS}} Paper: {{On}} the {{Almost Sure Convergence}} of {{Stochastic Gradient Descent}} in {{Non-Convex Problems}}},
  url = {https://papers.nips.cc/paper/2020/file/0cb5ebb1b34ec343dfe135db691e4a85-Review.html},
  urldate = {2022-09-20},
  file = {/home/jonathan/Zotero/storage/N2A8WXPN/0cb5ebb1b34ec343dfe135db691e4a85-Review.html}
}

@article{robbinsStochasticApproximationMethod1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  date = {1951-09},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full},
  urldate = {2022-09-29},
  abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = \textbackslash theta\$ of the equation \$M(x) = \textbackslash alpha\$, where \$\textbackslash alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,\textbackslash cdots\$ in such a way that \$x\_n\$ will tend to \$\textbackslash theta\$ in probability.},
  file = {/home/jonathan/Zotero/storage/7UGIIDFM/Robbins_Monro_1951_A Stochastic Approximation Method.pdf;/home/jonathan/Zotero/storage/SMM3N24C/1177729586.html}
}

@article{schrittwieserMasteringAtariGo2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  date = {2020-12-24},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  url = {http://arxiv.org/abs/1911.08265},
  urldate = {2022-09-12},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  keywords = {motivation,thesis},
  file = {/home/jonathan/Zotero/storage/Y4H7QWLF/Schrittwieser et al_2020_Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf;/home/jonathan/Zotero/storage/4QJ4BJ6Y/1911.html}
}

@inproceedings{sebbouhAlmostSureConvergence2021,
  title = {Almost Sure Convergence Rates for {{Stochastic Gradient Descent}} and {{Stochastic Heavy Ball}}},
  booktitle = {Proceedings of {{Thirty Fourth Conference}} on {{Learning Theory}}},
  author = {Sebbouh, Othmane and Gower, Robert M. and Defazio, Aaron},
  date = {2021-07-21},
  pages = {3935--3971},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v134/sebbouh21a.html},
  urldate = {2022-09-05},
  abstract = {We study stochastic gradient descent (SGD) and the stochastic heavy ball method (SHB, otherwise known as the momentum method) for the general stochastic approximation problem.  For SGD, in the convex and smooth setting, we provide the first \textbackslash emph\{almost sure\} asymptotic convergence \textbackslash emph\{rates\} for a weighted average of the iterates . More precisely, we show that the convergence rate of the function values is arbitrarily close to o(1/k−−√)o(1/k)o(1/\textbackslash sqrt\{k\}), and is exactly o(1/k)o(1/k)o(1/k) in the so-called overparametrized case. We show that these results still hold when using a decreasing step size version of stochastic line search and stochastic Polyak stepsizes, thereby giving the first proof of convergence of these methods in the non-overparametrized regime.  Using a substantially different analysis, we show that these rates hold for SHB as well, but at the last iterate. This distinction is important because it is the last iterate of SGD and SHB which is used in practice. We also show that the last iterate of SHB converges to a minimizer \textbackslash emph\{almost surely\}. Additionally, we prove that the function values of the deterministic HB converge at a o(1/k)o(1/k)o(1/k) rate, which is faster than the previously known O(1/k)O(1/k)O(1/k).  Finally, in the nonconvex setting, we prove similar rates on the lowest gradient norm along the trajectory of SGD.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/2NINK37W/Sebbouh et al_2021_Almost sure convergence rates for Stochastic Gradient Descent and Stochastic.pdf}
}

@article{shapiroConvergenceAnalysisGradient1996,
  title = {Convergence Analysis of Gradient Descent Stochastic Algorithms},
  author = {Shapiro, A. and Wardi, Y.},
  date = {1996-11-01},
  journaltitle = {Journal of Optimization Theory and Applications},
  shortjournal = {J Optim Theory Appl},
  volume = {91},
  number = {2},
  pages = {439--454},
  issn = {1573-2878},
  doi = {10.1007/BF02190104},
  url = {https://doi.org/10.1007/BF02190104},
  urldate = {2022-09-05},
  abstract = {This paper proves convergence of a sample-path based stochastic gradient-descent algorithm for optimizing expected-value performance measures in discrete event systems. The algorithm uses increasing precision at successive iterations, and it moves against the direction of a generalized gradient of the computed sample performance function. Two convergence results are established: one, for the case where the expected-value function is continuously differentiable; and the other, when that function is nondifferentiable but the sample performance functions are convex. The proofs are based on a version of the uniform law of large numbers which is provable for many discrete event systems where infinitesimal perturbation analysis is known to be strongly consistent.},
  langid = {english},
  keywords = {convergence,SGD,theory,thesis,toread},
  file = {/home/jonathan/Zotero/storage/NJ4IHM7A/Shapiro_Wardi_1996_Convergence analysis of gradient descent stochastic algorithms.pdf}
}

@inproceedings{simsekliTailIndexAnalysisStochastic2019,
  title = {A {{Tail-Index Analysis}} of {{Stochastic Gradient Noise}} in {{Deep Neural Networks}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  date = {2019-05-24},
  pages = {5827--5837},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/simsekli19a.html},
  urldate = {2022-08-17},
  abstract = {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed αα\textbackslash alpha-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Lévy motion. Such SDEs can incur ‘jumps’, which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the αα\textbackslash alpha-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {noise,paper,SDE,SGD,thesis},
  file = {/home/jonathan/Zotero/storage/B8IU5MQL/Simsekli et al. - 2019 - A Tail-Index Analysis of Stochastic Gradient Noise.pdf}
}

@article{soudryImplicitBiasGradient2018,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {19},
  number = {70},
  pages = {1--57},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v19/18-188.html},
  urldate = {2022-09-04},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
  keywords = {sgd,thesis,toread},
  file = {/home/jonathan/Zotero/storage/RFIJYHVS/Soudry et al_2018_The Implicit Bias of Gradient Descent on Separable Data.pdf}
}

@inproceedings{thomasInterplayNoiseCurvature2020,
  title = {On the Interplay between Noise and Curvature and Its Effect on Optimization and Generalization},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Thomas, Valentin and Pedregosa, Fabian and Merriënboer, Bart and Manzagol, Pierre-Antoine and Bengio, Yoshua and Roux, Nicolas Le},
  date = {2020-06-03},
  pages = {3503--3513},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v108/thomas20a.html},
  urldate = {2022-08-17},
  abstract = {The speed at which one can minimize an expected loss using stochastic methods depends on two properties: the curvature of the loss and the variance of the gradients. While most previous works focus on one or the other of these properties, we explore how their interaction affects optimization speed. Further, as the ultimate goal is good generalization performance, we clarify how both curvature and noise are relevant to properly estimate the generalization gap. Realizing that the limitations of some existing works stems from a confusion between these matrices, we also clarify the distinction between the Fisher matrix, the Hessian, and the covariance matrix of the gradients.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {fisher,paper,thesis},
  file = {/home/jonathan/Zotero/storage/4K8IEQ7T/Thomas et al. - 2020 - On the interplay between noise and curvature and i.pdf;/home/jonathan/Zotero/storage/HVRYFTD6/Thomas et al. - 2020 - On the interplay between noise and curvature and i.pdf}
}

@article{uhlenbeckTheoryBrownianMotion1930,
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.},
  date = {1930-09-01},
  journaltitle = {Physical Review},
  shortjournal = {Phys. Rev.},
  volume = {36},
  number = {5},
  pages = {823--841},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRev.36.823},
  url = {https://link.aps.org/doi/10.1103/PhysRev.36.823},
  urldate = {2022-09-12},
  abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u−u0exp(−βt) and s−u0β[1−exp(−βt)] where u0 is the initial velocity and β the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and Fürth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when β is much larger than the frequency and for values of t≫β−1, the formula takes the form of that previously given by Smoluchowski.},
  keywords = {SDE,thesis},
  file = {/home/jonathan/Zotero/storage/FQQXCPHW/PhysRev.36.html}
}

@inproceedings{vaswaniFastFasterConvergence2019,
  title = {Fast and {{Faster Convergence}} of {{SGD}} for {{Over-Parameterized Models}} and an {{Accelerated Perceptron}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  date = {2019-04-11},
  pages = {1195--1204},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/vaswani19a.html},
  urldate = {2022-09-07},
  abstract = {Modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely,  resulting in zero training loss. For such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. Under this condition, we prove that constant step-size stochastic gradient descent (SGD) with Nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. We also show that this condition implies that SGD can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. Under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. Given this weaker condition, we prove that SGD with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. Under additional assumptions, the above results enable us to prove an O(1/k2)O(1/k2)O(1/k\^2) mistake bound for kkk iterations of a stochastic perceptron algorithm using the squared-hinge loss. Finally, we validate our theoretical findings with experiments on synthetic and real datasets.},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/A3MP9FRI/Vaswani et al_2019_Fast and Faster Convergence of SGD for Over-Parameterized Models and an.pdf;/home/jonathan/Zotero/storage/TAFFELXT/Vaswani et al. - 2019 - Fast and Faster Convergence of SGD for Over-Parame.pdf}
}

@misc{wuGroupNormalization2018,
  title = {Group {{Normalization}}},
  author = {Wu, Yuxin and He, Kaiming},
  date = {2018-06-11},
  number = {arXiv:1803.08494},
  eprint = {1803.08494},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.08494},
  url = {http://arxiv.org/abs/1803.08494},
  urldate = {2022-08-17},
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  archiveprefix = {arXiv},
  keywords = {GN,NGD,paper,thesis,toread},
  file = {/home/jonathan/Zotero/storage/D7BRVQUJ/Wu and He - 2018 - Group Normalization.pdf;/home/jonathan/Zotero/storage/VK3T6FK6/1803.html}
}

@misc{wuNoisyGradientDescent2020,
  title = {On the {{Noisy Gradient Descent}} That {{Generalizes}} as {{SGD}}},
  author = {Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  date = {2020-06-19},
  number = {arXiv:1906.07405},
  eprint = {1906.07405},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.07405},
  url = {http://arxiv.org/abs/1906.07405},
  urldate = {2022-08-17},
  abstract = {The gradient noise of SGD is considered to play a central role in the observed strong generalization abilities of deep learning. While past studies confirm that the magnitude and the covariance structure of gradient noise are critical for regularization, it remains unclear whether or not the class of noise distributions is important. In this work we provide negative results by showing that noises in classes different from the SGD noise can also effectively regularize gradient descent. Our finding is based on a novel observation on the structure of the SGD noise: it is the multiplication of the gradient matrix and a sampling noise that arises from the mini-batch sampling procedure. Moreover, the sampling noises unify two kinds of gradient regularizing noises that belong to the Gaussian class: the one using (scaled) Fisher as covariance and the one using the gradient covariance of SGD as covariance. Finally, thanks to the flexibility of choosing noise class, an algorithm is proposed to perform noisy gradient descent that generalizes well, the variant of which even benefits large batch SGD training without hurting generalization.},
  archiveprefix = {arXiv},
  keywords = {important,paper,read,SDE,SGD,thesis},
  file = {/home/jonathan/Zotero/storage/5ATMKEPS/Wu et al. - 2020 - On the Noisy Gradient Descent that Generalizes as .pdf;/home/jonathan/Zotero/storage/G9K3M5XG/1906.html}
}

@misc{xieDiffusionTheoryDeep2021,
  title = {A {{Diffusion Theory For Deep Learning Dynamics}}: {{Stochastic Gradient Descent Exponentially Favors Flat Minima}}},
  shorttitle = {A {{Diffusion Theory For Deep Learning Dynamics}}},
  author = {Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  date = {2021-01-15},
  number = {arXiv:2002.03495},
  eprint = {2002.03495},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2002.03495},
  urldate = {2022-08-17},
  abstract = {Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.},
  archiveprefix = {arXiv},
  keywords = {paper,thesis,toread},
  file = {/home/jonathan/Zotero/storage/Z2AF6F2A/Xie et al. - 2021 - A Diffusion Theory For Deep Learning Dynamics Sto.pdf;/home/jonathan/Zotero/storage/IJDMK2NK/2002.html}
}

@misc{xingWalkSGD2018,
  title = {A {{Walk}} with {{SGD}}},
  author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  date = {2018-05-29},
  number = {arXiv:1802.08770},
  eprint = {1802.08770},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.08770},
  url = {http://arxiv.org/abs/1802.08770},
  urldate = {2022-09-22},
  abstract = {We present novel empirical observations regarding how stochastic gradient descent (SGD) navigates the loss landscape of over-parametrized deep neural networks (DNNs). These observations expose the qualitatively different roles of learning rate and batch-size in DNN optimization and generalization. Specifically we study the DNN loss surface along the trajectory of SGD by interpolating the loss surface between parameters from consecutive \textbackslash textit\{iterations\} and tracking various metrics during training. We find that the loss interpolation between parameters before and after each training iteration's update is roughly convex with a minimum (\textbackslash textit\{valley floor\}) in between for most of the training. Based on this and other metrics, we deduce that for most of the training update steps, SGD moves in valley like regions of the loss surface by jumping from one valley wall to another at a height above the valley floor. This 'bouncing between walls at a height' mechanism helps SGD traverse larger distance for small batch sizes and large learning rates which we find play qualitatively different roles in the dynamics. While a large learning rate maintains a large height from the valley floor, a small batch size injects noise facilitating exploration. We find this mechanism is crucial for generalization because the valley floor has barriers and this exploration above the valley floor allows SGD to quickly travel far away from the initialization point (without being affected by barriers) and find flatter regions, corresponding to better generalization.},
  archiveprefix = {arXiv},
  keywords = {SGD},
  file = {/home/jonathan/Zotero/storage/6UKFY3RR/Xing et al_2018_A Walk with SGD.pdf;/home/jonathan/Zotero/storage/YKWU3IFC/1802.html}
}

@misc{yangUnifiedConvergenceAnalysis2016,
  title = {Unified {{Convergence Analysis}} of {{Stochastic Momentum Methods}} for {{Convex}} and {{Non-convex Optimization}}},
  author = {Yang, Tianbao and Lin, Qihang and Li, Zhe},
  date = {2016-05-04},
  number = {arXiv:1604.03257},
  eprint = {1604.03257},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1604.03257},
  url = {http://arxiv.org/abs/1604.03257},
  urldate = {2022-09-19},
  abstract = {Recently, \{\textbackslash it stochastic momentum\} methods have been widely adopted in training deep neural networks. However, their convergence analysis is still underexplored at the moment, in particular for non-convex optimization. This paper fills the gap between practice and theory by developing a basic convergence analysis of two stochastic momentum methods, namely stochastic heavy-ball method and the stochastic variant of Nesterov's accelerated gradient method. We hope that the basic convergence results developed in this paper can serve the reference to the convergence of stochastic momentum methods and also serve the baselines for comparison in future development of stochastic momentum methods. The novelty of convergence analysis presented in this paper is a unified framework, revealing more insights about the similarities and differences between different stochastic momentum methods and stochastic gradient method. The unified framework exhibits a continuous change from the gradient method to Nesterov's accelerated gradient method and finally the heavy-ball method incurred by a free parameter, which can help explain a similar change observed in the testing error convergence behavior for deep learning. Furthermore, our empirical results for optimizing deep neural networks demonstrate that the stochastic variant of Nesterov's accelerated gradient method achieves a good tradeoff (between speed of convergence in training error and robustness of convergence in testing error) among the three stochastic methods.},
  archiveprefix = {arXiv},
  keywords = {convergence,SGD,theory,toread},
  file = {/home/jonathan/Zotero/storage/WWY323Y4/Yang et al_2016_Unified Convergence Analysis of Stochastic Momentum Methods for Convex and.pdf;/home/jonathan/Zotero/storage/INBQWV6T/1604.html}
}

@misc{zengStochasticVarianceReduced2021,
  title = {On {{Stochastic Variance Reduced Gradient Method}} for {{Semidefinite Optimization}}},
  author = {Zeng, Jinshan and Zha, Yixuan and Ma, Ke and Yao, Yuan},
  date = {2021-01-01},
  number = {arXiv:2101.00236},
  eprint = {2101.00236},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.00236},
  url = {http://arxiv.org/abs/2101.00236},
  urldate = {2022-09-05},
  abstract = {The low-rank stochastic semidefinite optimization has attracted rising attention due to its wide range of applications. The nonconvex reformulation based on the low-rank factorization, significantly improves the computational efficiency but brings some new challenge to the analysis. The stochastic variance reduced gradient (SVRG) method has been regarded as one of the most effective methods. SVRG in general consists of two loops, where a reference full gradient is first evaluated in the outer loop and then used to yield a variance reduced estimate of the current gradient in the inner loop. Two options have been suggested to yield the output of the inner loop, where Option I sets the output as its last iterate, and Option II yields the output via random sampling from all the iterates in the inner loop. However, there is a significant gap between the theory and practice of SVRG when adapted to the stochastic semidefinite programming (SDP). SVRG practically works better with Option I, while most of existing theoretical results focus on Option II. In this paper, we fill this gap via exploiting a new semi-stochastic variant of the original SVRG with Option I adapted to the semidefinite optimization. Equipped with this, we establish the global linear submanifold convergence (i.e., converging exponentially fast to a submanifold of a global minimum under the orthogonal group action) of the proposed SVRG method, given a provable initialization scheme and under certain smoothness and restricted strongly convex assumptions. Our analysis includes the effects of the mini-batch size and update frequency in the inner loop as well as two practical step size strategies, the fixed and stabilized Barzilai-Borwein step sizes. Some numerical results in matrix sensing demonstrate the efficiency of proposed SVRG method outperforming Option II counterpart as well as others.},
  archiveprefix = {arXiv},
  keywords = {convergence,SVRG,thesis,toread},
  file = {/home/jonathan/Zotero/storage/A8HZFT97/Zeng et al_2021_On Stochastic Variance Reduced Gradient Method for Semidefinite Optimization.pdf;/home/jonathan/Zotero/storage/2UZC4DPC/2101.html}
}

@article{zhangUnderstandingDeepLearning2021,
  title = {Understanding Deep Learning (Still) Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2021-02-22},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {64},
  number = {3},
  pages = {107--115},
  issn = {0001-0782},
  doi = {10.1145/3446776},
  url = {https://doi.org/10.1145/3446776},
  urldate = {2022-08-29},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
  keywords = {generalization,regularization,toread},
  file = {/home/jonathan/Zotero/storage/2PSPS7WB/Zhang et al_2021_Understanding deep learning (still) requires rethinking generalization.pdf}
}

@inproceedings{zhouStochasticMirrorDescent2017,
  title = {Stochastic {{Mirror Descent}} in {{Variationally Coherent Optimization Problems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Boyd, Stephen and Glynn, Peter W},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/e6ba70fc093b4ce912d769ede1ceeba8-Abstract.html},
  urldate = {2022-09-29},
  abstract = {In this paper, we examine a class of non-convex stochastic optimization problems which we call variationally coherent, and which properly includes pseudo-/quasiconvex and star-convex optimization problems. To solve such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case), and we show that the last iterate of SMD converges to the problem’s solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather, variational coherence, a much weaker requirement, suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.},
  file = {/home/jonathan/Zotero/storage/6CE883YF/Zhou et al_2017_Stochastic Mirror Descent in Variationally Coherent Optimization Problems.pdf}
}

@misc{zhuAnisotropicNoiseStochastic2019,
  title = {The {{Anisotropic Noise}} in {{Stochastic Gradient Descent}}: {{Its Behavior}} of {{Escaping}} from {{Sharp Minima}} and {{Regularization Effects}}},
  shorttitle = {The {{Anisotropic Noise}} in {{Stochastic Gradient Descent}}},
  author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  date = {2019-06-10},
  number = {arXiv:1803.00195},
  eprint = {1803.00195},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.00195},
  url = {http://arxiv.org/abs/1803.00195},
  urldate = {2022-08-25},
  abstract = {Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).},
  archiveprefix = {arXiv},
  keywords = {SDE,SGD,thesis,toread},
  file = {/home/jonathan/Zotero/storage/RPGC6YV3/Zhu et al_2019_The Anisotropic Noise in Stochastic Gradient Descent.pdf;/home/jonathan/Zotero/storage/9NTSUWQP/1803.html}
}


