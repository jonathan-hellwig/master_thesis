{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsc\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import scipy.stats as stats\n",
    "\n",
    "key = jr.PRNGKey(4)\n",
    "\n",
    "n_weight = 4\n",
    "\n",
    "\n",
    "def generate_data(key, n_data, n_weight):\n",
    "    x_key, noise_key, weight_key = jr.split(key, 3)\n",
    "    weights = jr.normal(weight_key, (1, n_weight))\n",
    "    bias = 1.0\n",
    "    x = jr.uniform(x_key, (n_data, n_weight))\n",
    "    noise = jr.normal(noise_key, (n_data, 1))\n",
    "    y = (weights * x).sum() + bias + noise\n",
    "    return x, y\n",
    "\n",
    "\n",
    "n_data = 1000\n",
    "x, y = generate_data(key, n_data, n_weight)\n",
    "X = jnp.column_stack((x, jnp.ones((n_data, 1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.vmap, in_axes=(0))\n",
    "def expm(x):\n",
    "    return jsc.linalg.expm(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(w, X, y):\n",
    "    return 0.5 * jnp.mean((X @ w - y.reshape((-1, 1))) ** 2)\n",
    "\n",
    "\n",
    "def update(carry, _):\n",
    "    w_gd, learning_rate = carry\n",
    "    grad = jax.grad(loss)(w_gd, X, y)\n",
    "    w_gd = w_gd - learning_rate * grad\n",
    "    return (w_gd, learning_rate), w_gd\n",
    "\n",
    "\n",
    "def fit_convergance_line(error, learning_rates):\n",
    "    A = jnp.column_stack(\n",
    "        (jnp.log(jnp.array(learning_rates)), jnp.ones((len(learning_rates),)))\n",
    "    )\n",
    "    log_error = jnp.log(jnp.array(error))\n",
    "    coefficients, _, _, _ = jnp.linalg.lstsq(A, log_error)\n",
    "    return coefficients\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def max_error(x, y):\n",
    "    return jnp.max((jnp.abs(jnp.linalg.norm(x, axis=1) - jnp.linalg.norm(y, axis=1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_variables = n_weight + 1\n",
    "w_0 = jnp.ones((n_variables, 1))\n",
    "M = X.T @ X\n",
    "w_star = jnp.linalg.solve(M, X.T @ y).reshape((-1, 1))\n",
    "\n",
    "first_order_error = []\n",
    "second_order_error = []\n",
    "learning_rates = [0.2, 0.1, 0.05, 0.025,\n",
    "                  0.01, 0.005, 0.0025, 0.00125, 0.000625, 0.0003125]\n",
    "final_time = 1\n",
    "results = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    max_iter = int(final_time / learning_rate)\n",
    "\n",
    "    # Analytical solution to the modified equation\n",
    "    t = jnp.linspace(0, final_time,\n",
    "                     max_iter+1).reshape((-1, 1, 1))\n",
    "    w_first_order = (expm(-1/n_data * M * t) @\n",
    "                     (w_0 - w_star) + w_star).reshape((-1, n_variables))\n",
    "    w_second_order = (expm(-1/n_data * (M @ (jnp.eye(n_variables) + learning_rate/(2*n_data) * M)) * t) @\n",
    "                      (w_0 - w_star) + w_star).reshape((-1, n_variables))\n",
    "\n",
    "    # Gradient descent iterates\n",
    "    _, iterates = jax.lax.scan(\n",
    "        update, (w_0, learning_rate), jnp.arange(0, max_iter))\n",
    "\n",
    "    w_gd = jnp.concatenate(\n",
    "        (w_0.reshape((1, n_variables, 1)), iterates)).squeeze()\n",
    "    first_order_error.append(max_error(w_gd, w_first_order))\n",
    "    second_order_error.append(max_error(w_gd, w_second_order))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(learning_rates, first_order_error,\n",
    "        '*', label='Experimental first order')\n",
    "ax.plot(learning_rates, second_order_error,\n",
    "        '*', label='Experimental second order')\n",
    "ax.plot(learning_rates, jnp.array(learning_rates) * (first_order_error[0]/learning_rates[0]),\n",
    "        '--', label='Theoretical first order')\n",
    "ax.plot(learning_rates, jnp.array(learning_rates) **\n",
    "        2 * (second_order_error[0]/learning_rates[0]**2), '--', label='Theoretical second order')\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel('$\\eta$')\n",
    "ax.set_ylabel('error')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.set_figwidth(7)\n",
    "fig.savefig('../seminar_talk/plots/linear_regression_error.pdf')\n",
    "first_order, _ = fit_convergance_line(first_order_error, learning_rates)\n",
    "second_order, _ = fit_convergance_line(second_order_error, learning_rates)\n",
    "first_order.item(), second_order.item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec9535f87be7d8c4f42574e8edf52bc718743b5ec1344a6584f30e3879820b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
