{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 15:26:44.770245: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to ~/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|██████████| 4/4 [00:04<00:00,  1.10s/ file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to ~/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-08-26 15:27:52.874595: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-26 15:27:52.874620: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jonathan/uni/thesis/code/mnist.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 192>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=188'>189</a>\u001b[0m config\u001b[39m.\u001b[39mnum_epochs \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=189'>190</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=191'>192</a>\u001b[0m train_and_evaluate(config, \u001b[39m'\u001b[39;49m\u001b[39m./results/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/jonathan/uni/thesis/code/mnist.ipynb Cell 1\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, workdir)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=139'>140</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_evaluate\u001b[39m(config: ml_collections\u001b[39m.\u001b[39mConfigDict,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=140'>141</a>\u001b[0m                        workdir: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m train_state\u001b[39m.\u001b[39mTrainState:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=141'>142</a>\u001b[0m     \u001b[39m\"\"\"Execute model training and evaluation loop.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=142'>143</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=143'>144</a>\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=148'>149</a>\u001b[0m \u001b[39m    The train state (which includes the `.params`).\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=149'>150</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=150'>151</a>\u001b[0m     train_ds, test_ds \u001b[39m=\u001b[39m get_datasets()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=151'>152</a>\u001b[0m     rng \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mPRNGKey(\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=153'>154</a>\u001b[0m     summary_writer \u001b[39m=\u001b[39m tensorboard\u001b[39m.\u001b[39mSummaryWriter(workdir)\n",
      "\u001b[1;32m/home/jonathan/uni/thesis/code/mnist.ipynb Cell 1\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=119'>120</a>\u001b[0m ds_builder \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mbuilder(\u001b[39m'\u001b[39m\u001b[39mmnist\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=120'>121</a>\u001b[0m ds_builder\u001b[39m.\u001b[39mdownload_and_prepare(download_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/jonathan/uni/thesis/code/data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=121'>122</a>\u001b[0m train_ds \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mas_numpy(\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=122'>123</a>\u001b[0m     ds_builder\u001b[39m.\u001b[39mas_dataset(split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=123'>124</a>\u001b[0m test_ds \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39mas_numpy(ds_builder\u001b[39m.\u001b[39mas_dataset(split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000000?line=124'>125</a>\u001b[0m train_ds[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mfloat32(train_ds[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39m255.\u001b[39m\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:81\u001b[0m, in \u001b[0;36mas_dataset.<locals>.decorator\u001b[0;34m(function, builder, args, kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m logger \u001b[39min\u001b[39;00m _get_registered_loggers():\n\u001b[1;32m     69\u001b[0m   logger\u001b[39m.\u001b[39mas_dataset(\n\u001b[1;32m     70\u001b[0m       dataset_name\u001b[39m=\u001b[39mbuilder\u001b[39m.\u001b[39mname,\n\u001b[1;32m     71\u001b[0m       config_name\u001b[39m=\u001b[39mconfig_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m       as_supervised\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mas_supervised\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     79\u001b[0m       decoders\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdecoders\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:607\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[1;32m    599\u001b[0m build_single_dataset \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    600\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_single_dataset,\n\u001b[1;32m    601\u001b[0m     shuffle_files\u001b[39m=\u001b[39mshuffle_files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     as_supervised\u001b[39m=\u001b[39mas_supervised,\n\u001b[1;32m    606\u001b[0m )\n\u001b[0;32m--> 607\u001b[0m all_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(build_single_dataset, split)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m all_ds\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:625\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    622\u001b[0m   batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mtotal_num_examples \u001b[39mor\u001b[39;00m sys\u001b[39m.\u001b[39mmaxsize\n\u001b[1;32m    624\u001b[0m \u001b[39m# Build base dataset\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_dataset(\n\u001b[1;32m    626\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m    627\u001b[0m     shuffle_files\u001b[39m=\u001b[39;49mshuffle_files,\n\u001b[1;32m    628\u001b[0m     decoders\u001b[39m=\u001b[39;49mdecoders,\n\u001b[1;32m    629\u001b[0m     read_config\u001b[39m=\u001b[39;49mread_config,\n\u001b[1;32m    630\u001b[0m )\n\u001b[1;32m    631\u001b[0m \u001b[39m# Auto-cache small datasets which are small enough to fit in memory.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_cache_ds(\n\u001b[1;32m    633\u001b[0m     split\u001b[39m=\u001b[39msplit, shuffle_files\u001b[39m=\u001b[39mshuffle_files, read_config\u001b[39m=\u001b[39mread_config):\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:991\u001b[0m, in \u001b[0;36mFileReaderBuilder._as_dataset\u001b[0;34m(self, split, decoders, read_config, shuffle_files)\u001b[0m\n\u001b[1;32m    985\u001b[0m reader \u001b[39m=\u001b[39m reader_lib\u001b[39m.\u001b[39mReader(\n\u001b[1;32m    986\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir,\n\u001b[1;32m    987\u001b[0m     example_specs\u001b[39m=\u001b[39mexample_specs,\n\u001b[1;32m    988\u001b[0m     file_format\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfile_format,\n\u001b[1;32m    989\u001b[0m )\n\u001b[1;32m    990\u001b[0m decode_fn \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(features\u001b[39m.\u001b[39mdecode_example, decoders\u001b[39m=\u001b[39mdecoders)\n\u001b[0;32m--> 991\u001b[0m \u001b[39mreturn\u001b[39;00m reader\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    992\u001b[0m     instructions\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m    993\u001b[0m     split_infos\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49msplits\u001b[39m.\u001b[39;49mvalues(),\n\u001b[1;32m    994\u001b[0m     decode_fn\u001b[39m=\u001b[39;49mdecode_fn,\n\u001b[1;32m    995\u001b[0m     read_config\u001b[39m=\u001b[39;49mread_config,\n\u001b[1;32m    996\u001b[0m     shuffle_files\u001b[39m=\u001b[39;49mshuffle_files,\n\u001b[1;32m    997\u001b[0m     disable_shuffling\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mdisable_shuffling,\n\u001b[1;32m    998\u001b[0m )\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:346\u001b[0m, in \u001b[0;36mReader.read\u001b[0;34m(self, instructions, split_infos, read_config, shuffle_files, disable_shuffling, decode_fn)\u001b[0m\n\u001b[1;32m    337\u001b[0m   file_instructions \u001b[39m=\u001b[39m splits_dict[instruction]\u001b[39m.\u001b[39mfile_instructions\n\u001b[1;32m    338\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_files(\n\u001b[1;32m    339\u001b[0m       file_instructions,\n\u001b[1;32m    340\u001b[0m       read_config\u001b[39m=\u001b[39mread_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m       decode_fn\u001b[39m=\u001b[39mdecode_fn,\n\u001b[1;32m    344\u001b[0m   )\n\u001b[0;32m--> 346\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_read_instruction_to_ds, instructions)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:338\u001b[0m, in \u001b[0;36mReader.read.<locals>._read_instruction_to_ds\u001b[0;34m(instruction)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_instruction_to_ds\u001b[39m(instruction):\n\u001b[1;32m    337\u001b[0m   file_instructions \u001b[39m=\u001b[39m splits_dict[instruction]\u001b[39m.\u001b[39mfile_instructions\n\u001b[0;32m--> 338\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_files(\n\u001b[1;32m    339\u001b[0m       file_instructions,\n\u001b[1;32m    340\u001b[0m       read_config\u001b[39m=\u001b[39;49mread_config,\n\u001b[1;32m    341\u001b[0m       shuffle_files\u001b[39m=\u001b[39;49mshuffle_files,\n\u001b[1;32m    342\u001b[0m       disable_shuffling\u001b[39m=\u001b[39;49mdisable_shuffling,\n\u001b[1;32m    343\u001b[0m       decode_fn\u001b[39m=\u001b[39;49mdecode_fn,\n\u001b[1;32m    344\u001b[0m   )\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:378\u001b[0m, in \u001b[0;36mReader.read_files\u001b[0;34m(self, file_instructions, read_config, shuffle_files, disable_shuffling, decode_fn)\u001b[0m\n\u001b[1;32m    375\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    377\u001b[0m \u001b[39m# Read serialized example (eventually with `tfds_id`)\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m ds \u001b[39m=\u001b[39m _read_files(\n\u001b[1;32m    379\u001b[0m     file_instructions\u001b[39m=\u001b[39;49mfile_instructions,\n\u001b[1;32m    380\u001b[0m     read_config\u001b[39m=\u001b[39;49mread_config,\n\u001b[1;32m    381\u001b[0m     shuffle_files\u001b[39m=\u001b[39;49mshuffle_files,\n\u001b[1;32m    382\u001b[0m     disable_shuffling\u001b[39m=\u001b[39;49mdisable_shuffling,\n\u001b[1;32m    383\u001b[0m     file_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file_format,\n\u001b[1;32m    384\u001b[0m )\n\u001b[1;32m    386\u001b[0m \u001b[39m# Parse and decode\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_and_decode\u001b[39m(ex: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TreeDict[Tensor]:\n\u001b[1;32m    388\u001b[0m   \u001b[39m# TODO(pierrot): `parse_example` uses\u001b[39;00m\n\u001b[1;32m    389\u001b[0m   \u001b[39m# `tf.io.parse_single_example`. It might be faster to use `parse_example`,\u001b[39;00m\n\u001b[1;32m    390\u001b[0m   \u001b[39m# after batching.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m   \u001b[39m# https://www.tensorflow.org/api_docs/python/tf/io/parse_example\u001b[39;00m\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:211\u001b[0m, in \u001b[0;36m_read_files\u001b[0;34m(file_instructions, read_config, shuffle_files, disable_shuffling, file_format)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m (shuffle_files \u001b[39mand\u001b[39;00m read_config\u001b[39m.\u001b[39mshuffle_seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     tf_compat\u001b[39m.\u001b[39mget_option_deterministic(read_config\u001b[39m.\u001b[39moptions) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    209\u001b[0m   deterministic \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m ds \u001b[39m=\u001b[39m instruction_ds\u001b[39m.\u001b[39;49minterleave(\n\u001b[1;32m    212\u001b[0m     functools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    213\u001b[0m         _get_dataset_from_filename,\n\u001b[1;32m    214\u001b[0m         do_skip\u001b[39m=\u001b[39;49mdo_skip,\n\u001b[1;32m    215\u001b[0m         do_take\u001b[39m=\u001b[39;49mdo_take,\n\u001b[1;32m    216\u001b[0m         file_format\u001b[39m=\u001b[39;49mfile_format,\n\u001b[1;32m    217\u001b[0m         add_tfds_id\u001b[39m=\u001b[39;49mread_config\u001b[39m.\u001b[39;49madd_tfds_id,\n\u001b[1;32m    218\u001b[0m     ),\n\u001b[1;32m    219\u001b[0m     cycle_length\u001b[39m=\u001b[39;49mcycle_length,\n\u001b[1;32m    220\u001b[0m     block_length\u001b[39m=\u001b[39;49mblock_length,\n\u001b[1;32m    221\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mread_config\u001b[39m.\u001b[39;49mnum_parallel_calls_for_interleave_files,\n\u001b[1;32m    222\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    223\u001b[0m )\n\u001b[1;32m    225\u001b[0m \u001b[39m# If the number of examples read in the tf-record is known, we forward\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m# the information to the tf.data.Dataset object.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m# Check the `tf.data.experimental` for backward compatibility with TF <= 2.1\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m (read_config\u001b[39m.\u001b[39massert_cardinality \u001b[39mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mnot\u001b[39;00m read_config\u001b[39m.\u001b[39minput_context \u001b[39mand\u001b[39;00m  \u001b[39m# TODO(epot): Restore cardinality\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39mhasattr\u001b[39m(tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexperimental, \u001b[39m'\u001b[39m\u001b[39massert_cardinality\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m    231\u001b[0m   \u001b[39m# TODO(b/154963426): Replace by per-shard cardinality (warning if\u001b[39;00m\n\u001b[1;32m    232\u001b[0m   \u001b[39m# `experimental_interleave_sort_fn` is set).\u001b[39;00m\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2215\u001b[0m, in \u001b[0;36mDatasetV2.interleave\u001b[0;34m(self, map_func, cycle_length, block_length, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2212\u001b[0m   \u001b[39mreturn\u001b[39;00m InterleaveDataset(\n\u001b[1;32m   2213\u001b[0m       \u001b[39mself\u001b[39m, map_func, cycle_length, block_length, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   2214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2215\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelInterleaveDataset(\n\u001b[1;32m   2216\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2217\u001b[0m       map_func,\n\u001b[1;32m   2218\u001b[0m       cycle_length,\n\u001b[1;32m   2219\u001b[0m       block_length,\n\u001b[1;32m   2220\u001b[0m       num_parallel_calls,\n\u001b[1;32m   2221\u001b[0m       deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m   2222\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5412\u001b[0m, in \u001b[0;36mParallelInterleaveDataset.__init__\u001b[0;34m(self, input_dataset, map_func, cycle_length, block_length, num_parallel_calls, buffer_output_elements, prefetch_input_elements, deterministic, name)\u001b[0m\n\u001b[1;32m   5410\u001b[0m \u001b[39m\"\"\"See `Dataset.interleave()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m   5411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m-> 5412\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m   5413\u001b[0m     map_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(), dataset\u001b[39m=\u001b[39;49minput_dataset)\n\u001b[1;32m   5414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[1;32m   5415\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   5416\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   5417\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_get_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2559\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m   2560\u001b[0m \n\u001b[1;32m   2561\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2565\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m   2566\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2567\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[1;32m   2568\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2569\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2531\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2532\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 2533\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2534\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m   2535\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   2536\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m   cache_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mgeneralize(cache_key)\n\u001b[1;32m   2709\u001b[0m   (args, kwargs) \u001b[39m=\u001b[39m cache_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 2711\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   2712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                          graph_function)\n\u001b[1;32m   2715\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2622\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   2623\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2624\u001b[0m ]\n\u001b[1;32m   2625\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[1;32m   2626\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   2628\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   2629\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   2630\u001b[0m         args,\n\u001b[1;32m   2631\u001b[0m         kwargs,\n\u001b[1;32m   2632\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   2633\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   2634\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   2635\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   2636\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   2637\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m   2638\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m   2639\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   2640\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   2641\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   2642\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   2643\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2644\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1038\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(op_return_value, ops\u001b[39m.\u001b[39mTensor), op_return_value\n\u001b[1;32m   1037\u001b[0m \u001b[39mif\u001b[39;00m func_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m   func_graph \u001b[39m=\u001b[39m FuncGraph(\n\u001b[1;32m   1039\u001b[0m       name, collections\u001b[39m=\u001b[39;49mcollections, capture_by_value\u001b[39m=\u001b[39;49mcapture_by_value)\n\u001b[1;32m   1040\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(func_graph, FuncGraph)\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m add_control_dependencies:\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:196\u001b[0m, in \u001b[0;36mFuncGraph.__init__\u001b[0;34m(self, name, collections, capture_by_value, structured_input_signature, structured_outputs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    168\u001b[0m              name,\n\u001b[1;32m    169\u001b[0m              collections\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    170\u001b[0m              capture_by_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    171\u001b[0m              structured_input_signature\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    172\u001b[0m              structured_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    173\u001b[0m   \u001b[39m\"\"\"Construct a new FuncGraph.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[39m  The graph will inherit its graph key, collections, seed, and distribution\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m      information.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m   \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m()\n\u001b[1;32m    197\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n\u001b[1;32m    198\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3136\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_deleters \u001b[39m=\u001b[39m {}\n\u001b[1;32m   3132\u001b[0m \u001b[39m# Allow optimizers and other objects to pseudo-uniquely key graphs (this key\u001b[39;00m\n\u001b[1;32m   3133\u001b[0m \u001b[39m# will be shared when defining function graphs, for example, so optimizers\u001b[39;00m\n\u001b[1;32m   3134\u001b[0m \u001b[39m# being called inside function definitions behave as if they were seeing the\u001b[39;00m\n\u001b[1;32m   3135\u001b[0m \u001b[39m# actual outside graph).\u001b[39;00m\n\u001b[0;32m-> 3136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39mgrap-key-\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m (uid(),)\n\u001b[1;32m   3137\u001b[0m \u001b[39m# A string with the last reduction method passed to\u001b[39;00m\n\u001b[1;32m   3138\u001b[0m \u001b[39m# losses.compute_weighted_loss(), or None. This is required only for\u001b[39;00m\n\u001b[1;32m   3139\u001b[0m \u001b[39m# backward compatibility with Estimator and optimizer V1 use cases.\u001b[39;00m\n\u001b[1;32m   3140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_loss_reduction \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Copyright 2022 The Flax Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"MNIST example.\n",
    "\n",
    "Library file which executes the training and evaluation loop for MNIST.\n",
    "The data is loaded using tensorflow_datasets.\n",
    "\"\"\"\n",
    "\n",
    "# See issue #620.\n",
    "# pytype: disable=wrong-keyword-args\n",
    "\n",
    "from statistics import covariance\n",
    "from absl import logging\n",
    "from flax import linen as nn\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, images, labels):\n",
    "    \"\"\"Computes gradients, loss and accuracy for a single batch.\"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, images)\n",
    "        one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(\n",
    "            optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return grads, loss, accuracy\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, rng):\n",
    "    \"\"\"Train for a single epoch.\"\"\"\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, len(train_ds['image']))\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_accuracy = []\n",
    "    gradient_noise = []\n",
    "    flattend_parameters, _ = jax.flatten_util.ravel_pytree(state.params)\n",
    "    number_of_parameters = flattend_parameters.size\n",
    "    average = jnp.zeros((number_of_parameters, ))\n",
    "    # print(flatten_variables)\n",
    "    for perm in perms:\n",
    "        batch_images = train_ds['image'][perm, ...]\n",
    "        batch_labels = train_ds['label'][perm, ...]\n",
    "        grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
    "        flattend_grads, _ = jax.flatten_util.ravel_pytree(grads)\n",
    "        average += batch_size / train_ds_size * flattend_grads\n",
    "\n",
    "    print('average done')\n",
    "    for perm in perms:\n",
    "        batch_images = train_ds['image'][perm, ...]\n",
    "        batch_labels = train_ds['label'][perm, ...]\n",
    "        grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
    "        flattend_grads, _ = jax.flatten_util.ravel_pytree(grads)\n",
    "        gradient_noise.append(jnp.linalg.norm(average - flattend_grads)**2)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    print('noise done')\n",
    "\n",
    "    for perm in perms:\n",
    "        batch_images = train_ds['image'][perm, ...]\n",
    "        batch_labels = train_ds['label'][perm, ...]\n",
    "        grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
    "        state = update_model(state, grads)\n",
    "        epoch_loss.append(loss)\n",
    "        epoch_accuracy.append(accuracy)\n",
    "    print('update done')\n",
    "\n",
    "    train_loss = np.mean(epoch_loss)\n",
    "    train_accuracy = np.mean(epoch_accuracy)\n",
    "    return state, train_loss, train_accuracy, gradient_noise\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n",
    "    ds_builder = tfds.builder('mnist')\n",
    "    ds_builder.download_and_prepare(download_dir='/home/jonathan/uni/thesis/code/data')\n",
    "    train_ds = tfds.as_numpy(\n",
    "        ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "    return train_ds, test_ds\n",
    "\n",
    "\n",
    "def create_train_state(rng, config):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    cnn = CNN()\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "    tx = optax.sgd(config.learning_rate, config.momentum)\n",
    "    return train_state.TrainState.create(apply_fn=cnn.apply,\n",
    "                                         params=params,\n",
    "                                         tx=tx)\n",
    "\n",
    "\n",
    "def train_and_evaluate(config: ml_collections.ConfigDict,\n",
    "                       workdir: str) -> train_state.TrainState:\n",
    "    \"\"\"Execute model training and evaluation loop.\n",
    "\n",
    "  Args:\n",
    "    config: Hyperparameter configuration for training and evaluation.\n",
    "    workdir: Directory where the tensorboard summaries are written to.\n",
    "\n",
    "  Returns:\n",
    "    The train state (which includes the `.params`).\n",
    "  \"\"\"\n",
    "    train_ds, test_ds = get_datasets()\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "\n",
    "    summary_writer = tensorboard.SummaryWriter(workdir)\n",
    "    summary_writer.hparams(dict(config))\n",
    "\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(init_rng, config)\n",
    "    print('start training')\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        print(f'epoch {epoch}')\n",
    "\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        state, train_loss, train_accuracy, gradient_noise = train_epoch(\n",
    "            state, train_ds, config.batch_size, input_rng)\n",
    "        _, test_loss, test_accuracy = apply_model(state, test_ds['image'],\n",
    "                                                  test_ds['label'])\n",
    "\n",
    "        logging.info(\n",
    "            'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'\n",
    "            % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
    "               test_accuracy * 100))\n",
    "\n",
    "        summary_writer.scalar('train_loss', train_loss, epoch)\n",
    "        summary_writer.scalar('train_accuracy', train_accuracy, epoch)\n",
    "        summary_writer.scalar('test_loss', test_loss, epoch)\n",
    "        summary_writer.scalar('test_accuracy', test_accuracy, epoch)\n",
    "        summary_writer.histogram('gradient_noies', gradient_noise, epoch)\n",
    "\n",
    "    summary_writer.flush()\n",
    "    return state\n",
    "\n",
    "\n",
    "config = ml_collections.ConfigDict()\n",
    "\n",
    "config.learning_rate = 0.1\n",
    "config.momentum = 0.9\n",
    "config.batch_size = 1024\n",
    "config.num_epochs = 15\n",
    "print('start')\n",
    "\n",
    "train_and_evaluate(config, './results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-26 15:25:17.993563: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to ~/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|██████████| 4/4 [00:05<00:00,  1.26s/ file]\n",
      "2022-08-26 15:25:24.738094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-26 15:25:24.738123: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to ~/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "epoch 1\n",
      "average done\n",
      "noise done\n",
      "update done\n",
      "epoch 2\n",
      "average done\n",
      "noise done\n",
      "update done\n",
      "epoch 3\n",
      "average done\n",
      "noise done\n",
      "update done\n",
      "epoch 4\n",
      "average done\n",
      "noise done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jonathan/uni/thesis/code/mnist.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=5'>6</a>\u001b[0m config\u001b[39m.\u001b[39mnum_epochs \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=8'>9</a>\u001b[0m train_and_evaluate(config, \u001b[39m'\u001b[39;49m\u001b[39m./results/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/jonathan/uni/thesis/code/mnist.ipynb Cell 2\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, workdir)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=160'>161</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=162'>163</a>\u001b[0m rng, input_rng \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(rng)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=163'>164</a>\u001b[0m state, train_loss, train_accuracy, gradient_noise \u001b[39m=\u001b[39m train_epoch(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=164'>165</a>\u001b[0m     state, train_ds, config\u001b[39m.\u001b[39;49mbatch_size, input_rng)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=165'>166</a>\u001b[0m _, test_loss, test_accuracy \u001b[39m=\u001b[39m apply_model(state, test_ds[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=166'>167</a>\u001b[0m                                           test_ds[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=168'>169</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=169'>170</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepoch:\u001b[39m\u001b[39m% 3d\u001b[39;00m\u001b[39m, train_loss: \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m, train_accuracy: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m, test_loss: \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m, test_accuracy: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=170'>171</a>\u001b[0m     \u001b[39m%\u001b[39m (epoch, train_loss, train_accuracy \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, test_loss,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=171'>172</a>\u001b[0m        test_accuracy \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m))\n",
      "\u001b[1;32m/home/jonathan/uni/thesis/code/mnist.ipynb Cell 2\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(state, train_ds, batch_size, rng)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=104'>105</a>\u001b[0m batch_images \u001b[39m=\u001b[39m train_ds[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][perm, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=105'>106</a>\u001b[0m batch_labels \u001b[39m=\u001b[39m train_ds[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m][perm, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=106'>107</a>\u001b[0m grads, loss, accuracy \u001b[39m=\u001b[39m apply_model(state, batch_images, batch_labels)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=107'>108</a>\u001b[0m state \u001b[39m=\u001b[39m update_model(state, grads)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jonathan/uni/thesis/code/mnist.ipynb#ch0000001?line=108'>109</a>\u001b[0m epoch_loss\u001b[39m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m~/uni/thesis/code/venv/lib/python3.10/site-packages/flax/core/frozen_dict.py:159\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, _, data)\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[39m\"\"\"Flattens this FrozenDict.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    A flattened version of this FrozenDict instance.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m   \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dict,), ()\n\u001b[0;32m--> 159\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_unflatten\u001b[39m(\u001b[39mcls\u001b[39m, _, data):\n\u001b[1;32m    161\u001b[0m   \u001b[39m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    162\u001b[0m   \u001b[39m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    163\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39mdata, __unsafe_skip_copy__\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a99f663a0081fc4d79982a9a3f8601320443115eaf3f50fe3abd32f90a3be28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
